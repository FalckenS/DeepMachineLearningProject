{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Machine Learning Project (SSY340)\n",
    "\n",
    "Project Group 92"
   ],
   "id": "755ce3c68a828da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment setup:",
   "id": "7ab7239d695adf25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports and CUDA setup:",
   "id": "6ac8efba5d72a9fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, time, math, random, torch, gc, ast, re\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from transformers import AutoModel\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Method for clearing cache and GPU memory\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "81b8df0429de0732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Constants:",
   "id": "33e675f07eca55ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "IMAGES_DIR = \"flickr30k-images\"\n",
    "CSV_PATH = \"flickr_annotations_30k.csv\"\n",
    "CAPTIONS_FILE = \"Flickr30k.token.txt\"\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "MIN_FREQ = 5 # Minimum frequency for vocab, lower value means slower training but bigger vocabulary\n",
    "NUM_EPOCHS = 5\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1"
   ],
   "id": "6aef934d3f1eddd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Flickr30k.token.txt (captions):",
   "id": "50c62e26d9f6e174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _get_image_name(row, df):\n",
    "    for col in ('file_name','filename','image','img','image_filename','img_name','image_name','img_id','image_id','path'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            return os.path.basename(str(val))\n",
    "    return f\"{row.name}.jpg\"\n",
    "\n",
    "def _get_captions(row, df):\n",
    "    for col in ('raw','captions','sentences','sentence','caption','raw_captions','sentids'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return [str(x).strip() for x in val if str(x).strip()]\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, (list, tuple)):\n",
    "                        return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "                    if isinstance(parsed, dict) and 'raw' in parsed:\n",
    "                        r = parsed['raw']\n",
    "                        if isinstance(r, (list, tuple)):\n",
    "                            return [str(x).strip() for x in r if str(x).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                for sep in ('|||', '||', '\\n'):\n",
    "                    if sep in val:\n",
    "                        return [s.strip() for s in val.split(sep) if s.strip()]\n",
    "                return [val.strip()]\n",
    "    return []\n",
    "\n",
    "def generate_token_file_from_csv(csv_path, captions_file):\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(\"CSV columns:\", list(df.columns))\n",
    "    with open(captions_file, 'w', encoding='utf-8') as fout:\n",
    "        for _, row in df.iterrows():\n",
    "            img_name = _get_image_name(row, df)\n",
    "            caps = _get_captions(row, df)\n",
    "            if not caps:\n",
    "                continue\n",
    "            for i, c in enumerate(caps):\n",
    "                fout.write(f\"{img_name}#{i}\\t{c}\\n\")\n",
    "    print(\"Wrote token file:\", captions_file)\n",
    "\n",
    "# If you already have CAPTIONS_FILE from earlier step, skip this call.\n",
    "if os.path.exists(CSV_PATH) and not os.path.exists(CAPTIONS_FILE):\n",
    "    generate_token_file_from_csv(CSV_PATH, CAPTIONS_FILE)"
   ],
   "id": "8eb6d435fed5dbeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup tokenizer/vocab:",
   "id": "ba2fcb2a0232f7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq=5, reserved=None):\n",
    "        if reserved is None:\n",
    "            reserved = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        self.min_freq = min_freq\n",
    "        self.reserved = reserved\n",
    "        self.freq = Counter()\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "\n",
    "    def build(self, token_lists):\n",
    "        for t in token_lists:\n",
    "            self.freq.update(t)\n",
    "        self.itos = list(self.reserved)\n",
    "        for tok, cnt in self.freq.most_common():\n",
    "            if cnt < self.min_freq:\n",
    "                continue\n",
    "            if tok in self.reserved:\n",
    "                continue\n",
    "            self.itos.append(tok)\n",
    "        self.stoi = {tok:i for i,tok in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.stoi.get(t, self.stoi['<unk>']) for t in tokens]\n",
    "\n",
    "\n",
    "def tokenize_caption(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ],
   "id": "b9bb3bc066e6ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datasets:",
   "id": "cd6c39a10b09d502"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, images_dir, captions_file, vocab=None, transform=None, split='train', seed=SEED):\n",
    "        self.images_dir = str(images_dir)\n",
    "        self.transform = transform\n",
    "        image_to_captions = defaultdict(list)\n",
    "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                img_token, cap = parts\n",
    "                img_name = img_token.split('#')[0]\n",
    "                image_to_captions[img_name].append(cap)\n",
    "\n",
    "        available = set(os.listdir(self.images_dir))\n",
    "        self.entries = []\n",
    "        for img, caps in image_to_captions.items():\n",
    "            if img not in available:\n",
    "                continue\n",
    "            for c in caps:\n",
    "                self.entries.append((img, c))\n",
    "\n",
    "        # Split images at the image level\n",
    "        images = sorted(list({e[0] for e in self.entries}))\n",
    "        random.Random(seed).shuffle(images)\n",
    "        n_train = int(len(images) * TRAIN_RATIO)\n",
    "        n_val = int(len(images) * VAL_RATIO)\n",
    "        train_images = set(images[:n_train])\n",
    "        val_images = set(images[n_train:n_train+n_val])\n",
    "        test_images = set(images[n_train+n_val:])\n",
    "\n",
    "        if split == 'train':\n",
    "            self.entries = [e for e in self.entries if e[0] in train_images]\n",
    "        elif split == 'val':\n",
    "            self.entries = [e for e in self.entries if e[0] in val_images]\n",
    "        elif split == 'test':\n",
    "            self.entries = [e for e in self.entries if e[0] in test_images]\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train', 'val', or 'test'\")\n",
    "\n",
    "        if vocab is None and split=='train':\n",
    "            token_lists = [tokenize_caption(c) for _, c in self.entries]\n",
    "            self.vocab = Vocab(min_freq=MIN_FREQ)\n",
    "            self.vocab.build(token_lists)\n",
    "        elif vocab is not None:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            raise ValueError(\"Provide vocab for val/test split\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.entries[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tokens = tokenize_caption(cap)\n",
    "        num_caption = torch.tensor([self.vocab.stoi['<start>']] + self.vocab.numericalize(tokens) + [self.vocab.stoi['<end>']])\n",
    "        return image, num_caption\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, caps = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    lengths = [c.size(0) for c in caps]\n",
    "    caps_padded = nn.utils.rnn.pad_sequence(caps, batch_first=True, padding_value=0)\n",
    "    return images, caps_padded, lengths\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=train_transform, split='train')\n",
    "vocab = train_ds.vocab\n",
    "val_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='val')\n",
    "test_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='test')"
   ],
   "id": "bbccdb64a818704",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training utilities:",
   "id": "f804f60af8e30afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_epoch(enc, dec, loader, criterion, enc_opt, dec_opt, device, print_every=PRINT_EVERY):\n",
    "    enc.train(); dec.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, caps, lengths in loader:\n",
    "        images = images.to(device)\n",
    "        caps = caps.to(device)\n",
    "\n",
    "        feats = enc(images)\n",
    "\n",
    "        # Clip captions to decoder's max length\n",
    "        caps_input = caps[:, :-1]             # (B, L-1)\n",
    "        caps_input = caps_input[:, :dec.max_len]  # clip to max_len\n",
    "        logits = dec(feats, caps_input)\n",
    "\n",
    "        targets = caps[:, 1:]                 # (B, L-1)\n",
    "        targets = targets[:, :dec.max_len]   # same length as logits\n",
    "        logits = logits[:, :targets.size(1), :]  # make shapes match\n",
    "\n",
    "        logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
    "        targets_flat = targets.contiguous().view(-1)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "        if enc_opt: enc_opt.zero_grad()\n",
    "        dec_opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "        if enc_opt: nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "\n",
    "        if enc_opt: enc_opt.step()\n",
    "        dec_opt.step()\n",
    "\n",
    "        # compute token-level accuracy ignoring <pad>\n",
    "        pred_tokens = logits.argmax(dim=2)  # (B, L)\n",
    "        mask = targets != 0  # ignore <pad>\n",
    "        correct = (pred_tokens == targets) & mask\n",
    "        correct_tokens += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(1, n_batches), 100*correct_tokens/total_tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(enc, dec, loader, criterion, device, vocab=None, print_every=PRINT_EVERY):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    n_batches = 0\n",
    "    pad_idx = vocab.stoi['<pad>'] if vocab else 0\n",
    "\n",
    "    for batch_idx, (images, caps, _) in enumerate(loader, 1):\n",
    "        images, caps = images.to(device), caps.to(device)\n",
    "        feats = enc(images)\n",
    "\n",
    "        # Always slice teacher-forcing input\n",
    "        logits = dec(feats, caps[:, :-1])\n",
    "        targets = caps[:, 1:]\n",
    "\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if vocab is not None:\n",
    "            pred_tokens = logits.argmax(dim=2)\n",
    "            mask = targets != pad_idx\n",
    "            correct_tokens += ((pred_tokens == targets) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    avg_acc = 100*correct_tokens/total_tokens if total_tokens > 0 else None\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(enc, dec, img_path, transform, vocab, device, max_len=30):\n",
    "    enc.eval(); dec.eval()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    feats = enc(x)  # (1, E)\n",
    "    start_id = vocab.stoi.get('<start>', None)\n",
    "    # pass start_id for transformer decoder; RNN ignores it\n",
    "    gen = dec.sample(feats, start_id=start_id, max_len=max_len)  # (1, max_len)\n",
    "    gen = gen[0].cpu().tolist()\n",
    "    words=[]\n",
    "    for idx in gen:\n",
    "        if idx < len(vocab.itos):\n",
    "            tok = vocab.itos[idx]\n",
    "        else:\n",
    "            tok = '<unk>'\n",
    "        if tok == '<end>': break\n",
    "        if tok not in ('<pad>','<start>'):\n",
    "            words.append(tok)\n",
    "    return ' '.join(words)"
   ],
   "id": "1033064fef890217",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fit and plot model functions:",
   "id": "846fe48b7a6b84f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # --------------- Plot loss: ---------------\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --------------- Plot accuracy: ---------------\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_acc'], label='Train Accuracy', marker='o')\n",
    "    plt.plot(epochs, history['val_acc'], label='Val Accuracy', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "CEL = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
    "\n",
    "def fit_model(\n",
    "    enc, dec,\n",
    "    train_loader, val_loader,\n",
    "    enc_opt, dec_opt,\n",
    "    device,\n",
    "    vocab,\n",
    "    output_dir,\n",
    "    num_epochs,\n",
    "    criterion=CEL,\n",
    "    print_every=PRINT_EVERY,\n",
    "    train_encoder_only=False\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    enc.to(device)\n",
    "    dec.to(device)\n",
    "\n",
    "    # Freeze decoder\n",
    "    if train_encoder_only:\n",
    "        for p in dec.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    best_val = float('inf')\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    pad_idx = vocab.stoi['<pad>'] if vocab else 0\n",
    "\n",
    "    def compute_accuracy(logits, targets):\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = targets != pad_idx\n",
    "        correct = (preds == targets) & mask\n",
    "        total = mask.sum().item()\n",
    "        return correct.sum().item() / total if total > 0 else 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        t0 = time.time()\n",
    "        enc.train(); dec.train()\n",
    "\n",
    "        # --------------- Train: ---------------\n",
    "\n",
    "        train_loss_accum, train_acc_accum, steps = 0.0, 0.0, 0\n",
    "        for batch_idx, (images, caps, _) in enumerate(train_loader, 1):\n",
    "            images, caps = images.to(device), caps.to(device)\n",
    "            features = enc(images)\n",
    "            logits = dec(features, caps[:, :-1])\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "\n",
    "            if enc_opt: enc_opt.zero_grad()\n",
    "            if not train_encoder_only and dec_opt: dec_opt.zero_grad()  # only if decoder is trainable\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            if enc_opt: torch.nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "            if not train_encoder_only and dec_opt: torch.nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "\n",
    "            if enc_opt: enc_opt.step()\n",
    "            if not train_encoder_only and dec_opt: dec_opt.step()\n",
    "\n",
    "            train_loss_accum += loss.item()\n",
    "            train_acc_accum += compute_accuracy(logits, caps[:, 1:])\n",
    "            steps += 1\n",
    "            if batch_idx % print_every == 0 or batch_idx == len(train_loader):\n",
    "                print(f\"Epoch {epoch} Train batch {batch_idx}/{len(train_loader)} \"\n",
    "                      f\"Loss={train_loss_accum/steps:.4f} Acc={100*train_acc_accum/steps:.2f}%\")\n",
    "\n",
    "        train_loss = train_loss_accum / steps\n",
    "        train_acc = 100 * train_acc_accum / steps\n",
    "\n",
    "        # --------------- Validate: ---------------\n",
    "\n",
    "        enc.eval(); dec.eval()\n",
    "        val_loss_accum, val_acc_accum, steps = 0.0, 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, caps, _) in enumerate(val_loader, 1):\n",
    "                images, caps = images.to(device), caps.to(device)\n",
    "                features = enc(images)\n",
    "                logits = dec(features, caps[:, :-1])\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "\n",
    "                val_loss_accum += loss.item()\n",
    "                pred_tokens = logits.argmax(dim=2)\n",
    "                mask = caps[:, 1:] != pad_idx\n",
    "                correct = (pred_tokens == caps[:, 1:]) & mask\n",
    "                val_acc_accum += correct.sum().item() / mask.sum().item()\n",
    "                steps += 1\n",
    "\n",
    "        val_loss = val_loss_accum / steps\n",
    "        val_acc = 100 * val_acc_accum / steps\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} train_loss={train_loss:.4f} acc={train_acc:.2f}% \"\n",
    "              f\"val_loss={val_loss:.4f} acc={val_acc:.2f}% time={(time.time()-t0):.1f}s\")\n",
    "\n",
    "        # --------------- Save checkpoint: ---------------\n",
    "\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': enc.state_dict(),\n",
    "            'decoder_state_dict': dec.state_dict(),\n",
    "            'vocab': getattr(vocab, 'itos', vocab),\n",
    "            'history': history,\n",
    "            'enc_optimizer_state_dict': enc_opt.state_dict() if enc_opt else None,\n",
    "            'dec_optimizer_state_dict': dec_opt.state_dict() if dec_opt else None,\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(output_dir, f\"ckpt_epoch_{epoch}.pth\"))\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(ckpt, os.path.join(output_dir, \"best.pth\"))\n",
    "\n",
    "    plot_training_history(history)\n",
    "    return history"
   ],
   "id": "35c183f0c3620a27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decoder base class:",
   "id": "a8a625cdfdeeb08f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DecoderBase(nn.Module):\n",
    "    def forward(self, features, captions):\n",
    "        raise NotImplementedError\n",
    "    def sample(self, features, start_id=None, max_len=30):\n",
    "        raise NotImplementedError"
   ],
   "id": "5c3e4c2346443dac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1: CNN-RNN",
   "id": "3c2c59171b681b76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Simple CNN->RNN image caption baseline model.\n",
    "\n",
    "Encoder: CNN with transfer learning from ResNet18.\n",
    "\n",
    "Decoder: Text RNN, no transfer learning."
   ],
   "id": "6e0057fff467073f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, fine_tune=False):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1]  # remove fc\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(512, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fine_tune(fine_tune)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)               # (B, 512, 1, 1)\n",
    "        feat = feat.view(feat.size(0), -1)    # (B, 512)\n",
    "        feat = self.fc(feat)                  # (B, embed)\n",
    "        feat = self.relu(feat)\n",
    "        return feat\n",
    "\n",
    "    def fine_tune(self, fine):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = fine\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "class RNNDecoder(DecoderBase):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Linear layers to project image features to hidden & cell states\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        features: image embeddings from encoder (B, E)\n",
    "        captions: token indices (B, L)\n",
    "        \"\"\"\n",
    "        embeddings = self.embed(captions)           # (B, L, E)\n",
    "\n",
    "        # Initialize LSTM hidden & cell states from image features\n",
    "        h0 = self.init_h(features).unsqueeze(0)    # (1, B, H)\n",
    "        c0 = self.init_c(features).unsqueeze(0)    # (1, B, H)\n",
    "\n",
    "        outputs, _ = self.lstm(embeddings, (h0, c0))  # (B, L, H)\n",
    "        logits = self.linear(outputs)                   # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "FINE_TUNE = False\n",
    "\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "OUTPUT_DIR = \"./models_cnn_rnn\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Instantiate models\n",
    "enc = CNNEncoder(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    fine_tune=FINE_TUNE\n",
    ")\n",
    "dec = RNNDecoder(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    vocab_size=len(vocab)\n",
    ")\n",
    "\n",
    "# Optimizers and loss\n",
    "enc_opt = optim.Adam(enc.parameters(), lr=LEARNING_RATE*0.1) if FINE_TUNE else None\n",
    "dec_opt = optim.Adam(dec.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Train\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "4a815cba02749e22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2: ViT trained from scratch",
   "id": "82eb4f9c519c051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers trained from scratch.\n",
    "\n",
    "Encoder: Visual transformer (ViT), no transfer learning.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "48aaf7e1f14ee57c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Simple patch embedding: conv with stride=patch_size.\"\"\"\n",
    "    def __init__(self, in_ch=3, embed_dim=256, patch_size=16):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 1 == 0\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)                     # (B, E, Hp, Wp)\n",
    "        B, E, Hp, Wp = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)     # (B, S, E) where S = Hp*Wp\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, patch_size=16, num_layers=4, num_heads=4, mlp_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(in_ch=3, embed_dim=embed_dim, patch_size=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = None  # created lazily based on sequence length\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,H,W)\n",
    "        x = self.patch_embed(x)   # (B, S, E)\n",
    "        B, S, E = x.shape\n",
    "        if self.pos_embed is None or self.pos_embed.size(1) != (S+1):\n",
    "            # initialize positional embedding (1, S+1, E)\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, S+1, E).to(x.device))\n",
    "            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B,1,E)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)          # (B, S+1, E)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer_encoder(x)                # (B, S+1, E)\n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:, 0]                              # (B, E) - pooled\n",
    "        return cls_out\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (T, B, D)\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerDecoderAdapter(DecoderBase):\n",
    "    def __init__(self, embed_size, vocab_size, nhead=8, num_decoder_layers=3, dim_feedforward=2048, dropout=0.1, max_len=50):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len=max_len)\n",
    "        self.feature_proj = nn.Linear(embed_size, embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.linear_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # features: (B, E)\n",
    "        device = features.device\n",
    "        B, L = captions.size()\n",
    "        memory = self.feature_proj(features).unsqueeze(0)  # (1, B, E)\n",
    "        tgt = self.embed(captions).permute(1, 0, 2)        # (L, B, E)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(L, device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # (L, B, E)\n",
    "        out = out.permute(1, 0, 2)  # (B, L, E)\n",
    "        logits = self.linear_out(out)  # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, features, start_id=None, max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        assert start_id is not None, \"start_id must be provided for transformer sampling\"\n",
    "        device = features.device\n",
    "        B = features.size(0)\n",
    "        memory = self.feature_proj(features).unsqueeze(0)  # (1, B, E)\n",
    "        generated = torch.full((B, 1), fill_value=start_id, dtype=torch.long, device=device)\n",
    "        ids = []\n",
    "        for t in range(max_len):\n",
    "            tgt = self.embed(generated).permute(1, 0, 2)   # (T, B, E)\n",
    "            tgt = self.pos_enc(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(tgt.size(0), device)\n",
    "            out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # (T, B, E)\n",
    "            last = out[-1]  # (B, E)\n",
    "            logits = self.linear_out(last)  # (B, V)\n",
    "            pred = logits.argmax(dim=1)     # (B,)\n",
    "            ids.append(pred)\n",
    "            generated = torch.cat([generated, pred.unsqueeze(1)], dim=1)\n",
    "        ids = torch.stack(ids, dim=1)  # (B, max_len)\n",
    "        return ids\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "EMBED_SIZE = 128\n",
    "\n",
    "OUTPUT_DIR = \"./models_vit_no_tl\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Instantiate models\n",
    "enc = ViTEncoder(\n",
    "    embed_dim=EMBED_SIZE,\n",
    "    patch_size=16,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    mlp_dim=512,\n",
    "    dropout=0.1\n",
    ")\n",
    "dec = TransformerDecoderAdapter(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    nhead=2,\n",
    "    num_decoder_layers=1,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_len=100\n",
    ")\n",
    "\n",
    "# Optimizers and loss\n",
    "enc_opt = optim.Adam(enc.parameters(), lr=LEARNING_RATE*0.1)\n",
    "dec_opt = optim.Adam(dec.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Train\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "3e4340c01d6f5a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 3: Pre-trained ViT as encoder, same text transformer decoder:",
   "id": "f825371530f54d33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers, pre-trained encoder.\n",
    "\n",
    "Encoder: ViT with transfer learning from google/vit-base-patch16-224.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "33697f2d30915e76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "# Load pretrained DINOv3 encoder\n",
    "encoder = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Freeze encoder parameters\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define a projection layer if necessary\n",
    "class EncoderWithProjection(nn.Module):\n",
    "    def __init__(self, encoder, embed_size):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.proj = nn.Linear(encoder.config.hidden_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.encoder(x)\n",
    "        pooled = outputs.pooler_output  # (batch_size, hidden_size)\n",
    "        return self.proj(pooled)\n",
    "\n",
    "# Initialize the encoder with projection\n",
    "encoder_with_proj = EncoderWithProjection(encoder, embed_size=128)\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "EMBED_SIZE = 128\n",
    "\n",
    "OUTPUT_DIR = \"./models_vit_DINOv3_enc\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Instantiate models\n",
    "enc = EncoderWithProjection(\n",
    "    encoder,\n",
    "    embed_size=128\n",
    ")\n",
    "\n",
    "# Optimizers and loss\n",
    "enc_opt = torch.optim.Adam(enc.parameters(), lr=LEARNING_RATE)\n",
    "dec_opt = None # decoder frozen\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Train\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_encoder_only=True\n",
    ")"
   ],
   "id": "ac8f5b46562fe1f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
