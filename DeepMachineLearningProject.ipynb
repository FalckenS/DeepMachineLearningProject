{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Machine Learning Project (SSY340)\n",
    "\n",
    "Project Group 92"
   ],
   "id": "755ce3c68a828da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup:",
   "id": "7ab7239d695adf25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports and CUDA setup:",
   "id": "6ac8efba5d72a9fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, math, random, torch, gc, ast, re\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "PIN_MEMORY = True\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Method for clearing cache and GPU memory\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "2a1126f1708db314",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Constants:",
   "id": "33e675f07eca55ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "IMAGES_DIR = \"flickr30k-images\"\n",
    "CSV_PATH = \"flickr_annotations_30k.csv\"\n",
    "CAPTIONS_FILE = \"Flickr30k.token.txt\"\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "MIN_FREQ = 5 # Minimum frequency for vocab, lower value means slower training but bigger vocabulary\n",
    "MAX_LEN = 50\n",
    "EVAL_MAX_LEN = 30\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "# TEST_RATIO = 0.1"
   ],
   "id": "6970da4f76792d23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Flickr30k.token.txt (captions):",
   "id": "50c62e26d9f6e174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _get_image_name(row, df):\n",
    "    for col in ('file_name','filename','image','img','image_filename','img_name','image_name','img_id','image_id','path'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            return os.path.basename(str(val))\n",
    "    return f\"{row.name}.jpg\"\n",
    "\n",
    "def _get_captions(row, df):\n",
    "    for col in ('raw','captions','sentences','sentence','caption','raw_captions','sentids'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return [str(x).strip() for x in val if str(x).strip()]\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, (list, tuple)):\n",
    "                        return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "                    if isinstance(parsed, dict) and 'raw' in parsed:\n",
    "                        r = parsed['raw']\n",
    "                        if isinstance(r, (list, tuple)):\n",
    "                            return [str(x).strip() for x in r if str(x).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                for sep in ('|||', '||', '\\n'):\n",
    "                    if sep in val:\n",
    "                        return [s.strip() for s in val.split(sep) if s.strip()]\n",
    "                return [val.strip()]\n",
    "    return []\n",
    "\n",
    "def generate_token_file_from_csv(csv_path, captions_file):\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(\"CSV columns:\", list(df.columns))\n",
    "    with open(captions_file, 'w', encoding='utf-8') as fout:\n",
    "        for _, row in df.iterrows():\n",
    "            img_name = _get_image_name(row, df)\n",
    "            caps = _get_captions(row, df)\n",
    "            if not caps:\n",
    "                continue\n",
    "            for i, c in enumerate(caps):\n",
    "                fout.write(f\"{img_name}#{i}\\t{c}\\n\")\n",
    "    print(\"Wrote token file:\", captions_file)"
   ],
   "id": "127f788fd6720e27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup tokenizer/vocab:",
   "id": "ba2fcb2a0232f7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq=MIN_FREQ, reserved=None):\n",
    "        if reserved is None:\n",
    "            reserved = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        self.min_freq = min_freq\n",
    "        self.reserved = reserved\n",
    "        self.freq = Counter()\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "\n",
    "    def build(self, token_lists):\n",
    "        for t in token_lists:\n",
    "            self.freq.update(t)\n",
    "        self.itos = list(self.reserved)\n",
    "        for tok, cnt in self.freq.most_common():\n",
    "            if cnt < self.min_freq:\n",
    "                continue\n",
    "            if tok in self.reserved:\n",
    "                continue\n",
    "            self.itos.append(tok)\n",
    "        self.stoi = {tok:i for i,tok in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.stoi.get(t, self.stoi['<unk>']) for t in tokens]\n",
    "\n",
    "\n",
    "def tokenize_caption(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ],
   "id": "67cd345b8d58078b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Datasets:",
   "id": "cd6c39a10b09d502"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, images_dir, captions_file, vocab=None, transform=None, split='train', seed=SEED, return_raw_caption=False):\n",
    "        super().__init__()\n",
    "        self.images_dir = str(images_dir)\n",
    "        self.transform = transform\n",
    "        self.return_raw_caption = return_raw_caption\n",
    "\n",
    "        image_to_captions = defaultdict(list)\n",
    "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                img_token, cap = parts\n",
    "                img_name = img_token.split('#')[0]\n",
    "                image_to_captions[img_name].append(cap)\n",
    "\n",
    "        available = set(os.listdir(self.images_dir))\n",
    "        self.entries = []\n",
    "        for img, caps in image_to_captions.items():\n",
    "            if img not in available:\n",
    "                continue\n",
    "            for c in caps:\n",
    "                self.entries.append((img, c))\n",
    "\n",
    "        images = sorted(list({e[0] for e in self.entries}))\n",
    "        random.Random(seed).shuffle(images)\n",
    "        n_train = int(len(images) * TRAIN_RATIO)\n",
    "        n_val = int(len(images) * VAL_RATIO)\n",
    "        train_images = set(images[:n_train])\n",
    "        val_images = set(images[n_train:n_train+n_val])\n",
    "        test_images = set(images[n_train+n_val:])\n",
    "\n",
    "        if split == 'train':\n",
    "            self.entries = [e for e in self.entries if e[0] in train_images]\n",
    "        elif split == 'val':\n",
    "            self.entries = [e for e in self.entries if e[0] in val_images]\n",
    "        elif split == 'test':\n",
    "            self.entries = [e for e in self.entries if e[0] in test_images]\n",
    "\n",
    "        if vocab is None and split == 'train':\n",
    "            token_lists = [tokenize_caption(c) for _, c in self.entries]\n",
    "            self.vocab = Vocab(min_freq=MIN_FREQ)\n",
    "            self.vocab.build(token_lists)\n",
    "        elif vocab is not None:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.entries[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image_transformed = self.transform(image)\n",
    "        else:\n",
    "            image_transformed = image\n",
    "\n",
    "        if self.return_raw_caption:\n",
    "            return image_transformed, cap, len(cap)\n",
    "\n",
    "        tokens = tokenize_caption(cap)\n",
    "        numeric = [self.vocab.stoi['<start>']] + self.vocab.numericalize(tokens) + [self.vocab.stoi['<end>']]\n",
    "        num_caption = torch.tensor(numeric, dtype=torch.long)\n",
    "        return image_transformed, num_caption, num_caption.size(0)\n",
    "\n",
    "\n",
    "def make_collate_fn(pad_idx):\n",
    "    def collate_fn(batch):\n",
    "        images, caps, lengths = zip(*batch)\n",
    "        images = torch.stack(images, dim=0)\n",
    "        caps_padded = nn.utils.rnn.pad_sequence(\n",
    "            caps, batch_first=True, padding_value=pad_idx\n",
    "        )\n",
    "        return images, caps_padded, lengths\n",
    "    return collate_fn\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=train_transform, split='train')\n",
    "vocab = train_ds.vocab\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "PAD_IDX = vocab.stoi['<pad>']\n",
    "collate_base = make_collate_fn(PAD_IDX)\n",
    "val_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='val')\n",
    "test_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='test')"
   ],
   "id": "e973ef3542cfdadc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fit and plot model functions:",
   "id": "846fe48b7a6b84f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Plot loss:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_acc'], label='Train Accuracy', marker='o')\n",
    "    plt.plot(epochs, history['val_acc'], label='Val Accuracy', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "CEL_base = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
    "\n",
    "def fit_model(\n",
    "    enc, dec,\n",
    "    train_loader, val_loader,\n",
    "    enc_opt, dec_opt,\n",
    "    device,\n",
    "    vocab,\n",
    "    output_dir,\n",
    "    num_epochs,\n",
    "    criterion=CEL_base,\n",
    "    print_every=PRINT_EVERY\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    enc.to(device)\n",
    "    dec.to(device)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    pad_idx = vocab.stoi['<pad>'] if vocab else 0\n",
    "\n",
    "    def compute_accuracy(logits, targets):\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = targets != pad_idx\n",
    "        correct = (preds == targets) & mask\n",
    "        total = mask.sum().item()\n",
    "        return correct.sum().item() / total if total > 0 else 0.0\n",
    "\n",
    "    amp_enabled = (device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=amp_enabled)\n",
    "\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        enc.train(); dec.train()\n",
    "        train_loss_accum, train_acc_accum, steps = 0.0, 0.0, 0\n",
    "\n",
    "        for batch_idx, (images, caps, _) in enumerate(train_loader, 1):\n",
    "            if device.type == \"cuda\":\n",
    "                images = images.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "            else:\n",
    "                images = images.to(device)\n",
    "            caps = caps.to(device, non_blocking=(device.type == \"cuda\"))\n",
    "\n",
    "            if enc_opt: enc_opt.zero_grad(set_to_none=True)\n",
    "            if dec_opt: dec_opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            # ---- Trim to actual max caption length (no extra pad tokens) ----\n",
    "            inp = caps[:, :-1]\n",
    "            tgt = caps[:,  1:]\n",
    "\n",
    "            max_len = (tgt != pad_idx).sum(dim=1).max().item()\n",
    "            if max_len == 0:\n",
    "                continue  # nothing to learn this batch\n",
    "            inp = inp[:, :max_len]\n",
    "            tgt = tgt[:, :max_len]\n",
    "\n",
    "            # ---- Forward (mixed precision) ----\n",
    "            with torch.amp.autocast(\"cuda\", enabled=amp_enabled):\n",
    "                features = enc(images)\n",
    "                logits = dec(features, inp)\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1))\n",
    "\n",
    "            # ---- backward (scaled) ----\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # (optional) grad clipping\n",
    "            if enc_opt:\n",
    "                scaler.unscale_(enc_opt)\n",
    "                torch.nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "            if dec_opt:\n",
    "                scaler.unscale_(dec_opt)\n",
    "                torch.nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "\n",
    "            # ---- step ----\n",
    "            if enc_opt: scaler.step(enc_opt)\n",
    "            if dec_opt: scaler.step(dec_opt)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss_accum += loss.item()\n",
    "            train_acc_accum += compute_accuracy(logits, tgt)\n",
    "            steps += 1\n",
    "            if batch_idx % print_every == 0 or batch_idx == len(train_loader):\n",
    "                print(f\"Epoch {epoch} Train batch {batch_idx}/{len(train_loader)} \"\n",
    "                      f\"Loss={train_loss_accum/steps:.4f} Acc={100*train_acc_accum/steps:.2f}%\")\n",
    "\n",
    "        train_loss = train_loss_accum / steps\n",
    "        train_acc = 100 * train_acc_accum / steps\n",
    "\n",
    "        # --------------- Validate: ---------------\n",
    "\n",
    "        enc.eval(); dec.eval()\n",
    "        val_loss_accum, val_acc_accum, steps = 0.0, 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, caps, _) in enumerate(val_loader, 1):\n",
    "                images, caps = images.to(device), caps.to(device)\n",
    "\n",
    "                # Trim to batch’s true max length\n",
    "                inp = caps[:, :-1]\n",
    "                tgt = caps[:,  1:]\n",
    "                max_len = (tgt != pad_idx).sum(dim=1).max().item()\n",
    "                if max_len == 0:\n",
    "                    continue\n",
    "                inp = inp[:, :max_len]\n",
    "                tgt = tgt[:, :max_len]\n",
    "\n",
    "                # Faster eval with AMP\n",
    "                with torch.amp.autocast(\"cuda\", enabled=amp_enabled):\n",
    "                    features = enc(images)\n",
    "                    logits = dec(features, inp)\n",
    "                    loss = criterion(logits.reshape(-1, logits.size(-1)),\n",
    "                                     tgt.reshape(-1))\n",
    "\n",
    "                val_loss_accum += loss.item()\n",
    "\n",
    "                # Accuracy on trimmed targets\n",
    "                pred_tokens = logits.argmax(dim=2)\n",
    "                mask = tgt != pad_idx\n",
    "                correct = (pred_tokens == tgt) & mask\n",
    "                nonpad = mask.sum().item()\n",
    "                if nonpad > 0:\n",
    "                    val_acc_accum += correct.sum().item() / nonpad\n",
    "                    steps += 1\n",
    "\n",
    "        val_loss = val_loss_accum / steps\n",
    "        val_acc = 100 * val_acc_accum / steps\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} train_loss={train_loss:.4f} train_acc={train_acc:.2f}% \"\n",
    "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.2f}%\")\n",
    "\n",
    "        # --------------- Save model: ---------------\n",
    "\n",
    "        from pathlib import Path\n",
    "\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"encoder_state_dict\": enc.state_dict(),\n",
    "            \"decoder_state_dict\": dec.state_dict(),\n",
    "            \"vocab\": getattr(vocab, \"itos\", vocab),\n",
    "            \"history\": history,\n",
    "            \"enc_optimizer_state_dict\": enc_opt.state_dict() if enc_opt else None,\n",
    "            \"dec_optimizer_state_dict\": dec_opt.state_dict() if dec_opt else None,\n",
    "        }\n",
    "\n",
    "        outdir = Path(output_dir).resolve()\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        ckpt_path = outdir / f\"ckpt_epoch_{epoch:02d}.pth\"\n",
    "        try:\n",
    "            torch.save(ckpt, ckpt_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plot_training_history(history)\n",
    "    return history"
   ],
   "id": "ec53c557ee2c9793",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BLEU evaluation:",
   "id": "5af672ea77318a31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def _build_refs_for_split(ds):\n",
    "    \"\"\"\n",
    "    Collect all reference captions (tokenized) per image *within the split*.\n",
    "    Returns: dict img_name -> list of reference token lists.\n",
    "    \"\"\"\n",
    "    img2refs = {}\n",
    "    for img_name, cap in ds.entries:\n",
    "        img2refs.setdefault(img_name, []).append(tokenize_caption(cap))\n",
    "    return img2refs\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(enc, dec, img_path, transform, vocab, device,\n",
    "                     max_len=MAX_LEN, use_amp=True):\n",
    "    enc.to(device).eval()\n",
    "    dec.to(device).eval()\n",
    "\n",
    "    with Image.open(img_path) as im:\n",
    "        img = im.convert('RGB')\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    dtype = \"cuda\" if device.type == \"cuda\" else None\n",
    "    with torch.amp.autocast(dtype, enabled=(dtype is not None and use_amp)):\n",
    "        feats = enc(x)\n",
    "\n",
    "    # --- ensure encoder output dtype matches decoder params (fix Half vs Float) ---\n",
    "    dec_dtype = next(dec.parameters()).dtype\n",
    "    feats = feats.to(dec_dtype)\n",
    "\n",
    "    start_id = vocab.stoi['<start>']\n",
    "    end_id   = vocab.stoi['<end>']\n",
    "    pad_id   = vocab.stoi['<pad>']\n",
    "\n",
    "    if hasattr(dec, \"sample\") and callable(getattr(dec, \"sample\")):\n",
    "        try:\n",
    "            # Newer signature (with EOS, blocking, etc.)\n",
    "            ids = dec.sample(\n",
    "                feats,\n",
    "                start_id=start_id,\n",
    "                end_id=end_id,\n",
    "                pad_id=pad_id,\n",
    "                max_len=max_len,\n",
    "                no_repeat_ngram_size=3,\n",
    "                temperature=1.0,\n",
    "                top_k=0\n",
    "            )[0].tolist()\n",
    "        except TypeError:\n",
    "            # Older signature: (features, start_id=None, max_len=None)\n",
    "            ids = dec.sample(feats, start_id=start_id, max_len=max_len)[0].tolist()\n",
    "    else:\n",
    "        # Greedy fallback via forward()\n",
    "        generated = torch.tensor([[start_id]], device=device, dtype=torch.long)\n",
    "        ids = []\n",
    "        for _ in range(max_len):\n",
    "            with torch.amp.autocast(dtype, enabled=(dtype is not None and use_amp)):\n",
    "                logits = dec(feats, generated)        # [1,T,V]\n",
    "                next_id = logits[:, -1, :].argmax(-1) # [1]\n",
    "            nid = int(next_id.item())\n",
    "            ids.append(nid)\n",
    "            if nid == end_id:\n",
    "                break\n",
    "            generated = torch.cat([generated, next_id.unsqueeze(1)], dim=1)\n",
    "\n",
    "    # ids -> words\n",
    "    words = []\n",
    "    for idx in ids:\n",
    "        if idx == end_id: break\n",
    "        if idx in (pad_id, start_id): continue\n",
    "        tok = vocab.itos[idx] if idx < len(vocab.itos) else '<unk>'\n",
    "        words.append(tok)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bleu(enc, dec, ds, transform, vocab, device, max_len=MAX_LEN, limit=None, show_examples=0, use_amp=False):\n",
    "    \"\"\"\n",
    "    Runs greedy decoding once per image, compares to all refs for that image,\n",
    "    and returns BLEU-1..4 (corpus-level).\n",
    "    \"\"\"\n",
    "    enc.eval(); dec.eval()\n",
    "\n",
    "    img2refs = _build_refs_for_split(ds)\n",
    "    img_names = list(img2refs.keys())\n",
    "    if limit is not None:\n",
    "        img_names = img_names[:limit]\n",
    "\n",
    "    list_of_references = []   # shape: N x (#refs_i) x (tokens)\n",
    "    hypotheses = []           # shape: N x (tokens)\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img_path = os.path.join(ds.images_dir, img_name)\n",
    "        hyp_text = generate_caption(enc, dec, img_path, transform, vocab, device, max_len=max_len, use_amp=use_amp)\n",
    "        hyp_tok  = tokenize_caption(hyp_text)\n",
    "\n",
    "        references = img2refs[img_name]  # already tokenized (multiple refs)\n",
    "        list_of_references.append(references)\n",
    "        hypotheses.append(hyp_tok)\n",
    "\n",
    "    smooth = SmoothingFunction().method1\n",
    "\n",
    "    # BLEU-1 .. BLEU-4 (corpus)\n",
    "    bleu1 = corpus_bleu(list_of_references, hypotheses, weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=smooth)\n",
    "    bleu2 = corpus_bleu(list_of_references, hypotheses, weights=(0.5, 0.5, 0.0, 0.0), smoothing_function=smooth)\n",
    "    bleu3 = corpus_bleu(list_of_references, hypotheses, weights=(1/3, 1/3, 1/3, 0.0), smoothing_function=smooth)\n",
    "    bleu4 = corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "\n",
    "    # Print a few examples\n",
    "    for i in range(min(show_examples, len(img_names))):\n",
    "        img_path = os.path.join(ds.images_dir, img_names[i])\n",
    "\n",
    "        # show image\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(Image.open(img_path).convert(\"RGB\"))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(img_names[i])\n",
    "        plt.show()\n",
    "\n",
    "        # then print texts\n",
    "        print(\"Hyp:\", \" \".join(hypotheses[i]))\n",
    "        print(\"Ref 1:\", \" \".join(list_of_references[i][0]))\n",
    "        if len(list_of_references[i]) > 1:\n",
    "            print(\"Ref 2:\", \" \".join(list_of_references[i][1]))\n",
    "\n",
    "    return {\n",
    "        \"BLEU-1\": bleu1,\n",
    "        \"BLEU-2\": bleu2,\n",
    "        \"BLEU-3\": bleu3,\n",
    "        \"BLEU-4\": bleu4,\n",
    "        \"num_images\": len(img_names)\n",
    "    }"
   ],
   "id": "3a92276ccdb59eca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1: CNN-RNN",
   "id": "3c2c59171b681b76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Simple CNN->RNN image caption baseline model.\n",
    "\n",
    "Encoder: CNN with transfer learning from ResNet18.\n",
    "\n",
    "Decoder: Text RNN, no transfer learning."
   ],
   "id": "6e0057fff467073f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.fc = nn.Linear(512, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.backbone.eval()\n",
    "        with torch.no_grad():\n",
    "            feat = self.backbone(x)\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        feat = self.fc(feat)\n",
    "        return feat\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.3, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embed  = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.lstm   = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        emb = self.dropout(self.embed(captions))\n",
    "        h0 = self.init_h(features).unsqueeze(0)\n",
    "        c0 = self.init_c(features).unsqueeze(0)\n",
    "        out, _ = self.lstm(emb, (h0, c0))\n",
    "        logits = self.linear(self.dropout(out))\n",
    "        return logits\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "DROPOUT = 0.3\n",
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 1024\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 64\n",
    "DEC_LR = 1e-3\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "OUTPUT_DIR = \"./models_cnn_rnn\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_loader_cnn_rnn = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_base, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader_cnn_rnn   = DataLoader(val_ds,   batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_base, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "CNN_enc = CNNEncoder(EMBED_SIZE)\n",
    "RNN_dec = RNNDecoder(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=vocab.stoi['<pad>']\n",
    ")\n",
    "\n",
    "CNN_enc = CNN_enc.to(DEVICE).to(memory_format=torch.channels_last)\n",
    "RNN_dec = RNN_dec.to(DEVICE)\n",
    "\n",
    "dec_opt = optim.Adam(RNN_dec.parameters(), lr=DEC_LR)"
   ],
   "id": "111163f847a2f0b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cnn_rnn_history = fit_model(\n",
    "    enc=CNN_enc, dec=RNN_dec,\n",
    "    train_loader=train_loader_cnn_rnn, val_loader=val_loader_cnn_rnn,\n",
    "    enc_opt=None, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "b3a713da93b70ed4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2: ViT trained from scratch",
   "id": "82eb4f9c519c051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers trained from scratch.\n",
    "\n",
    "Encoder: Visual transformer (ViT), no transfer learning.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "48aaf7e1f14ee57c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_ch=3, embed_dim=256, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, patch_size=16, num_layers=4, num_heads=4, mlp_dim=512, dropout=0.2, img_size=224):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(in_ch=3, embed_dim=embed_dim, patch_size=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)                     # [B,S,E]\n",
    "        B, S, E = x.shape\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B,1,E]\n",
    "        x = torch.cat([cls_tokens, x], dim=1)       # [B,S+1,E]\n",
    "        x = x + self.pos_embed[:, :S+1, :]          # handle safety if S differs\n",
    "        x = self.drop(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]                               # CLS\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.register_buffer('pe', self._build_pe(max_len, d_model))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_pe(max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)  # [max_len,1,d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        if seq_len > self.pe.size(0):\n",
    "            new_pe = self._build_pe(seq_len, self.d_model).to(self.pe.device)\n",
    "            self.register_buffer('pe', new_pe, persistent=False)\n",
    "        return x + self.pe[:seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerDecoderAdapter(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, nhead=8, num_layers=3,  dim_feedforward=2048, dropout=0.2, max_len=MAX_LEN, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        # --- Embedding ---\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=self.pad_idx)\n",
    "\n",
    "        # --- Positional encoding ---\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len=max_len)\n",
    "\n",
    "        # --- Transformer decoder layers ---\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Output projection ---\n",
    "        self.linear_out = nn.Linear(embed_size, vocab_size, bias=False)\n",
    "\n",
    "        # --- Weight tying ---\n",
    "        self.linear_out.weight = self.embed.weight\n",
    "\n",
    "        # --- Misc ---\n",
    "        self.embed_size = embed_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        return torch.triu(torch.full((sz, sz), float('-inf'), device=device), diagonal=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        device = features.device\n",
    "        B, L = captions.size()\n",
    "        memory = features.unsqueeze(0)                    # [1, B, E]\n",
    "        tgt = self.embed(captions).permute(1, 0, 2)       # [L, B, E]\n",
    "        tgt = self.pos_enc(tgt)                           # [L, B, E]\n",
    "        tgt_mask = self._generate_square_subsequent_mask(L, device)   # [L, L]\n",
    "        tgt_key_padding_mask = (captions == self.pad_idx)                   # [B, L]\n",
    "        out = self.transformer_decoder(\n",
    "            tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )                                                 # [L, B, E]\n",
    "        out = out.permute(1, 0, 2)                        # [B, L, E]\n",
    "        logits = self.linear_out(out)                     # [B, L, V]\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        features,\n",
    "        start_id=None,\n",
    "        end_id=None,\n",
    "        pad_id=None,\n",
    "        max_len=None,\n",
    "        no_repeat_ngram_size=3,\n",
    "        temperature=1.0,\n",
    "        top_k=0,          # 0 = no top-k\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Greedy-ish decoding with:\n",
    "          - EOS-aware early stop\n",
    "          - no-repeat n-gram blocking\n",
    "          - optional temperature and top-k sampling\n",
    "        Works for batch>1.\n",
    "        \"\"\"\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        device = features.device\n",
    "        B = features.size(0)\n",
    "        memory = features.unsqueeze(0)                              # [1,B,E]\n",
    "\n",
    "        if start_id is None:\n",
    "            raise ValueError(\"start_id must be provided\")\n",
    "        if end_id is None:\n",
    "            # fall back to very large id that will never match\n",
    "            end_id = -1\n",
    "        if pad_id is None:\n",
    "            pad_id = end_id  # harmless default\n",
    "\n",
    "        # [B,1]\n",
    "        generated = torch.full((B, 1), start_id, dtype=torch.long, device=device)\n",
    "        finished  = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "\n",
    "        def _ngram_blocking(next_token_ids, seq, n=no_repeat_ngram_size):\n",
    "            if n <= 0 or seq.size(1) < n-1:\n",
    "                return next_token_ids\n",
    "            # build set of existing n-grams for each batch item\n",
    "            blocked = next_token_ids.clone()\n",
    "            for b in range(seq.size(0)):\n",
    "                if finished[b]:\n",
    "                    continue\n",
    "                history = seq[b].tolist()\n",
    "                tails = tuple(history[-(n-1):])  # (n-1)-gram context\n",
    "                # collect all tokens that would repeat an existing n-gram\n",
    "                bad = set()\n",
    "                for i in range(len(history) - (n-1)):\n",
    "                    if tuple(history[i:i+n-1]) == tails and i+n-1 < len(history):\n",
    "                        bad.add(history[i+n-1])\n",
    "                if bad:\n",
    "                    # if our chosen token is in bad set, mark it invalid by setting to PAD\n",
    "                    if int(blocked[b].item()) in bad:\n",
    "                        blocked[b] = pad_id\n",
    "            return blocked\n",
    "\n",
    "        ids_collected = []\n",
    "        for t in range(max_len):\n",
    "            tgt = self.embed(generated).permute(1, 0, 2)           # [T,B,E]\n",
    "            tgt = self.pos_enc(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(tgt.size(0), device)\n",
    "            out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # [T,B,E]\n",
    "            logits = self.linear_out(out[-1])                       # [B,V]\n",
    "\n",
    "            # temperature\n",
    "            if temperature != 1.0:\n",
    "                logits = logits / max(temperature, 1e-6)\n",
    "\n",
    "            # softmax once so we can do top-k if needed\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            if top_k and top_k > 0:\n",
    "                topk_vals, topk_idx = torch.topk(probs, k=min(top_k, probs.size(-1)), dim=-1)\n",
    "                # sample from top-k\n",
    "                next_ids_rel = torch.multinomial(topk_vals, num_samples=1).squeeze(1)   # [B]\n",
    "                next_ids = topk_idx.gather(1, next_ids_rel.unsqueeze(1)).squeeze(1)     # [B]\n",
    "            else:\n",
    "                # greedy\n",
    "                next_ids = probs.argmax(dim=-1)  # [B]\n",
    "\n",
    "            # n-gram blocking (on chosen ids)\n",
    "            if no_repeat_ngram_size and no_repeat_ngram_size > 1:\n",
    "                next_ids = _ngram_blocking(next_ids, generated, no_repeat_ngram_size)\n",
    "\n",
    "            # force EOS if we picked an invalid (blocked) id\n",
    "            next_ids = torch.where(next_ids == pad_id, torch.tensor(end_id, device=device), next_ids)\n",
    "\n",
    "            ids_collected.append(next_ids)\n",
    "            generated = torch.cat([generated, next_ids.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # update finished mask and early-stop if all done\n",
    "            finished |= (next_ids == end_id)\n",
    "            if torch.all(finished):\n",
    "                break\n",
    "\n",
    "        if not ids_collected:\n",
    "            return torch.full((B, 1), end_id, dtype=torch.long, device=device)\n",
    "\n",
    "        return torch.stack(ids_collected, dim=1)  # [B, T]\n",
    "\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VAL_BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "DROPOUT = 0.2\n",
    "EMBED_SIZE = 768\n",
    "ENC_NUM_LAYERS = 6\n",
    "DEC_NUM_LAYERS = 3\n",
    "NUM_HEADS = 8\n",
    "DIM_FEEDFORWARD = 2048\n",
    "\n",
    "OUTPUT_DIR = \"./models_vit_no_tl\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_loader_vit = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_base, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader_vit = DataLoader(val_ds,   batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_base, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "vit_enc = ViTEncoder(\n",
    "    embed_dim=EMBED_SIZE,\n",
    "    patch_size=16,\n",
    "    num_layers=ENC_NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dim=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "vit_dec = TransformerDecoderAdapter(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    nhead=NUM_HEADS,\n",
    "    num_layers=DEC_NUM_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    "    pad_idx=PAD_IDX\n",
    ")\n",
    "\n",
    "vit_enc = vit_enc.to(DEVICE).to(memory_format=torch.channels_last)\n",
    "vit_dec = vit_dec.to(DEVICE)\n",
    "\n",
    "enc_opt = optim.AdamW(vit_enc.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "dec_opt = optim.AdamW(vit_dec.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "\n",
    "CEL_label_smoothing = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)"
   ],
   "id": "7e7192aa12c98b6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vit_history = fit_model(\n",
    "    enc=vit_enc, dec=vit_dec,\n",
    "    train_loader=train_loader_vit, val_loader=val_loader_vit,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab, criterion=CEL_label_smoothing,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "a78d79ac17315f15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 3: Pre-trained ViT as encoder, same text transformer decoder:",
   "id": "f825371530f54d33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers, pre-trained encoder.\n",
    "\n",
    "Encoder: ViT with transfer learning from google/vit-base-patch16-224.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "33697f2d30915e76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "vit_tl_train = transforms.Compose([\n",
    "    transforms.Resize(processor.size[\"height\"]),\n",
    "    transforms.RandomCrop(processor.size[\"height\"]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "])\n",
    "\n",
    "vit_tl_val = transforms.Compose([\n",
    "    transforms.Resize(processor.size[\"height\"]),\n",
    "    transforms.CenterCrop(processor.size[\"height\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "])\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "class ViTEncoderTL(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT encoder with pretrained weights (transfer learning).\n",
    "    Uses CLS token -> projects to `embed_size`.\n",
    "    Fine-tunes the last `trainable_layers` transformer blocks (0 = frozen).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224-in21k\",\n",
    "                 embed_size=768, trainable_layers=0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.vit = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "        hidden = self.vit.config.hidden_size\n",
    "\n",
    "        # freeze all\n",
    "        for p in self.vit.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # unfreeze last N encoder blocks if requested\n",
    "        if trainable_layers > 0:\n",
    "            blocks = self.vit.encoder.layer\n",
    "            for b in blocks[-trainable_layers:]:\n",
    "                for p in b.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "        # optional LN + projection to your decoder size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, embed_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,3,224,224] already resized/normalized by vit_tl_* transforms\n",
    "        out = self.vit(pixel_values=x, output_hidden_states=False)\n",
    "        cls = out.last_hidden_state[:, 0]      # [B, hidden]\n",
    "        feat = self.head(cls)                  # [B, embed_size]\n",
    "        return feat\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VAL_BATCH_SIZE = 32\n",
    "\n",
    "# Use TL transforms here:\n",
    "train_ds_tl = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=vit_tl_train, split='train')\n",
    "vocab_tl = train_ds_tl.vocab\n",
    "PAD_IDX_TL = vocab_tl.stoi['<pad>']\n",
    "collate_tl = make_collate_fn(PAD_IDX_TL)\n",
    "\n",
    "val_ds_tl  = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab_tl, transform=vit_tl_val, split='val')\n",
    "test_ds_tl = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab_tl, transform=vit_tl_val, split='test')\n",
    "\n",
    "train_loader_tl = DataLoader(train_ds_tl, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_tl, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader_tl = DataLoader(val_ds_tl,  batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_tl, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "EMBED_SIZE = 768         # match decoder\n",
    "vit_tl_enc = ViTEncoderTL(\n",
    "    model_name=\"google/vit-base-patch16-224-in21k\",\n",
    "    embed_size=EMBED_SIZE,\n",
    "    trainable_layers=2,     # try 0 (frozen), 2, or 4\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "vit_tl_dec = TransformerDecoderAdapter(\n",
    "    embed_size=EMBED_SIZE, vocab_size=len(vocab_tl),\n",
    "    nhead=8, num_layers=3, dim_feedforward=2048,\n",
    "    dropout=0.2, max_len=MAX_LEN, pad_idx=PAD_IDX_TL\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optimizers: smaller LR for (partially) unfrozen encoder, larger for decoder\n",
    "enc_params = [p for p in vit_tl_enc.parameters() if p.requires_grad]\n",
    "enc_opt = optim.AdamW(enc_params, lr=1e-5, weight_decay=0.01) if enc_params else None\n",
    "dec_opt = optim.AdamW(vit_tl_dec.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "OUTPUT_DIR = \"./models_vit_tl\"\n",
    "\n",
    "CEL_tl = nn.CrossEntropyLoss(ignore_index=PAD_IDX_TL)"
   ],
   "id": "fc04db88975c7d38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vit_tl_history = fit_model(\n",
    "    enc=vit_tl_enc, dec=vit_tl_dec,\n",
    "    train_loader=train_loader_tl, val_loader=val_loader_tl,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE, vocab=vocab_tl, criterion=CEL_tl,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "42d7bbecb40777eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Eval:",
   "id": "ffb5ade2a676abc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cnn_rnn_scores = evaluate_bleu(\n",
    "    enc=CNN_enc, dec=RNN_dec,\n",
    "    ds=test_ds, transform=val_transform,\n",
    "    vocab=vocab, device=DEVICE,\n",
    "    max_len=EVAL_MAX_LEN , limit=None,\n",
    "    show_examples=3, use_amp=False\n",
    ")\n",
    "print(\"CNN→RNN BLEU:\", cnn_rnn_scores)"
   ],
   "id": "7379d3c20ac570ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vit_scores = evaluate_bleu(\n",
    "    enc=vit_enc, dec=vit_dec,\n",
    "    ds=test_ds, transform=val_transform,\n",
    "    vocab=vocab, device=DEVICE,\n",
    "    max_len=EVAL_MAX_LEN , limit=None,\n",
    "    show_examples=3, use_amp=True\n",
    ")\n",
    "print(\"ViT→Transformer BLEU:\", vit_scores)"
   ],
   "id": "879fd002f7c8450c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vit_tl_scores = evaluate_bleu(\n",
    "    enc=vit_tl_enc, dec=vit_tl_dec,\n",
    "    ds=test_ds_tl, transform=vit_tl_val,\n",
    "    vocab=vocab_tl, device=DEVICE,\n",
    "    max_len=EVAL_MAX_LEN , limit=None,\n",
    "    show_examples=3, use_amp=True\n",
    ")\n",
    "print(\"TL ViT→Transformer BLEU:\", vit_tl_scores)"
   ],
   "id": "1d251447d880a9e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
