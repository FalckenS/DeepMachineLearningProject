{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Machine Learning Project (SSY340)\n",
    "\n",
    "Project Group 92"
   ],
   "id": "755ce3c68a828da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment setup:",
   "id": "7ab7239d695adf25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports and CUDA setup:",
   "id": "6ac8efba5d72a9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T13:37:00.571906Z",
     "start_time": "2025-10-21T13:36:54.924326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, math, random, torch, gc, ast, re\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "PIN_MEMORY = True\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Method for clearing cache and GPU memory\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "2a1126f1708db314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\python312\\lib\\site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
      "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (2.8.0+cu129)\n",
      "Requirement already satisfied: torchvision in c:\\python312\\lib\\site-packages (0.23.0+cu129)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python312\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "PyTorch: 2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA version: 12.9\n",
      "Device name: NVIDIA GeForce RTX 2070\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Constants:",
   "id": "33e675f07eca55ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T13:45:58.816411Z",
     "start_time": "2025-10-21T13:45:58.812404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "IMAGES_DIR = \"flickr30k-images\"\n",
    "CSV_PATH = \"flickr_annotations_30k.csv\"\n",
    "CAPTIONS_FILE = \"Flickr30k.token.txt\"\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "MIN_FREQ = 5 # Minimum frequency for vocab, lower value means slower training but bigger vocabulary\n",
    "MAX_LEN = 100\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1"
   ],
   "id": "6970da4f76792d23",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Flickr30k.token.txt (captions):",
   "id": "50c62e26d9f6e174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _get_image_name(row, df):\n",
    "    for col in ('file_name','filename','image','img','image_filename','img_name','image_name','img_id','image_id','path'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            return os.path.basename(str(val))\n",
    "    return f\"{row.name}.jpg\"\n",
    "\n",
    "def _get_captions(row, df):\n",
    "    for col in ('raw','captions','sentences','sentence','caption','raw_captions','sentids'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return [str(x).strip() for x in val if str(x).strip()]\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, (list, tuple)):\n",
    "                        return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "                    if isinstance(parsed, dict) and 'raw' in parsed:\n",
    "                        r = parsed['raw']\n",
    "                        if isinstance(r, (list, tuple)):\n",
    "                            return [str(x).strip() for x in r if str(x).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                for sep in ('|||', '||', '\\n'):\n",
    "                    if sep in val:\n",
    "                        return [s.strip() for s in val.split(sep) if s.strip()]\n",
    "                return [val.strip()]\n",
    "    return []\n",
    "\n",
    "def generate_token_file_from_csv(csv_path, captions_file):\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(\"CSV columns:\", list(df.columns))\n",
    "    with open(captions_file, 'w', encoding='utf-8') as fout:\n",
    "        for _, row in df.iterrows():\n",
    "            img_name = _get_image_name(row, df)\n",
    "            caps = _get_captions(row, df)\n",
    "            if not caps:\n",
    "                continue\n",
    "            for i, c in enumerate(caps):\n",
    "                fout.write(f\"{img_name}#{i}\\t{c}\\n\")\n",
    "    print(\"Wrote token file:\", captions_file)"
   ],
   "id": "127f788fd6720e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup tokenizer/vocab:",
   "id": "ba2fcb2a0232f7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq=MIN_FREQ, reserved=None):\n",
    "        if reserved is None:\n",
    "            reserved = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        self.min_freq = min_freq\n",
    "        self.reserved = reserved\n",
    "        self.freq = Counter()\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "\n",
    "    def build(self, token_lists):\n",
    "        for t in token_lists:\n",
    "            self.freq.update(t)\n",
    "        self.itos = list(self.reserved)\n",
    "        for tok, cnt in self.freq.most_common():\n",
    "            if cnt < self.min_freq:\n",
    "                continue\n",
    "            if tok in self.reserved:\n",
    "                continue\n",
    "            self.itos.append(tok)\n",
    "        self.stoi = {tok:i for i,tok in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.stoi.get(t, self.stoi['<unk>']) for t in tokens]\n",
    "\n",
    "\n",
    "def tokenize_caption(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ],
   "id": "67cd345b8d58078b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datasets:",
   "id": "cd6c39a10b09d502"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T13:37:18.613691Z",
     "start_time": "2025-10-21T13:37:16.900250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, images_dir, captions_file, vocab=None, transform=None, split='train', seed=SEED, return_raw_caption=False):\n",
    "        super().__init__()\n",
    "        self.images_dir = str(images_dir)\n",
    "        self.transform = transform\n",
    "        self.return_raw_caption = return_raw_caption\n",
    "\n",
    "        image_to_captions = defaultdict(list)\n",
    "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                img_token, cap = parts\n",
    "                img_name = img_token.split('#')[0]\n",
    "                image_to_captions[img_name].append(cap)\n",
    "\n",
    "        available = set(os.listdir(self.images_dir))\n",
    "        self.entries = []\n",
    "        for img, caps in image_to_captions.items():\n",
    "            if img not in available:\n",
    "                continue\n",
    "            for c in caps:\n",
    "                self.entries.append((img, c))\n",
    "\n",
    "        images = sorted(list({e[0] for e in self.entries}))\n",
    "        random.Random(seed).shuffle(images)\n",
    "        n_train = int(len(images) * TRAIN_RATIO)\n",
    "        n_val = int(len(images) * VAL_RATIO)\n",
    "        train_images = set(images[:n_train])\n",
    "        val_images = set(images[n_train:n_train+n_val])\n",
    "        test_images = set(images[n_train+n_val:])\n",
    "\n",
    "        if split == 'train':\n",
    "            self.entries = [e for e in self.entries if e[0] in train_images]\n",
    "        elif split == 'val':\n",
    "            self.entries = [e for e in self.entries if e[0] in val_images]\n",
    "        elif split == 'test':\n",
    "            self.entries = [e for e in self.entries if e[0] in test_images]\n",
    "\n",
    "        if vocab is None and split == 'train':\n",
    "            token_lists = [tokenize_caption(c) for _, c in self.entries]\n",
    "            self.vocab = Vocab(min_freq=MIN_FREQ)\n",
    "            self.vocab.build(token_lists)\n",
    "        elif vocab is not None:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.entries[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image_transformed = self.transform(image)\n",
    "        else:\n",
    "            image_transformed = image\n",
    "\n",
    "        if self.return_raw_caption:\n",
    "            return image_transformed, cap, len(cap)\n",
    "\n",
    "        tokens = tokenize_caption(cap)\n",
    "        numeric = [self.vocab.stoi['<start>']] + self.vocab.numericalize(tokens) + [self.vocab.stoi['<end>']]\n",
    "        num_caption = torch.tensor(numeric, dtype=torch.long)\n",
    "        return image_transformed, num_caption, num_caption.size(0)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, caps = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    lengths = [c.size(0) for c in caps]\n",
    "    caps_padded = nn.utils.rnn.pad_sequence(caps, batch_first=True, padding_value=PAD_IDX)\n",
    "    return images, caps_padded, lengths\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=train_transform, split='train')\n",
    "vocab = train_ds.vocab\n",
    "val_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='val')\n",
    "test_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='test')"
   ],
   "id": "e973ef3542cfdadc",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training utilities:",
   "id": "f804f60af8e30afc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T13:37:20.183453Z",
     "start_time": "2025-10-21T13:37:20.171892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(enc, dec, loader, criterion, enc_opt, dec_opt, device):\n",
    "    enc.train(); dec.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, caps, lengths in loader:\n",
    "        images = images.to(device)\n",
    "        caps = caps.to(device)\n",
    "\n",
    "        feats = enc(images)\n",
    "\n",
    "        caps_input = caps[:, :-1]\n",
    "        caps_input = caps_input[:, :dec.max_len]\n",
    "        logits = dec(feats, caps_input)\n",
    "\n",
    "        targets = caps[:, 1:]\n",
    "        targets = targets[:, :dec.max_len]\n",
    "        logits = logits[:, :targets.size(1), :]\n",
    "\n",
    "        logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
    "        targets_flat = targets.contiguous().view(-1)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "        if enc_opt is not None: enc_opt.zero_grad()\n",
    "        if dec_opt is not None: dec_opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "        if enc_opt: nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "\n",
    "        if enc_opt is not None: enc_opt.step()\n",
    "        if dec_opt is not None: dec_opt.step()\n",
    "\n",
    "        pred_tokens = logits.argmax(dim=2)\n",
    "        mask = targets != 0\n",
    "        correct = (pred_tokens == targets) & mask\n",
    "        correct_tokens += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    return total_loss / max(1, n_batches), 100*correct_tokens/total_tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(enc, dec, loader, criterion, device, vocab=None):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    n_batches = 0\n",
    "    pad_idx = vocab.stoi['<pad>'] if vocab else 0\n",
    "\n",
    "    for batch_idx, (images, caps, _) in enumerate(loader, 1):\n",
    "        images, caps = images.to(device), caps.to(device)\n",
    "        feats = enc(images)\n",
    "\n",
    "        logits = dec(feats, caps[:, :-1])\n",
    "        targets = caps[:, 1:]\n",
    "\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if vocab is not None:\n",
    "            pred_tokens = logits.argmax(dim=2)\n",
    "            mask = targets != pad_idx\n",
    "            correct_tokens += ((pred_tokens == targets) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    avg_acc = 100*correct_tokens/total_tokens if total_tokens > 0 else None\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(enc, dec, img_path, transform, vocab, device, max_len=MAX_LEN):\n",
    "    enc.eval(); dec.eval()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    feats = enc(x)\n",
    "    start_id = vocab.stoi.get('<start>', None)\n",
    "    gen = dec.sample(feats, start_id=start_id, max_len=max_len)\n",
    "    gen = gen[0].cpu().tolist()\n",
    "    words=[]\n",
    "    for idx in gen:\n",
    "        if idx < len(vocab.itos):\n",
    "            tok = vocab.itos[idx]\n",
    "        else:\n",
    "            tok = '<unk>'\n",
    "        if tok == '<end>': break\n",
    "        if tok not in ('<pad>','<start>'):\n",
    "            words.append(tok)\n",
    "    return ' '.join(words)"
   ],
   "id": "704a3a4b8dd6bb2f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fit and plot model functions:",
   "id": "846fe48b7a6b84f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T13:37:22.524674Z",
     "start_time": "2025-10-21T13:37:22.509952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Plot loss:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_acc'], label='Train Accuracy', marker='o')\n",
    "    plt.plot(epochs, history['val_acc'], label='Val Accuracy', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "CEL = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
    "\n",
    "def fit_model(\n",
    "    enc, dec,\n",
    "    train_loader, val_loader,\n",
    "    enc_opt, dec_opt,\n",
    "    device,\n",
    "    vocab,\n",
    "    output_dir,\n",
    "    num_epochs,\n",
    "    criterion=CEL,\n",
    "    print_every=PRINT_EVERY\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    enc.to(device)\n",
    "    dec.to(device)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    pad_idx = vocab.stoi['<pad>'] if vocab else 0\n",
    "\n",
    "    def compute_accuracy(logits, targets):\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = targets != pad_idx\n",
    "        correct = (preds == targets) & mask\n",
    "        total = mask.sum().item()\n",
    "        return correct.sum().item() / total if total > 0 else 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        enc.train(); dec.train()\n",
    "\n",
    "        # --------------- Train: ---------------\n",
    "\n",
    "        train_loss_accum, train_acc_accum, steps = 0.0, 0.0, 0\n",
    "        for batch_idx, (images, caps, _) in enumerate(train_loader, 1):\n",
    "            images, caps = images.to(device), caps.to(device)\n",
    "            features = enc(images)\n",
    "            logits = dec(features, caps[:, :-1])\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "\n",
    "            if enc_opt: enc_opt.zero_grad()\n",
    "            if dec_opt: dec_opt.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            if enc_opt: torch.nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "            if dec_opt: torch.nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "\n",
    "            if enc_opt: enc_opt.step()\n",
    "            if dec_opt: dec_opt.step()\n",
    "\n",
    "            train_loss_accum += loss.item()\n",
    "            train_acc_accum += compute_accuracy(logits, caps[:, 1:])\n",
    "            steps += 1\n",
    "            if batch_idx % print_every == 0 or batch_idx == len(train_loader):\n",
    "                print(f\"Epoch {epoch} Train batch {batch_idx}/{len(train_loader)} \"\n",
    "                      f\"Loss={train_loss_accum/steps:.4f} Acc={100*train_acc_accum/steps:.2f}%\")\n",
    "\n",
    "        train_loss = train_loss_accum / steps\n",
    "        train_acc = 100 * train_acc_accum / steps\n",
    "\n",
    "        # --------------- Validate: ---------------\n",
    "\n",
    "        enc.eval(); dec.eval()\n",
    "        val_loss_accum, val_acc_accum, steps = 0.0, 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, caps, _) in enumerate(val_loader, 1):\n",
    "                images, caps = images.to(device), caps.to(device)\n",
    "                features = enc(images)\n",
    "                logits = dec(features, caps[:, :-1])\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "\n",
    "                val_loss_accum += loss.item()\n",
    "                pred_tokens = logits.argmax(dim=2)\n",
    "                mask = caps[:, 1:] != pad_idx\n",
    "                correct = (pred_tokens == caps[:, 1:]) & mask\n",
    "                nonpad = mask.sum().item()\n",
    "                if nonpad > 0:\n",
    "                    val_acc_accum += correct.sum().item() / nonpad\n",
    "                else:\n",
    "                    # Skip this batch too avoid dividing by zero\n",
    "                    continue\n",
    "                steps += 1\n",
    "\n",
    "        val_loss = val_loss_accum / steps\n",
    "        val_acc = 100 * val_acc_accum / steps\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} train_loss={train_loss:.4f} train_acc={train_acc:.2f}% \"\n",
    "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.2f}%\")\n",
    "\n",
    "        # --------------- Save model: ---------------\n",
    "\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': enc.state_dict(),\n",
    "            'decoder_state_dict': dec.state_dict(),\n",
    "            'vocab': getattr(vocab, 'itos', vocab),\n",
    "            'history': history,\n",
    "            'enc_optimizer_state_dict': enc_opt.state_dict() if enc_opt else None,\n",
    "            'dec_optimizer_state_dict': dec_opt.state_dict() if dec_opt else None,\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(output_dir, f\"ckpt_epoch_{epoch}.pth\"))\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(ckpt, os.path.join(output_dir, \"best.pth\"))\n",
    "\n",
    "    plot_training_history(history)\n",
    "    return history"
   ],
   "id": "ec53c557ee2c9793",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decoder base class:",
   "id": "a8a625cdfdeeb08f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DecoderBase(nn.Module):\n",
    "    def forward(self, features, captions):\n",
    "        raise NotImplementedError\n",
    "    def sample(self, features, start_id=None, max_len=MAX_LEN):\n",
    "        raise NotImplementedError"
   ],
   "id": "c5d9bac225dd4c3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1: CNN-RNN",
   "id": "3c2c59171b681b76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Simple CNN->RNN image caption baseline model.\n",
    "\n",
    "Encoder: CNN with transfer learning from ResNet18.\n",
    "\n",
    "Decoder: Text RNN, no transfer learning."
   ],
   "id": "6e0057fff467073f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, fine_tune=False):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(512, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fine_tune(fine_tune)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        feat = self.fc(feat)\n",
    "        feat = self.relu(feat)\n",
    "        return feat\n",
    "\n",
    "    def fine_tune(self, fine):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = fine\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "class RNNDecoder(DecoderBase):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        h0 = self.init_h(features).unsqueeze(0)\n",
    "        c0 = self.init_c(features).unsqueeze(0)\n",
    "        outputs, _ = self.lstm(embeddings, (h0, c0))\n",
    "        outputs = self.dropout(outputs)\n",
    "        logits = self.linear(outputs)\n",
    "        return logits\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "ENC_LR = 1e-3\n",
    "DEC_LR = 1e-3\n",
    "FINE_TUNE = False\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "DROPOUT = 0.3\n",
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 1024\n",
    "\n",
    "OUTPUT_DIR = \"./models_cnn_rnn\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "enc = CNNEncoder(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    fine_tune=FINE_TUNE\n",
    ")\n",
    "dec = RNNDecoder(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "enc_opt = optim.Adam(enc.parameters(), lr=ENC_LR) if FINE_TUNE else None\n",
    "dec_opt = optim.Adam(dec.parameters(), lr=DEC_LR)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "111163f847a2f0b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "OUTPUT_DIR = \"./models_cnn_rnn\"\n",
    "\n",
    "ckpt_path = os.path.join(OUTPUT_DIR, \"best.pth\")\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
    "\n",
    "enc = CNNEncoder(embed_size=EMBED_SIZE, fine_tune=FINE_TUNE)\n",
    "dec = RNNDecoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=len(vocab), dropout=DROPOUT)\n",
    "\n",
    "enc.load_state_dict(ckpt['encoder_state_dict'])\n",
    "dec.load_state_dict(ckpt['decoder_state_dict'])\n",
    "\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "enc.to(DEVICE)\n",
    "dec.to(DEVICE)\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "references = []\n",
    "candidates = []\n",
    "\n",
    "for idx in range(len(test_ds)):\n",
    "    img, cap, _ = test_ds[idx]\n",
    "    img_path = os.path.join(IMAGES_DIR, test_ds.entries[idx][0])\n",
    "\n",
    "    pred_caption = generate_caption(enc, dec, img_path, val_transform, vocab, DEVICE)\n",
    "\n",
    "    pred_tokens = word_tokenize(pred_caption.lower())\n",
    "    ref_tokens = [word_tokenize(cap.lower())]\n",
    "\n",
    "    candidates.append(pred_tokens)\n",
    "    references.append(ref_tokens)\n",
    "\n",
    "bleu1 = corpus_bleu(references, candidates, weights=(1,0,0,0), smoothing_function=smooth_fn)\n",
    "bleu2 = corpus_bleu(references, candidates, weights=(0.5,0.5,0,0), smoothing_function=smooth_fn)\n",
    "bleu3 = corpus_bleu(references, candidates, weights=(0.33,0.33,0.33,0), smoothing_function=smooth_fn)\n",
    "bleu4 = corpus_bleu(references, candidates, weights=(0.25,0.25,0.25,0.25), smoothing_function=smooth_fn)\n",
    "\n",
    "print(f\"BLEU-1: {bleu1:.4f}, BLEU-2: {bleu2:.4f}, BLEU-3: {bleu3:.4f}, BLEU-4: {bleu4:.4f}\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"GT:\", test_ds.entries[i][1])\n",
    "    print(\"Pred:\", \" \".join(candidates[i]))\n",
    "    print(\"-\"*50)"
   ],
   "id": "b184b28a9d6cdb77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2: ViT trained from scratch",
   "id": "82eb4f9c519c051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers trained from scratch.\n",
    "\n",
    "Encoder: Visual transformer (ViT), no transfer learning.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "48aaf7e1f14ee57c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_ch=3, embed_dim=256, patch_size=16):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 1 == 0\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, patch_size=16, num_layers=4, num_heads=4, mlp_dim=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(in_ch=3, embed_dim=embed_dim, patch_size=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = None\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B, S, E = x.shape\n",
    "        if self.pos_embed is None or self.pos_embed.size(1) != (S+1):\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, S+1, E).to(x.device))\n",
    "            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:, 0]\n",
    "        return cls_out\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerDecoderAdapter(DecoderBase):\n",
    "    def __init__(self, embed_size, vocab_size, nhead=8, num_layers=3, dim_feedforward=2048, dropout=0.2, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len=max_len)\n",
    "        self.feature_proj = nn.Linear(embed_size, embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.linear_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        device = features.device\n",
    "        B, L = captions.size()\n",
    "        memory = self.feature_proj(features).unsqueeze(0)\n",
    "        tgt = self.embed(captions).permute(1, 0, 2)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(L, device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, features, start_id=None, max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        device = features.device\n",
    "        B = features.size(0)\n",
    "        memory = self.feature_proj(features).unsqueeze(0)\n",
    "        generated = torch.full((B, 1), fill_value=start_id, dtype=torch.long, device=device)\n",
    "        ids = []\n",
    "        for t in range(max_len):\n",
    "            tgt = self.embed(generated).permute(1, 0, 2)\n",
    "            tgt = self.pos_enc(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(tgt.size(0), device)\n",
    "            out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "            last = out[-1]\n",
    "            logits = self.linear_out(last)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            ids.append(pred)\n",
    "            generated = torch.cat([generated, pred.unsqueeze(1)], dim=1)\n",
    "        ids = torch.stack(ids, dim=1)\n",
    "        return ids\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "ENC_LR = 5e-5\n",
    "DEC_LR = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "DROPOUT = 0.2\n",
    "EMBED_SIZE = 768\n",
    "ENC_NUM_LAYERS = 6\n",
    "DEC_NUM_LAYERS = 3\n",
    "NUM_HEADS = 8\n",
    "DIM_FEEDFORWARD = 2048\n",
    "\n",
    "OUTPUT_DIR = \"./models_vit_no_tl\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "enc = ViTEncoder(\n",
    "    embed_dim=EMBED_SIZE,\n",
    "    patch_size=16,\n",
    "    num_layers=ENC_NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dim=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "dec = TransformerDecoderAdapter(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    nhead=NUM_HEADS,\n",
    "    num_layers=DEC_NUM_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "enc_opt = optim.Adam(enc.parameters(), lr=ENC_LR)\n",
    "dec_opt = optim.Adam(dec.parameters(), lr=DEC_LR)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "7e7192aa12c98b6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 3: Pre-trained ViT as encoder, same text transformer decoder:",
   "id": "f825371530f54d33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers, pre-trained encoder.\n",
    "\n",
    "Encoder: ViT with transfer learning from google/vit-base-patch16-224.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "33697f2d30915e76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:18:15.972350Z",
     "start_time": "2025-10-21T13:46:08.314435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "IMAGE_PROCESSOR_NAME = \"google/vit-base-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(IMAGE_PROCESSOR_NAME)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    # Leave as PIL, processor will convert to tensors\n",
    "])\n",
    "\n",
    "vit = AutoModel.from_pretrained(IMAGE_PROCESSOR_NAME)\n",
    "for p in vit.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "class EncoderWithProjection(nn.Module):\n",
    "    def __init__(self, vit_model, embed_size):\n",
    "        super().__init__()\n",
    "        self.vit = vit_model\n",
    "        hidden = vit_model.config.hidden_size\n",
    "        self.proj = nn.Linear(hidden, embed_size)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values, return_dict=True)\n",
    "        last = outputs.last_hidden_state\n",
    "        return self.proj(last)\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len, :].to(dtype=x.dtype, device=x.device)\n",
    "\n",
    "\n",
    "class TransformerDecoderAdapter(DecoderBase):\n",
    "    def __init__(self, embed_size, vocab_size, pad_idx=0, nhead=8, num_layers=3, dim_feedforward=2048, dropout=0.2, max_len=50):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len=max_len)\n",
    "        self.feature_proj = nn.Identity()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.linear_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        device = features.device\n",
    "        B, L = captions.size()\n",
    "        memory = self.feature_proj(features).permute(1, 0, 2)\n",
    "        tgt = self.embed(captions).permute(1, 0, 2)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(L, device)\n",
    "        tgt_key_padding_mask = (captions == self.pad_idx)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, features, start_id=None, max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        device = features.device\n",
    "        B = features.size(0)\n",
    "\n",
    "        if features.dim() == 2:\n",
    "            memory = self.feature_proj(features).unsqueeze(0)\n",
    "        elif features.dim() == 3:\n",
    "            memory = self.feature_proj(features).permute(1, 0, 2)\n",
    "\n",
    "        generated = torch.full((B, 1), fill_value=start_id, dtype=torch.long, device=device)\n",
    "        ids = []\n",
    "        for t in range(max_len):\n",
    "            tgt = self.embed(generated).permute(1, 0, 2)\n",
    "            tgt = self.pos_enc(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(tgt.size(0), device)\n",
    "            out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "            last = out[-1]\n",
    "            logits = self.linear_out(last)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            ids.append(pred)\n",
    "            generated = torch.cat([generated, pred.unsqueeze(1)], dim=1)\n",
    "        ids = torch.stack(ids, dim=1)\n",
    "        return ids\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "ENC_LEARNING_RATE = 2e-5\n",
    "DEC_LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "EMBED_SIZE = 768\n",
    "\n",
    "OUTPUT_DIR = \"./models_vit_TL_enc\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=train_transform, split='train')\n",
    "vocab = train_ds.vocab\n",
    "val_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='val')\n",
    "test_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='test')\n",
    "\n",
    "def make_collate_fn(pad_idx, image_processor):\n",
    "    def collate_fn(batch):\n",
    "        images, caps = zip(*batch)\n",
    "        pixel_values = image_processor(images=list(images), return_tensors=\"pt\")['pixel_values']\n",
    "        lengths = [c.size(0) for c in caps]\n",
    "        caps_padded = nn.utils.rnn.pad_sequence(caps, batch_first=True, padding_value=pad_idx)\n",
    "        return pixel_values, caps_padded, lengths\n",
    "    return collate_fn\n",
    "\n",
    "PAD_IDX = vocab.stoi['<pad>']\n",
    "collate_fn = make_collate_fn(PAD_IDX, image_processor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "enc = EncoderWithProjection(\n",
    "    vit,\n",
    "    embed_size=EMBED_SIZE\n",
    ")\n",
    "\n",
    "dec = TransformerDecoderAdapter(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    pad_idx=PAD_IDX,\n",
    "    nhead=8,\n",
    "    num_layers=3,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.2,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "enc_opt = torch.optim.AdamW(enc.parameters(), lr=ENC_LEARNING_RATE, weight_decay=1e-2)\n",
    "dec.linear_out.weight = dec.embed.weight\n",
    "dec_opt = torch.optim.AdamW(dec.parameters(), lr=DEC_LEARNING_RATE, weight_decay=1e-2)\n",
    "CEL = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "fc04db88975c7d38",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 6989\n",
      "Epoch 1 Train batch 10/3877 Loss=145.0011 Acc=1.54%\n",
      "Epoch 1 Train batch 20/3877 Loss=98.7827 Acc=2.26%\n",
      "Epoch 1 Train batch 30/3877 Loss=80.6523 Acc=2.50%\n",
      "Epoch 1 Train batch 40/3877 Loss=70.3881 Acc=2.48%\n",
      "Epoch 1 Train batch 50/3877 Loss=63.4386 Acc=2.73%\n",
      "Epoch 1 Train batch 60/3877 Loss=58.2453 Acc=2.89%\n",
      "Epoch 1 Train batch 70/3877 Loss=54.5016 Acc=2.97%\n",
      "Epoch 1 Train batch 80/3877 Loss=51.3798 Acc=3.09%\n",
      "Epoch 1 Train batch 90/3877 Loss=48.8354 Acc=3.27%\n",
      "Epoch 1 Train batch 100/3877 Loss=46.7316 Acc=3.55%\n",
      "Epoch 1 Train batch 110/3877 Loss=44.9001 Acc=3.78%\n",
      "Epoch 1 Train batch 120/3877 Loss=43.3250 Acc=4.00%\n",
      "Epoch 1 Train batch 130/3877 Loss=41.9054 Acc=4.20%\n",
      "Epoch 1 Train batch 140/3877 Loss=40.6740 Acc=4.43%\n",
      "Epoch 1 Train batch 150/3877 Loss=39.5439 Acc=4.66%\n",
      "Epoch 1 Train batch 160/3877 Loss=38.5444 Acc=4.84%\n",
      "Epoch 1 Train batch 170/3877 Loss=37.6234 Acc=5.09%\n",
      "Epoch 1 Train batch 180/3877 Loss=36.7596 Acc=5.31%\n",
      "Epoch 1 Train batch 190/3877 Loss=35.9839 Acc=5.52%\n",
      "Epoch 1 Train batch 200/3877 Loss=35.2758 Acc=5.71%\n",
      "Epoch 1 Train batch 210/3877 Loss=34.5904 Acc=5.86%\n",
      "Epoch 1 Train batch 220/3877 Loss=34.0006 Acc=6.01%\n",
      "Epoch 1 Train batch 230/3877 Loss=33.4370 Acc=6.13%\n",
      "Epoch 1 Train batch 240/3877 Loss=32.8900 Acc=6.34%\n",
      "Epoch 1 Train batch 250/3877 Loss=32.4038 Acc=6.49%\n",
      "Epoch 1 Train batch 260/3877 Loss=31.9399 Acc=6.62%\n",
      "Epoch 1 Train batch 270/3877 Loss=31.4855 Acc=6.75%\n",
      "Epoch 1 Train batch 280/3877 Loss=31.0529 Acc=6.87%\n",
      "Epoch 1 Train batch 290/3877 Loss=30.6498 Acc=6.98%\n",
      "Epoch 1 Train batch 300/3877 Loss=30.2863 Acc=7.09%\n",
      "Epoch 1 Train batch 310/3877 Loss=29.9273 Acc=7.20%\n",
      "Epoch 1 Train batch 320/3877 Loss=29.5808 Acc=7.30%\n",
      "Epoch 1 Train batch 330/3877 Loss=29.2508 Acc=7.41%\n",
      "Epoch 1 Train batch 340/3877 Loss=28.9250 Acc=7.52%\n",
      "Epoch 1 Train batch 350/3877 Loss=28.6171 Acc=7.64%\n",
      "Epoch 1 Train batch 360/3877 Loss=28.3252 Acc=7.74%\n",
      "Epoch 1 Train batch 370/3877 Loss=28.0479 Acc=7.83%\n",
      "Epoch 1 Train batch 380/3877 Loss=27.7816 Acc=7.92%\n",
      "Epoch 1 Train batch 390/3877 Loss=27.5005 Acc=8.03%\n",
      "Epoch 1 Train batch 400/3877 Loss=27.2469 Acc=8.12%\n",
      "Epoch 1 Train batch 410/3877 Loss=26.9860 Acc=8.21%\n",
      "Epoch 1 Train batch 420/3877 Loss=26.7573 Acc=8.29%\n",
      "Epoch 1 Train batch 430/3877 Loss=26.5317 Acc=8.39%\n",
      "Epoch 1 Train batch 440/3877 Loss=26.3260 Acc=8.48%\n",
      "Epoch 1 Train batch 450/3877 Loss=26.1169 Acc=8.54%\n",
      "Epoch 1 Train batch 460/3877 Loss=25.9021 Acc=8.63%\n",
      "Epoch 1 Train batch 470/3877 Loss=25.7011 Acc=8.73%\n",
      "Epoch 1 Train batch 480/3877 Loss=25.5014 Acc=8.81%\n",
      "Epoch 1 Train batch 490/3877 Loss=25.3210 Acc=8.89%\n",
      "Epoch 1 Train batch 500/3877 Loss=25.1334 Acc=8.95%\n",
      "Epoch 1 Train batch 510/3877 Loss=24.9588 Acc=9.03%\n",
      "Epoch 1 Train batch 520/3877 Loss=24.7760 Acc=9.12%\n",
      "Epoch 1 Train batch 530/3877 Loss=24.6087 Acc=9.19%\n",
      "Epoch 1 Train batch 540/3877 Loss=24.4438 Acc=9.25%\n",
      "Epoch 1 Train batch 550/3877 Loss=24.2766 Acc=9.31%\n",
      "Epoch 1 Train batch 560/3877 Loss=24.1177 Acc=9.36%\n",
      "Epoch 1 Train batch 570/3877 Loss=23.9619 Acc=9.43%\n",
      "Epoch 1 Train batch 580/3877 Loss=23.8165 Acc=9.48%\n",
      "Epoch 1 Train batch 590/3877 Loss=23.6706 Acc=9.53%\n",
      "Epoch 1 Train batch 600/3877 Loss=23.5305 Acc=9.58%\n",
      "Epoch 1 Train batch 610/3877 Loss=23.4002 Acc=9.62%\n",
      "Epoch 1 Train batch 620/3877 Loss=23.2635 Acc=9.68%\n",
      "Epoch 1 Train batch 630/3877 Loss=23.1305 Acc=9.75%\n",
      "Epoch 1 Train batch 640/3877 Loss=22.9951 Acc=9.80%\n",
      "Epoch 1 Train batch 650/3877 Loss=22.8682 Acc=9.86%\n",
      "Epoch 1 Train batch 660/3877 Loss=22.7392 Acc=9.91%\n",
      "Epoch 1 Train batch 670/3877 Loss=22.6181 Acc=9.96%\n",
      "Epoch 1 Train batch 680/3877 Loss=22.4959 Acc=10.02%\n",
      "Epoch 1 Train batch 690/3877 Loss=22.3769 Acc=10.08%\n",
      "Epoch 1 Train batch 700/3877 Loss=22.2519 Acc=10.12%\n",
      "Epoch 1 Train batch 710/3877 Loss=22.1400 Acc=10.16%\n",
      "Epoch 1 Train batch 720/3877 Loss=22.0278 Acc=10.20%\n",
      "Epoch 1 Train batch 730/3877 Loss=21.9244 Acc=10.23%\n",
      "Epoch 1 Train batch 740/3877 Loss=21.8149 Acc=10.29%\n",
      "Epoch 1 Train batch 750/3877 Loss=21.7120 Acc=10.33%\n",
      "Epoch 1 Train batch 760/3877 Loss=21.6061 Acc=10.37%\n",
      "Epoch 1 Train batch 770/3877 Loss=21.5052 Acc=10.42%\n",
      "Epoch 1 Train batch 780/3877 Loss=21.4066 Acc=10.47%\n",
      "Epoch 1 Train batch 790/3877 Loss=21.3076 Acc=10.51%\n",
      "Epoch 1 Train batch 800/3877 Loss=21.2174 Acc=10.54%\n",
      "Epoch 1 Train batch 810/3877 Loss=21.1150 Acc=10.57%\n",
      "Epoch 1 Train batch 820/3877 Loss=21.0238 Acc=10.60%\n",
      "Epoch 1 Train batch 830/3877 Loss=20.9295 Acc=10.66%\n",
      "Epoch 1 Train batch 840/3877 Loss=20.8394 Acc=10.72%\n",
      "Epoch 1 Train batch 850/3877 Loss=20.7540 Acc=10.76%\n",
      "Epoch 1 Train batch 860/3877 Loss=20.6663 Acc=10.80%\n",
      "Epoch 1 Train batch 870/3877 Loss=20.5804 Acc=10.84%\n",
      "Epoch 1 Train batch 880/3877 Loss=20.4897 Acc=10.88%\n",
      "Epoch 1 Train batch 890/3877 Loss=20.4047 Acc=10.92%\n",
      "Epoch 1 Train batch 900/3877 Loss=20.3260 Acc=10.96%\n",
      "Epoch 1 Train batch 910/3877 Loss=20.2419 Acc=11.00%\n",
      "Epoch 1 Train batch 920/3877 Loss=20.1589 Acc=11.03%\n",
      "Epoch 1 Train batch 930/3877 Loss=20.0797 Acc=11.06%\n",
      "Epoch 1 Train batch 940/3877 Loss=19.9964 Acc=11.09%\n",
      "Epoch 1 Train batch 950/3877 Loss=19.9209 Acc=11.12%\n",
      "Epoch 1 Train batch 960/3877 Loss=19.8464 Acc=11.15%\n",
      "Epoch 1 Train batch 970/3877 Loss=19.7744 Acc=11.19%\n",
      "Epoch 1 Train batch 980/3877 Loss=19.7004 Acc=11.22%\n",
      "Epoch 1 Train batch 990/3877 Loss=19.6256 Acc=11.25%\n",
      "Epoch 1 Train batch 1000/3877 Loss=19.5496 Acc=11.30%\n",
      "Epoch 1 Train batch 1010/3877 Loss=19.4758 Acc=11.34%\n",
      "Epoch 1 Train batch 1020/3877 Loss=19.4059 Acc=11.37%\n",
      "Epoch 1 Train batch 1030/3877 Loss=19.3406 Acc=11.39%\n",
      "Epoch 1 Train batch 1040/3877 Loss=19.2712 Acc=11.42%\n",
      "Epoch 1 Train batch 1050/3877 Loss=19.2030 Acc=11.46%\n",
      "Epoch 1 Train batch 1060/3877 Loss=19.1354 Acc=11.48%\n",
      "Epoch 1 Train batch 1070/3877 Loss=19.0678 Acc=11.52%\n",
      "Epoch 1 Train batch 1080/3877 Loss=19.0027 Acc=11.56%\n",
      "Epoch 1 Train batch 1090/3877 Loss=18.9377 Acc=11.60%\n",
      "Epoch 1 Train batch 1100/3877 Loss=18.8751 Acc=11.62%\n",
      "Epoch 1 Train batch 1110/3877 Loss=18.8099 Acc=11.65%\n",
      "Epoch 1 Train batch 1120/3877 Loss=18.7485 Acc=11.69%\n",
      "Epoch 1 Train batch 1130/3877 Loss=18.6867 Acc=11.72%\n",
      "Epoch 1 Train batch 1140/3877 Loss=18.6240 Acc=11.75%\n",
      "Epoch 1 Train batch 1150/3877 Loss=18.5648 Acc=11.78%\n",
      "Epoch 1 Train batch 1160/3877 Loss=18.5015 Acc=11.81%\n",
      "Epoch 1 Train batch 1170/3877 Loss=18.4464 Acc=11.84%\n",
      "Epoch 1 Train batch 1180/3877 Loss=18.3908 Acc=11.86%\n",
      "Epoch 1 Train batch 1190/3877 Loss=18.3320 Acc=11.89%\n",
      "Epoch 1 Train batch 1200/3877 Loss=18.2759 Acc=11.92%\n",
      "Epoch 1 Train batch 1210/3877 Loss=18.2190 Acc=11.94%\n",
      "Epoch 1 Train batch 1220/3877 Loss=18.1624 Acc=11.98%\n",
      "Epoch 1 Train batch 1230/3877 Loss=18.1085 Acc=12.00%\n",
      "Epoch 1 Train batch 1240/3877 Loss=18.0545 Acc=12.02%\n",
      "Epoch 1 Train batch 1250/3877 Loss=18.0020 Acc=12.05%\n",
      "Epoch 1 Train batch 1260/3877 Loss=17.9472 Acc=12.08%\n",
      "Epoch 1 Train batch 1270/3877 Loss=17.8960 Acc=12.10%\n",
      "Epoch 1 Train batch 1280/3877 Loss=17.8458 Acc=12.12%\n",
      "Epoch 1 Train batch 1290/3877 Loss=17.7912 Acc=12.15%\n",
      "Epoch 1 Train batch 1300/3877 Loss=17.7417 Acc=12.17%\n",
      "Epoch 1 Train batch 1310/3877 Loss=17.6903 Acc=12.20%\n",
      "Epoch 1 Train batch 1320/3877 Loss=17.6381 Acc=12.23%\n",
      "Epoch 1 Train batch 1330/3877 Loss=17.5848 Acc=12.27%\n",
      "Epoch 1 Train batch 1340/3877 Loss=17.5337 Acc=12.30%\n",
      "Epoch 1 Train batch 1350/3877 Loss=17.4828 Acc=12.34%\n",
      "Epoch 1 Train batch 1360/3877 Loss=17.4355 Acc=12.36%\n",
      "Epoch 1 Train batch 1370/3877 Loss=17.3887 Acc=12.38%\n",
      "Epoch 1 Train batch 1380/3877 Loss=17.3407 Acc=12.41%\n",
      "Epoch 1 Train batch 1390/3877 Loss=17.2947 Acc=12.43%\n",
      "Epoch 1 Train batch 1400/3877 Loss=17.2459 Acc=12.46%\n",
      "Epoch 1 Train batch 1410/3877 Loss=17.1978 Acc=12.48%\n",
      "Epoch 1 Train batch 1420/3877 Loss=17.1531 Acc=12.51%\n",
      "Epoch 1 Train batch 1430/3877 Loss=17.1066 Acc=12.53%\n",
      "Epoch 1 Train batch 1440/3877 Loss=17.0597 Acc=12.56%\n",
      "Epoch 1 Train batch 1450/3877 Loss=17.0144 Acc=12.59%\n",
      "Epoch 1 Train batch 1460/3877 Loss=16.9687 Acc=12.61%\n",
      "Epoch 1 Train batch 1470/3877 Loss=16.9266 Acc=12.63%\n",
      "Epoch 1 Train batch 1480/3877 Loss=16.8832 Acc=12.66%\n",
      "Epoch 1 Train batch 1490/3877 Loss=16.8410 Acc=12.68%\n",
      "Epoch 1 Train batch 1500/3877 Loss=16.7969 Acc=12.71%\n",
      "Epoch 1 Train batch 1510/3877 Loss=16.7526 Acc=12.74%\n",
      "Epoch 1 Train batch 1520/3877 Loss=16.7103 Acc=12.75%\n",
      "Epoch 1 Train batch 1530/3877 Loss=16.6681 Acc=12.78%\n",
      "Epoch 1 Train batch 1540/3877 Loss=16.6272 Acc=12.80%\n",
      "Epoch 1 Train batch 1550/3877 Loss=16.5853 Acc=12.82%\n",
      "Epoch 1 Train batch 1560/3877 Loss=16.5441 Acc=12.84%\n",
      "Epoch 1 Train batch 1570/3877 Loss=16.5030 Acc=12.87%\n",
      "Epoch 1 Train batch 1580/3877 Loss=16.4657 Acc=12.89%\n",
      "Epoch 1 Train batch 1590/3877 Loss=16.4253 Acc=12.91%\n",
      "Epoch 1 Train batch 1600/3877 Loss=16.3862 Acc=12.93%\n",
      "Epoch 1 Train batch 1610/3877 Loss=16.3483 Acc=12.95%\n",
      "Epoch 1 Train batch 1620/3877 Loss=16.3099 Acc=12.97%\n",
      "Epoch 1 Train batch 1630/3877 Loss=16.2713 Acc=12.99%\n",
      "Epoch 1 Train batch 1640/3877 Loss=16.2319 Acc=13.01%\n",
      "Epoch 1 Train batch 1650/3877 Loss=16.1953 Acc=13.03%\n",
      "Epoch 1 Train batch 1660/3877 Loss=16.1572 Acc=13.05%\n",
      "Epoch 1 Train batch 1670/3877 Loss=16.1196 Acc=13.07%\n",
      "Epoch 1 Train batch 1680/3877 Loss=16.0843 Acc=13.09%\n",
      "Epoch 1 Train batch 1690/3877 Loss=16.0473 Acc=13.11%\n",
      "Epoch 1 Train batch 1700/3877 Loss=16.0131 Acc=13.12%\n",
      "Epoch 1 Train batch 1710/3877 Loss=15.9777 Acc=13.15%\n",
      "Epoch 1 Train batch 1720/3877 Loss=15.9418 Acc=13.17%\n",
      "Epoch 1 Train batch 1730/3877 Loss=15.9055 Acc=13.18%\n",
      "Epoch 1 Train batch 1740/3877 Loss=15.8727 Acc=13.20%\n",
      "Epoch 1 Train batch 1750/3877 Loss=15.8401 Acc=13.21%\n",
      "Epoch 1 Train batch 1760/3877 Loss=15.8046 Acc=13.24%\n",
      "Epoch 1 Train batch 1770/3877 Loss=15.7678 Acc=13.26%\n",
      "Epoch 1 Train batch 1780/3877 Loss=15.7345 Acc=13.28%\n",
      "Epoch 1 Train batch 1790/3877 Loss=15.7006 Acc=13.30%\n",
      "Epoch 1 Train batch 1800/3877 Loss=15.6659 Acc=13.32%\n",
      "Epoch 1 Train batch 1810/3877 Loss=15.6337 Acc=13.33%\n",
      "Epoch 1 Train batch 1820/3877 Loss=15.6010 Acc=13.35%\n",
      "Epoch 1 Train batch 1830/3877 Loss=15.5690 Acc=13.37%\n",
      "Epoch 1 Train batch 1840/3877 Loss=15.5359 Acc=13.38%\n",
      "Epoch 1 Train batch 1850/3877 Loss=15.5035 Acc=13.40%\n",
      "Epoch 1 Train batch 1860/3877 Loss=15.4719 Acc=13.42%\n",
      "Epoch 1 Train batch 1870/3877 Loss=15.4397 Acc=13.45%\n",
      "Epoch 1 Train batch 1880/3877 Loss=15.4068 Acc=13.47%\n",
      "Epoch 1 Train batch 1890/3877 Loss=15.3747 Acc=13.49%\n",
      "Epoch 1 Train batch 1900/3877 Loss=15.3455 Acc=13.51%\n",
      "Epoch 1 Train batch 1910/3877 Loss=15.3154 Acc=13.53%\n",
      "Epoch 1 Train batch 1920/3877 Loss=15.2847 Acc=13.55%\n",
      "Epoch 1 Train batch 1930/3877 Loss=15.2554 Acc=13.58%\n",
      "Epoch 1 Train batch 1940/3877 Loss=15.2250 Acc=13.60%\n",
      "Epoch 1 Train batch 1950/3877 Loss=15.1929 Acc=13.62%\n",
      "Epoch 1 Train batch 1960/3877 Loss=15.1633 Acc=13.64%\n",
      "Epoch 1 Train batch 1970/3877 Loss=15.1341 Acc=13.65%\n",
      "Epoch 1 Train batch 1980/3877 Loss=15.1057 Acc=13.67%\n",
      "Epoch 1 Train batch 1990/3877 Loss=15.0769 Acc=13.68%\n",
      "Epoch 1 Train batch 2000/3877 Loss=15.0474 Acc=13.71%\n",
      "Epoch 1 Train batch 2010/3877 Loss=15.0178 Acc=13.73%\n",
      "Epoch 1 Train batch 2020/3877 Loss=14.9898 Acc=13.75%\n",
      "Epoch 1 Train batch 2030/3877 Loss=14.9602 Acc=13.77%\n",
      "Epoch 1 Train batch 2040/3877 Loss=14.9330 Acc=13.78%\n",
      "Epoch 1 Train batch 2050/3877 Loss=14.9057 Acc=13.79%\n",
      "Epoch 1 Train batch 2060/3877 Loss=14.8773 Acc=13.81%\n",
      "Epoch 1 Train batch 2070/3877 Loss=14.8484 Acc=13.83%\n",
      "Epoch 1 Train batch 2080/3877 Loss=14.8198 Acc=13.85%\n",
      "Epoch 1 Train batch 2090/3877 Loss=14.7928 Acc=13.87%\n",
      "Epoch 1 Train batch 2100/3877 Loss=14.7644 Acc=13.89%\n",
      "Epoch 1 Train batch 2110/3877 Loss=14.7353 Acc=13.92%\n",
      "Epoch 1 Train batch 2120/3877 Loss=14.7063 Acc=13.94%\n",
      "Epoch 1 Train batch 2130/3877 Loss=14.6782 Acc=13.96%\n",
      "Epoch 1 Train batch 2140/3877 Loss=14.6496 Acc=13.98%\n",
      "Epoch 1 Train batch 2150/3877 Loss=14.6224 Acc=14.00%\n",
      "Epoch 1 Train batch 2160/3877 Loss=14.5950 Acc=14.02%\n",
      "Epoch 1 Train batch 2170/3877 Loss=14.5680 Acc=14.04%\n",
      "Epoch 1 Train batch 2180/3877 Loss=14.5417 Acc=14.06%\n",
      "Epoch 1 Train batch 2190/3877 Loss=14.5144 Acc=14.08%\n",
      "Epoch 1 Train batch 2200/3877 Loss=14.4879 Acc=14.10%\n",
      "Epoch 1 Train batch 2210/3877 Loss=14.4613 Acc=14.12%\n",
      "Epoch 1 Train batch 2220/3877 Loss=14.4362 Acc=14.13%\n",
      "Epoch 1 Train batch 2230/3877 Loss=14.4102 Acc=14.15%\n",
      "Epoch 1 Train batch 2240/3877 Loss=14.3851 Acc=14.17%\n",
      "Epoch 1 Train batch 2250/3877 Loss=14.3585 Acc=14.19%\n",
      "Epoch 1 Train batch 2260/3877 Loss=14.3335 Acc=14.21%\n",
      "Epoch 1 Train batch 2270/3877 Loss=14.3088 Acc=14.22%\n",
      "Epoch 1 Train batch 2280/3877 Loss=14.2840 Acc=14.24%\n",
      "Epoch 1 Train batch 2290/3877 Loss=14.2584 Acc=14.26%\n",
      "Epoch 1 Train batch 2300/3877 Loss=14.2332 Acc=14.27%\n",
      "Epoch 1 Train batch 2310/3877 Loss=14.2084 Acc=14.29%\n",
      "Epoch 1 Train batch 2320/3877 Loss=14.1840 Acc=14.31%\n",
      "Epoch 1 Train batch 2330/3877 Loss=14.1597 Acc=14.32%\n",
      "Epoch 1 Train batch 2340/3877 Loss=14.1351 Acc=14.35%\n",
      "Epoch 1 Train batch 2350/3877 Loss=14.1112 Acc=14.37%\n",
      "Epoch 1 Train batch 2360/3877 Loss=14.0864 Acc=14.39%\n",
      "Epoch 1 Train batch 2370/3877 Loss=14.0626 Acc=14.41%\n",
      "Epoch 1 Train batch 2380/3877 Loss=14.0386 Acc=14.42%\n",
      "Epoch 1 Train batch 2390/3877 Loss=14.0140 Acc=14.44%\n",
      "Epoch 1 Train batch 2400/3877 Loss=13.9898 Acc=14.47%\n",
      "Epoch 1 Train batch 2410/3877 Loss=13.9660 Acc=14.48%\n",
      "Epoch 1 Train batch 2420/3877 Loss=13.9428 Acc=14.50%\n",
      "Epoch 1 Train batch 2430/3877 Loss=13.9200 Acc=14.51%\n",
      "Epoch 1 Train batch 2440/3877 Loss=13.8961 Acc=14.53%\n",
      "Epoch 1 Train batch 2450/3877 Loss=13.8730 Acc=14.55%\n",
      "Epoch 1 Train batch 2460/3877 Loss=13.8492 Acc=14.56%\n",
      "Epoch 1 Train batch 2470/3877 Loss=13.8276 Acc=14.58%\n",
      "Epoch 1 Train batch 2480/3877 Loss=13.8056 Acc=14.60%\n",
      "Epoch 1 Train batch 2490/3877 Loss=13.7826 Acc=14.62%\n",
      "Epoch 1 Train batch 2500/3877 Loss=13.7603 Acc=14.63%\n",
      "Epoch 1 Train batch 2510/3877 Loss=13.7379 Acc=14.65%\n",
      "Epoch 1 Train batch 2520/3877 Loss=13.7159 Acc=14.67%\n",
      "Epoch 1 Train batch 2530/3877 Loss=13.6936 Acc=14.68%\n",
      "Epoch 1 Train batch 2540/3877 Loss=13.6720 Acc=14.69%\n",
      "Epoch 1 Train batch 2550/3877 Loss=13.6500 Acc=14.71%\n",
      "Epoch 1 Train batch 2560/3877 Loss=13.6290 Acc=14.73%\n",
      "Epoch 1 Train batch 2570/3877 Loss=13.6075 Acc=14.74%\n",
      "Epoch 1 Train batch 2580/3877 Loss=13.5866 Acc=14.76%\n",
      "Epoch 1 Train batch 2590/3877 Loss=13.5649 Acc=14.77%\n",
      "Epoch 1 Train batch 2600/3877 Loss=13.5429 Acc=14.79%\n",
      "Epoch 1 Train batch 2610/3877 Loss=13.5221 Acc=14.81%\n",
      "Epoch 1 Train batch 2620/3877 Loss=13.5005 Acc=14.82%\n",
      "Epoch 1 Train batch 2630/3877 Loss=13.4795 Acc=14.84%\n",
      "Epoch 1 Train batch 2640/3877 Loss=13.4582 Acc=14.86%\n",
      "Epoch 1 Train batch 2650/3877 Loss=13.4378 Acc=14.88%\n",
      "Epoch 1 Train batch 2660/3877 Loss=13.4170 Acc=14.89%\n",
      "Epoch 1 Train batch 2670/3877 Loss=13.3960 Acc=14.91%\n",
      "Epoch 1 Train batch 2680/3877 Loss=13.3743 Acc=14.93%\n",
      "Epoch 1 Train batch 2690/3877 Loss=13.3550 Acc=14.95%\n",
      "Epoch 1 Train batch 2700/3877 Loss=13.3340 Acc=14.96%\n",
      "Epoch 1 Train batch 2710/3877 Loss=13.3132 Acc=14.98%\n",
      "Epoch 1 Train batch 2720/3877 Loss=13.2925 Acc=15.00%\n",
      "Epoch 1 Train batch 2730/3877 Loss=13.2714 Acc=15.02%\n",
      "Epoch 1 Train batch 2740/3877 Loss=13.2509 Acc=15.03%\n",
      "Epoch 1 Train batch 2750/3877 Loss=13.2311 Acc=15.05%\n",
      "Epoch 1 Train batch 2760/3877 Loss=13.2122 Acc=15.06%\n",
      "Epoch 1 Train batch 2770/3877 Loss=13.1924 Acc=15.08%\n",
      "Epoch 1 Train batch 2780/3877 Loss=13.1724 Acc=15.10%\n",
      "Epoch 1 Train batch 2790/3877 Loss=13.1526 Acc=15.12%\n",
      "Epoch 1 Train batch 2800/3877 Loss=13.1323 Acc=15.14%\n",
      "Epoch 1 Train batch 2810/3877 Loss=13.1128 Acc=15.16%\n",
      "Epoch 1 Train batch 2820/3877 Loss=13.0933 Acc=15.17%\n",
      "Epoch 1 Train batch 2830/3877 Loss=13.0733 Acc=15.19%\n",
      "Epoch 1 Train batch 2840/3877 Loss=13.0547 Acc=15.20%\n",
      "Epoch 1 Train batch 2850/3877 Loss=13.0357 Acc=15.22%\n",
      "Epoch 1 Train batch 2860/3877 Loss=13.0175 Acc=15.24%\n",
      "Epoch 1 Train batch 2870/3877 Loss=12.9983 Acc=15.25%\n",
      "Epoch 1 Train batch 2880/3877 Loss=12.9794 Acc=15.27%\n",
      "Epoch 1 Train batch 2890/3877 Loss=12.9607 Acc=15.29%\n",
      "Epoch 1 Train batch 2900/3877 Loss=12.9421 Acc=15.30%\n",
      "Epoch 1 Train batch 2910/3877 Loss=12.9227 Acc=15.32%\n",
      "Epoch 1 Train batch 2920/3877 Loss=12.9037 Acc=15.34%\n",
      "Epoch 1 Train batch 2930/3877 Loss=12.8843 Acc=15.35%\n",
      "Epoch 1 Train batch 2940/3877 Loss=12.8668 Acc=15.36%\n",
      "Epoch 1 Train batch 2950/3877 Loss=12.8483 Acc=15.38%\n",
      "Epoch 1 Train batch 2960/3877 Loss=12.8298 Acc=15.39%\n",
      "Epoch 1 Train batch 2970/3877 Loss=12.8113 Acc=15.41%\n",
      "Epoch 1 Train batch 2980/3877 Loss=12.7928 Acc=15.42%\n",
      "Epoch 1 Train batch 2990/3877 Loss=12.7741 Acc=15.44%\n",
      "Epoch 1 Train batch 3000/3877 Loss=12.7561 Acc=15.46%\n",
      "Epoch 1 Train batch 3010/3877 Loss=12.7380 Acc=15.47%\n",
      "Epoch 1 Train batch 3020/3877 Loss=12.7197 Acc=15.50%\n",
      "Epoch 1 Train batch 3030/3877 Loss=12.7019 Acc=15.51%\n",
      "Epoch 1 Train batch 3040/3877 Loss=12.6842 Acc=15.52%\n",
      "Epoch 1 Train batch 3050/3877 Loss=12.6668 Acc=15.54%\n",
      "Epoch 1 Train batch 3060/3877 Loss=12.6486 Acc=15.56%\n",
      "Epoch 1 Train batch 3070/3877 Loss=12.6315 Acc=15.57%\n",
      "Epoch 1 Train batch 3080/3877 Loss=12.6145 Acc=15.59%\n",
      "Epoch 1 Train batch 3090/3877 Loss=12.5971 Acc=15.60%\n",
      "Epoch 1 Train batch 3100/3877 Loss=12.5797 Acc=15.62%\n",
      "Epoch 1 Train batch 3110/3877 Loss=12.5617 Acc=15.64%\n",
      "Epoch 1 Train batch 3120/3877 Loss=12.5440 Acc=15.65%\n",
      "Epoch 1 Train batch 3130/3877 Loss=12.5270 Acc=15.67%\n",
      "Epoch 1 Train batch 3140/3877 Loss=12.5100 Acc=15.68%\n",
      "Epoch 1 Train batch 3150/3877 Loss=12.4918 Acc=15.70%\n",
      "Epoch 1 Train batch 3160/3877 Loss=12.4737 Acc=15.72%\n",
      "Epoch 1 Train batch 3170/3877 Loss=12.4564 Acc=15.74%\n",
      "Epoch 1 Train batch 3180/3877 Loss=12.4397 Acc=15.75%\n",
      "Epoch 1 Train batch 3190/3877 Loss=12.4225 Acc=15.77%\n",
      "Epoch 1 Train batch 3200/3877 Loss=12.4054 Acc=15.79%\n",
      "Epoch 1 Train batch 3210/3877 Loss=12.3885 Acc=15.80%\n",
      "Epoch 1 Train batch 3220/3877 Loss=12.3726 Acc=15.81%\n",
      "Epoch 1 Train batch 3230/3877 Loss=12.3558 Acc=15.82%\n",
      "Epoch 1 Train batch 3240/3877 Loss=12.3397 Acc=15.84%\n",
      "Epoch 1 Train batch 3250/3877 Loss=12.3232 Acc=15.86%\n",
      "Epoch 1 Train batch 3260/3877 Loss=12.3068 Acc=15.87%\n",
      "Epoch 1 Train batch 3270/3877 Loss=12.2908 Acc=15.89%\n",
      "Epoch 1 Train batch 3280/3877 Loss=12.2746 Acc=15.90%\n",
      "Epoch 1 Train batch 3290/3877 Loss=12.2592 Acc=15.91%\n",
      "Epoch 1 Train batch 3300/3877 Loss=12.2435 Acc=15.92%\n",
      "Epoch 1 Train batch 3310/3877 Loss=12.2274 Acc=15.94%\n",
      "Epoch 1 Train batch 3320/3877 Loss=12.2108 Acc=15.96%\n",
      "Epoch 1 Train batch 3330/3877 Loss=12.1937 Acc=15.98%\n",
      "Epoch 1 Train batch 3340/3877 Loss=12.1784 Acc=15.99%\n",
      "Epoch 1 Train batch 3350/3877 Loss=12.1622 Acc=16.01%\n",
      "Epoch 1 Train batch 3360/3877 Loss=12.1461 Acc=16.02%\n",
      "Epoch 1 Train batch 3370/3877 Loss=12.1300 Acc=16.04%\n",
      "Epoch 1 Train batch 3380/3877 Loss=12.1143 Acc=16.06%\n",
      "Epoch 1 Train batch 3390/3877 Loss=12.0986 Acc=16.07%\n",
      "Epoch 1 Train batch 3400/3877 Loss=12.0826 Acc=16.09%\n",
      "Epoch 1 Train batch 3410/3877 Loss=12.0672 Acc=16.10%\n",
      "Epoch 1 Train batch 3420/3877 Loss=12.0512 Acc=16.12%\n",
      "Epoch 1 Train batch 3430/3877 Loss=12.0356 Acc=16.13%\n",
      "Epoch 1 Train batch 3440/3877 Loss=12.0199 Acc=16.15%\n",
      "Epoch 1 Train batch 3450/3877 Loss=12.0041 Acc=16.17%\n",
      "Epoch 1 Train batch 3460/3877 Loss=11.9887 Acc=16.18%\n",
      "Epoch 1 Train batch 3470/3877 Loss=11.9725 Acc=16.20%\n",
      "Epoch 1 Train batch 3480/3877 Loss=11.9571 Acc=16.22%\n",
      "Epoch 1 Train batch 3490/3877 Loss=11.9418 Acc=16.23%\n",
      "Epoch 1 Train batch 3500/3877 Loss=11.9272 Acc=16.25%\n",
      "Epoch 1 Train batch 3510/3877 Loss=11.9118 Acc=16.27%\n",
      "Epoch 1 Train batch 3520/3877 Loss=11.8964 Acc=16.28%\n",
      "Epoch 1 Train batch 3530/3877 Loss=11.8811 Acc=16.30%\n",
      "Epoch 1 Train batch 3540/3877 Loss=11.8660 Acc=16.31%\n",
      "Epoch 1 Train batch 3550/3877 Loss=11.8514 Acc=16.33%\n",
      "Epoch 1 Train batch 3560/3877 Loss=11.8358 Acc=16.35%\n",
      "Epoch 1 Train batch 3570/3877 Loss=11.8208 Acc=16.36%\n",
      "Epoch 1 Train batch 3580/3877 Loss=11.8058 Acc=16.38%\n",
      "Epoch 1 Train batch 3590/3877 Loss=11.7904 Acc=16.40%\n",
      "Epoch 1 Train batch 3600/3877 Loss=11.7755 Acc=16.42%\n",
      "Epoch 1 Train batch 3610/3877 Loss=11.7607 Acc=16.43%\n",
      "Epoch 1 Train batch 3620/3877 Loss=11.7459 Acc=16.45%\n",
      "Epoch 1 Train batch 3630/3877 Loss=11.7313 Acc=16.46%\n",
      "Epoch 1 Train batch 3640/3877 Loss=11.7169 Acc=16.48%\n",
      "Epoch 1 Train batch 3650/3877 Loss=11.7022 Acc=16.50%\n",
      "Epoch 1 Train batch 3660/3877 Loss=11.6875 Acc=16.51%\n",
      "Epoch 1 Train batch 3670/3877 Loss=11.6732 Acc=16.53%\n",
      "Epoch 1 Train batch 3680/3877 Loss=11.6590 Acc=16.54%\n",
      "Epoch 1 Train batch 3690/3877 Loss=11.6444 Acc=16.56%\n",
      "Epoch 1 Train batch 3700/3877 Loss=11.6300 Acc=16.57%\n",
      "Epoch 1 Train batch 3710/3877 Loss=11.6160 Acc=16.59%\n",
      "Epoch 1 Train batch 3720/3877 Loss=11.6009 Acc=16.61%\n",
      "Epoch 1 Train batch 3730/3877 Loss=11.5861 Acc=16.63%\n",
      "Epoch 1 Train batch 3740/3877 Loss=11.5721 Acc=16.64%\n",
      "Epoch 1 Train batch 3750/3877 Loss=11.5578 Acc=16.66%\n",
      "Epoch 1 Train batch 3760/3877 Loss=11.5442 Acc=16.67%\n",
      "Epoch 1 Train batch 3770/3877 Loss=11.5302 Acc=16.69%\n",
      "Epoch 1 Train batch 3780/3877 Loss=11.5157 Acc=16.70%\n",
      "Epoch 1 Train batch 3790/3877 Loss=11.5016 Acc=16.72%\n",
      "Epoch 1 Train batch 3800/3877 Loss=11.4880 Acc=16.73%\n",
      "Epoch 1 Train batch 3810/3877 Loss=11.4747 Acc=16.74%\n",
      "Epoch 1 Train batch 3820/3877 Loss=11.4606 Acc=16.76%\n",
      "Epoch 1 Train batch 3830/3877 Loss=11.4473 Acc=16.78%\n",
      "Epoch 1 Train batch 3840/3877 Loss=11.4336 Acc=16.79%\n",
      "Epoch 1 Train batch 3850/3877 Loss=11.4196 Acc=16.81%\n",
      "Epoch 1 Train batch 3860/3877 Loss=11.4059 Acc=16.83%\n",
      "Epoch 1 Train batch 3870/3877 Loss=11.3918 Acc=16.85%\n",
      "Epoch 1 Train batch 3877/3877 Loss=11.3818 Acc=16.86%\n",
      "Epoch 1/5 train_loss=11.3818 train_acc=16.86% val_loss=5.1393 val_acc=31.47%\n",
      "Epoch 2 Train batch 10/3877 Loss=6.0682 Acc=23.38%\n",
      "Epoch 2 Train batch 20/3877 Loss=6.0942 Acc=23.24%\n",
      "Epoch 2 Train batch 30/3877 Loss=6.0615 Acc=23.38%\n",
      "Epoch 2 Train batch 40/3877 Loss=6.0615 Acc=23.52%\n",
      "Epoch 2 Train batch 50/3877 Loss=6.0406 Acc=23.36%\n",
      "Epoch 2 Train batch 60/3877 Loss=6.0126 Acc=23.58%\n",
      "Epoch 2 Train batch 70/3877 Loss=6.0431 Acc=23.44%\n",
      "Epoch 2 Train batch 80/3877 Loss=6.0502 Acc=23.52%\n",
      "Epoch 2 Train batch 90/3877 Loss=6.0415 Acc=23.67%\n",
      "Epoch 2 Train batch 100/3877 Loss=6.0358 Acc=23.61%\n",
      "Epoch 2 Train batch 110/3877 Loss=6.0270 Acc=23.61%\n",
      "Epoch 2 Train batch 120/3877 Loss=6.0200 Acc=23.67%\n",
      "Epoch 2 Train batch 130/3877 Loss=6.0102 Acc=23.65%\n",
      "Epoch 2 Train batch 140/3877 Loss=6.0053 Acc=23.64%\n",
      "Epoch 2 Train batch 150/3877 Loss=6.0124 Acc=23.58%\n",
      "Epoch 2 Train batch 160/3877 Loss=5.9992 Acc=23.56%\n",
      "Epoch 2 Train batch 170/3877 Loss=5.9888 Acc=23.61%\n",
      "Epoch 2 Train batch 180/3877 Loss=5.9867 Acc=23.57%\n",
      "Epoch 2 Train batch 190/3877 Loss=5.9726 Acc=23.58%\n",
      "Epoch 2 Train batch 200/3877 Loss=5.9708 Acc=23.65%\n",
      "Epoch 2 Train batch 210/3877 Loss=5.9623 Acc=23.64%\n",
      "Epoch 2 Train batch 220/3877 Loss=5.9670 Acc=23.67%\n",
      "Epoch 2 Train batch 230/3877 Loss=5.9577 Acc=23.69%\n",
      "Epoch 2 Train batch 240/3877 Loss=5.9510 Acc=23.69%\n",
      "Epoch 2 Train batch 250/3877 Loss=5.9423 Acc=23.70%\n",
      "Epoch 2 Train batch 260/3877 Loss=5.9314 Acc=23.72%\n",
      "Epoch 2 Train batch 270/3877 Loss=5.9387 Acc=23.67%\n",
      "Epoch 2 Train batch 280/3877 Loss=5.9320 Acc=23.68%\n",
      "Epoch 2 Train batch 290/3877 Loss=5.9181 Acc=23.75%\n",
      "Epoch 2 Train batch 300/3877 Loss=5.9116 Acc=23.80%\n",
      "Epoch 2 Train batch 310/3877 Loss=5.8966 Acc=23.83%\n",
      "Epoch 2 Train batch 320/3877 Loss=5.8919 Acc=23.84%\n",
      "Epoch 2 Train batch 330/3877 Loss=5.8842 Acc=23.87%\n",
      "Epoch 2 Train batch 340/3877 Loss=5.8807 Acc=23.90%\n",
      "Epoch 2 Train batch 350/3877 Loss=5.8766 Acc=23.88%\n",
      "Epoch 2 Train batch 360/3877 Loss=5.8717 Acc=23.88%\n",
      "Epoch 2 Train batch 370/3877 Loss=5.8694 Acc=23.89%\n",
      "Epoch 2 Train batch 380/3877 Loss=5.8604 Acc=23.94%\n",
      "Epoch 2 Train batch 390/3877 Loss=5.8536 Acc=23.98%\n",
      "Epoch 2 Train batch 400/3877 Loss=5.8502 Acc=23.97%\n",
      "Epoch 2 Train batch 410/3877 Loss=5.8451 Acc=24.01%\n",
      "Epoch 2 Train batch 420/3877 Loss=5.8387 Acc=24.02%\n",
      "Epoch 2 Train batch 430/3877 Loss=5.8324 Acc=24.06%\n",
      "Epoch 2 Train batch 440/3877 Loss=5.8253 Acc=24.10%\n",
      "Epoch 2 Train batch 450/3877 Loss=5.8209 Acc=24.12%\n",
      "Epoch 2 Train batch 460/3877 Loss=5.8147 Acc=24.17%\n",
      "Epoch 2 Train batch 470/3877 Loss=5.8089 Acc=24.19%\n",
      "Epoch 2 Train batch 480/3877 Loss=5.8049 Acc=24.20%\n",
      "Epoch 2 Train batch 490/3877 Loss=5.7995 Acc=24.23%\n",
      "Epoch 2 Train batch 500/3877 Loss=5.7943 Acc=24.23%\n",
      "Epoch 2 Train batch 510/3877 Loss=5.7905 Acc=24.24%\n",
      "Epoch 2 Train batch 520/3877 Loss=5.7850 Acc=24.26%\n",
      "Epoch 2 Train batch 530/3877 Loss=5.7816 Acc=24.29%\n",
      "Epoch 2 Train batch 540/3877 Loss=5.7734 Acc=24.31%\n",
      "Epoch 2 Train batch 550/3877 Loss=5.7634 Acc=24.36%\n",
      "Epoch 2 Train batch 560/3877 Loss=5.7595 Acc=24.39%\n",
      "Epoch 2 Train batch 570/3877 Loss=5.7521 Acc=24.41%\n",
      "Epoch 2 Train batch 580/3877 Loss=5.7442 Acc=24.43%\n",
      "Epoch 2 Train batch 590/3877 Loss=5.7383 Acc=24.45%\n",
      "Epoch 2 Train batch 600/3877 Loss=5.7325 Acc=24.47%\n",
      "Epoch 2 Train batch 610/3877 Loss=5.7306 Acc=24.50%\n",
      "Epoch 2 Train batch 620/3877 Loss=5.7255 Acc=24.51%\n",
      "Epoch 2 Train batch 630/3877 Loss=5.7190 Acc=24.54%\n",
      "Epoch 2 Train batch 640/3877 Loss=5.7145 Acc=24.56%\n",
      "Epoch 2 Train batch 650/3877 Loss=5.7104 Acc=24.58%\n",
      "Epoch 2 Train batch 660/3877 Loss=5.7081 Acc=24.58%\n",
      "Epoch 2 Train batch 670/3877 Loss=5.7049 Acc=24.58%\n",
      "Epoch 2 Train batch 680/3877 Loss=5.6995 Acc=24.61%\n",
      "Epoch 2 Train batch 690/3877 Loss=5.6958 Acc=24.64%\n",
      "Epoch 2 Train batch 700/3877 Loss=5.6908 Acc=24.65%\n",
      "Epoch 2 Train batch 710/3877 Loss=5.6852 Acc=24.65%\n",
      "Epoch 2 Train batch 720/3877 Loss=5.6777 Acc=24.68%\n",
      "Epoch 2 Train batch 730/3877 Loss=5.6755 Acc=24.68%\n",
      "Epoch 2 Train batch 740/3877 Loss=5.6712 Acc=24.68%\n",
      "Epoch 2 Train batch 750/3877 Loss=5.6651 Acc=24.71%\n",
      "Epoch 2 Train batch 760/3877 Loss=5.6605 Acc=24.73%\n",
      "Epoch 2 Train batch 770/3877 Loss=5.6537 Acc=24.75%\n",
      "Epoch 2 Train batch 780/3877 Loss=5.6474 Acc=24.78%\n",
      "Epoch 2 Train batch 790/3877 Loss=5.6423 Acc=24.79%\n",
      "Epoch 2 Train batch 800/3877 Loss=5.6354 Acc=24.82%\n",
      "Epoch 2 Train batch 810/3877 Loss=5.6313 Acc=24.81%\n",
      "Epoch 2 Train batch 820/3877 Loss=5.6278 Acc=24.84%\n",
      "Epoch 2 Train batch 830/3877 Loss=5.6244 Acc=24.85%\n",
      "Epoch 2 Train batch 840/3877 Loss=5.6203 Acc=24.87%\n",
      "Epoch 2 Train batch 850/3877 Loss=5.6145 Acc=24.89%\n",
      "Epoch 2 Train batch 860/3877 Loss=5.6099 Acc=24.89%\n",
      "Epoch 2 Train batch 870/3877 Loss=5.6054 Acc=24.90%\n",
      "Epoch 2 Train batch 880/3877 Loss=5.6003 Acc=24.93%\n",
      "Epoch 2 Train batch 890/3877 Loss=5.5949 Acc=24.93%\n",
      "Epoch 2 Train batch 900/3877 Loss=5.5891 Acc=24.94%\n",
      "Epoch 2 Train batch 910/3877 Loss=5.5838 Acc=24.97%\n",
      "Epoch 2 Train batch 920/3877 Loss=5.5785 Acc=24.99%\n",
      "Epoch 2 Train batch 930/3877 Loss=5.5730 Acc=25.02%\n",
      "Epoch 2 Train batch 940/3877 Loss=5.5679 Acc=25.04%\n",
      "Epoch 2 Train batch 950/3877 Loss=5.5633 Acc=25.06%\n",
      "Epoch 2 Train batch 960/3877 Loss=5.5569 Acc=25.09%\n",
      "Epoch 2 Train batch 970/3877 Loss=5.5516 Acc=25.11%\n",
      "Epoch 2 Train batch 980/3877 Loss=5.5462 Acc=25.13%\n",
      "Epoch 2 Train batch 990/3877 Loss=5.5412 Acc=25.15%\n",
      "Epoch 2 Train batch 1000/3877 Loss=5.5375 Acc=25.16%\n",
      "Epoch 2 Train batch 1010/3877 Loss=5.5344 Acc=25.18%\n",
      "Epoch 2 Train batch 1020/3877 Loss=5.5298 Acc=25.20%\n",
      "Epoch 2 Train batch 1030/3877 Loss=5.5244 Acc=25.22%\n",
      "Epoch 2 Train batch 1040/3877 Loss=5.5184 Acc=25.24%\n",
      "Epoch 2 Train batch 1050/3877 Loss=5.5121 Acc=25.27%\n",
      "Epoch 2 Train batch 1060/3877 Loss=5.5084 Acc=25.28%\n",
      "Epoch 2 Train batch 1070/3877 Loss=5.5040 Acc=25.29%\n",
      "Epoch 2 Train batch 1080/3877 Loss=5.4981 Acc=25.32%\n",
      "Epoch 2 Train batch 1090/3877 Loss=5.4930 Acc=25.34%\n",
      "Epoch 2 Train batch 1100/3877 Loss=5.4892 Acc=25.34%\n",
      "Epoch 2 Train batch 1110/3877 Loss=5.4834 Acc=25.37%\n",
      "Epoch 2 Train batch 1120/3877 Loss=5.4784 Acc=25.39%\n",
      "Epoch 2 Train batch 1130/3877 Loss=5.4759 Acc=25.39%\n",
      "Epoch 2 Train batch 1140/3877 Loss=5.4713 Acc=25.40%\n",
      "Epoch 2 Train batch 1150/3877 Loss=5.4678 Acc=25.41%\n",
      "Epoch 2 Train batch 1160/3877 Loss=5.4621 Acc=25.43%\n",
      "Epoch 2 Train batch 1170/3877 Loss=5.4573 Acc=25.45%\n",
      "Epoch 2 Train batch 1180/3877 Loss=5.4530 Acc=25.47%\n",
      "Epoch 2 Train batch 1190/3877 Loss=5.4484 Acc=25.48%\n",
      "Epoch 2 Train batch 1200/3877 Loss=5.4437 Acc=25.51%\n",
      "Epoch 2 Train batch 1210/3877 Loss=5.4401 Acc=25.52%\n",
      "Epoch 2 Train batch 1220/3877 Loss=5.4357 Acc=25.54%\n",
      "Epoch 2 Train batch 1230/3877 Loss=5.4322 Acc=25.54%\n",
      "Epoch 2 Train batch 1240/3877 Loss=5.4286 Acc=25.56%\n",
      "Epoch 2 Train batch 1250/3877 Loss=5.4231 Acc=25.57%\n",
      "Epoch 2 Train batch 1260/3877 Loss=5.4180 Acc=25.60%\n",
      "Epoch 2 Train batch 1270/3877 Loss=5.4129 Acc=25.62%\n",
      "Epoch 2 Train batch 1280/3877 Loss=5.4077 Acc=25.65%\n",
      "Epoch 2 Train batch 1290/3877 Loss=5.4036 Acc=25.66%\n",
      "Epoch 2 Train batch 1300/3877 Loss=5.3991 Acc=25.68%\n",
      "Epoch 2 Train batch 1310/3877 Loss=5.3953 Acc=25.69%\n",
      "Epoch 2 Train batch 1320/3877 Loss=5.3912 Acc=25.70%\n",
      "Epoch 2 Train batch 1330/3877 Loss=5.3863 Acc=25.72%\n",
      "Epoch 2 Train batch 1340/3877 Loss=5.3835 Acc=25.73%\n",
      "Epoch 2 Train batch 1350/3877 Loss=5.3794 Acc=25.75%\n",
      "Epoch 2 Train batch 1360/3877 Loss=5.3751 Acc=25.77%\n",
      "Epoch 2 Train batch 1370/3877 Loss=5.3704 Acc=25.78%\n",
      "Epoch 2 Train batch 1380/3877 Loss=5.3659 Acc=25.80%\n",
      "Epoch 2 Train batch 1390/3877 Loss=5.3610 Acc=25.82%\n",
      "Epoch 2 Train batch 1400/3877 Loss=5.3574 Acc=25.83%\n",
      "Epoch 2 Train batch 1410/3877 Loss=5.3531 Acc=25.85%\n",
      "Epoch 2 Train batch 1420/3877 Loss=5.3477 Acc=25.88%\n",
      "Epoch 2 Train batch 1430/3877 Loss=5.3440 Acc=25.89%\n",
      "Epoch 2 Train batch 1440/3877 Loss=5.3392 Acc=25.91%\n",
      "Epoch 2 Train batch 1450/3877 Loss=5.3353 Acc=25.93%\n",
      "Epoch 2 Train batch 1460/3877 Loss=5.3320 Acc=25.95%\n",
      "Epoch 2 Train batch 1470/3877 Loss=5.3282 Acc=25.97%\n",
      "Epoch 2 Train batch 1480/3877 Loss=5.3232 Acc=26.00%\n",
      "Epoch 2 Train batch 1490/3877 Loss=5.3191 Acc=26.01%\n",
      "Epoch 2 Train batch 1500/3877 Loss=5.3148 Acc=26.04%\n",
      "Epoch 2 Train batch 1510/3877 Loss=5.3095 Acc=26.06%\n",
      "Epoch 2 Train batch 1520/3877 Loss=5.3042 Acc=26.07%\n",
      "Epoch 2 Train batch 1530/3877 Loss=5.2999 Acc=26.08%\n",
      "Epoch 2 Train batch 1540/3877 Loss=5.2952 Acc=26.10%\n",
      "Epoch 2 Train batch 1550/3877 Loss=5.2914 Acc=26.11%\n",
      "Epoch 2 Train batch 1560/3877 Loss=5.2882 Acc=26.13%\n",
      "Epoch 2 Train batch 1570/3877 Loss=5.2839 Acc=26.14%\n",
      "Epoch 2 Train batch 1580/3877 Loss=5.2796 Acc=26.17%\n",
      "Epoch 2 Train batch 1590/3877 Loss=5.2758 Acc=26.18%\n",
      "Epoch 2 Train batch 1600/3877 Loss=5.2722 Acc=26.20%\n",
      "Epoch 2 Train batch 1610/3877 Loss=5.2679 Acc=26.22%\n",
      "Epoch 2 Train batch 1620/3877 Loss=5.2645 Acc=26.23%\n",
      "Epoch 2 Train batch 1630/3877 Loss=5.2608 Acc=26.25%\n",
      "Epoch 2 Train batch 1640/3877 Loss=5.2569 Acc=26.26%\n",
      "Epoch 2 Train batch 1650/3877 Loss=5.2543 Acc=26.27%\n",
      "Epoch 2 Train batch 1660/3877 Loss=5.2510 Acc=26.29%\n",
      "Epoch 2 Train batch 1670/3877 Loss=5.2469 Acc=26.30%\n",
      "Epoch 2 Train batch 1680/3877 Loss=5.2432 Acc=26.32%\n",
      "Epoch 2 Train batch 1690/3877 Loss=5.2402 Acc=26.33%\n",
      "Epoch 2 Train batch 1700/3877 Loss=5.2362 Acc=26.34%\n",
      "Epoch 2 Train batch 1710/3877 Loss=5.2316 Acc=26.37%\n",
      "Epoch 2 Train batch 1720/3877 Loss=5.2286 Acc=26.37%\n",
      "Epoch 2 Train batch 1730/3877 Loss=5.2242 Acc=26.38%\n",
      "Epoch 2 Train batch 1740/3877 Loss=5.2198 Acc=26.40%\n",
      "Epoch 2 Train batch 1750/3877 Loss=5.2173 Acc=26.41%\n",
      "Epoch 2 Train batch 1760/3877 Loss=5.2133 Acc=26.43%\n",
      "Epoch 2 Train batch 1770/3877 Loss=5.2095 Acc=26.44%\n",
      "Epoch 2 Train batch 1780/3877 Loss=5.2064 Acc=26.45%\n",
      "Epoch 2 Train batch 1790/3877 Loss=5.2024 Acc=26.47%\n",
      "Epoch 2 Train batch 1800/3877 Loss=5.1987 Acc=26.48%\n",
      "Epoch 2 Train batch 1810/3877 Loss=5.1953 Acc=26.49%\n",
      "Epoch 2 Train batch 1820/3877 Loss=5.1922 Acc=26.50%\n",
      "Epoch 2 Train batch 1830/3877 Loss=5.1892 Acc=26.52%\n",
      "Epoch 2 Train batch 1840/3877 Loss=5.1853 Acc=26.54%\n",
      "Epoch 2 Train batch 1850/3877 Loss=5.1807 Acc=26.55%\n",
      "Epoch 2 Train batch 1860/3877 Loss=5.1776 Acc=26.56%\n",
      "Epoch 2 Train batch 1870/3877 Loss=5.1744 Acc=26.57%\n",
      "Epoch 2 Train batch 1880/3877 Loss=5.1704 Acc=26.59%\n",
      "Epoch 2 Train batch 1890/3877 Loss=5.1673 Acc=26.61%\n",
      "Epoch 2 Train batch 1900/3877 Loss=5.1637 Acc=26.62%\n",
      "Epoch 2 Train batch 1910/3877 Loss=5.1600 Acc=26.64%\n",
      "Epoch 2 Train batch 1920/3877 Loss=5.1564 Acc=26.65%\n",
      "Epoch 2 Train batch 1930/3877 Loss=5.1532 Acc=26.66%\n",
      "Epoch 2 Train batch 1940/3877 Loss=5.1498 Acc=26.68%\n",
      "Epoch 2 Train batch 1950/3877 Loss=5.1459 Acc=26.70%\n",
      "Epoch 2 Train batch 1960/3877 Loss=5.1427 Acc=26.72%\n",
      "Epoch 2 Train batch 1970/3877 Loss=5.1399 Acc=26.73%\n",
      "Epoch 2 Train batch 1980/3877 Loss=5.1362 Acc=26.75%\n",
      "Epoch 2 Train batch 1990/3877 Loss=5.1324 Acc=26.76%\n",
      "Epoch 2 Train batch 2000/3877 Loss=5.1293 Acc=26.78%\n",
      "Epoch 2 Train batch 2010/3877 Loss=5.1253 Acc=26.79%\n",
      "Epoch 2 Train batch 2020/3877 Loss=5.1219 Acc=26.80%\n",
      "Epoch 2 Train batch 2030/3877 Loss=5.1188 Acc=26.82%\n",
      "Epoch 2 Train batch 2040/3877 Loss=5.1158 Acc=26.83%\n",
      "Epoch 2 Train batch 2050/3877 Loss=5.1125 Acc=26.84%\n",
      "Epoch 2 Train batch 2060/3877 Loss=5.1104 Acc=26.85%\n",
      "Epoch 2 Train batch 2070/3877 Loss=5.1070 Acc=26.86%\n",
      "Epoch 2 Train batch 2080/3877 Loss=5.1031 Acc=26.88%\n",
      "Epoch 2 Train batch 2090/3877 Loss=5.1000 Acc=26.89%\n",
      "Epoch 2 Train batch 2100/3877 Loss=5.0964 Acc=26.91%\n",
      "Epoch 2 Train batch 2110/3877 Loss=5.0922 Acc=26.93%\n",
      "Epoch 2 Train batch 2120/3877 Loss=5.0890 Acc=26.95%\n",
      "Epoch 2 Train batch 2130/3877 Loss=5.0853 Acc=26.97%\n",
      "Epoch 2 Train batch 2140/3877 Loss=5.0822 Acc=26.97%\n",
      "Epoch 2 Train batch 2150/3877 Loss=5.0783 Acc=26.99%\n",
      "Epoch 2 Train batch 2160/3877 Loss=5.0746 Acc=27.01%\n",
      "Epoch 2 Train batch 2170/3877 Loss=5.0712 Acc=27.03%\n",
      "Epoch 2 Train batch 2180/3877 Loss=5.0683 Acc=27.04%\n",
      "Epoch 2 Train batch 2190/3877 Loss=5.0655 Acc=27.06%\n",
      "Epoch 2 Train batch 2200/3877 Loss=5.0630 Acc=27.07%\n",
      "Epoch 2 Train batch 2210/3877 Loss=5.0599 Acc=27.09%\n",
      "Epoch 2 Train batch 2220/3877 Loss=5.0565 Acc=27.10%\n",
      "Epoch 2 Train batch 2230/3877 Loss=5.0535 Acc=27.11%\n",
      "Epoch 2 Train batch 2240/3877 Loss=5.0506 Acc=27.12%\n",
      "Epoch 2 Train batch 2250/3877 Loss=5.0474 Acc=27.13%\n",
      "Epoch 2 Train batch 2260/3877 Loss=5.0444 Acc=27.15%\n",
      "Epoch 2 Train batch 2270/3877 Loss=5.0414 Acc=27.16%\n",
      "Epoch 2 Train batch 2280/3877 Loss=5.0388 Acc=27.17%\n",
      "Epoch 2 Train batch 2290/3877 Loss=5.0356 Acc=27.18%\n",
      "Epoch 2 Train batch 2300/3877 Loss=5.0324 Acc=27.19%\n",
      "Epoch 2 Train batch 2310/3877 Loss=5.0293 Acc=27.21%\n",
      "Epoch 2 Train batch 2320/3877 Loss=5.0265 Acc=27.22%\n",
      "Epoch 2 Train batch 2330/3877 Loss=5.0231 Acc=27.24%\n",
      "Epoch 2 Train batch 2340/3877 Loss=5.0199 Acc=27.25%\n",
      "Epoch 2 Train batch 2350/3877 Loss=5.0168 Acc=27.26%\n",
      "Epoch 2 Train batch 2360/3877 Loss=5.0139 Acc=27.28%\n",
      "Epoch 2 Train batch 2370/3877 Loss=5.0101 Acc=27.29%\n",
      "Epoch 2 Train batch 2380/3877 Loss=5.0075 Acc=27.31%\n",
      "Epoch 2 Train batch 2390/3877 Loss=5.0042 Acc=27.32%\n",
      "Epoch 2 Train batch 2400/3877 Loss=5.0012 Acc=27.34%\n",
      "Epoch 2 Train batch 2410/3877 Loss=4.9974 Acc=27.35%\n",
      "Epoch 2 Train batch 2420/3877 Loss=4.9939 Acc=27.37%\n",
      "Epoch 2 Train batch 2430/3877 Loss=4.9910 Acc=27.39%\n",
      "Epoch 2 Train batch 2440/3877 Loss=4.9880 Acc=27.40%\n",
      "Epoch 2 Train batch 2450/3877 Loss=4.9853 Acc=27.41%\n",
      "Epoch 2 Train batch 2460/3877 Loss=4.9823 Acc=27.43%\n",
      "Epoch 2 Train batch 2470/3877 Loss=4.9796 Acc=27.44%\n",
      "Epoch 2 Train batch 2480/3877 Loss=4.9763 Acc=27.46%\n",
      "Epoch 2 Train batch 2490/3877 Loss=4.9732 Acc=27.48%\n",
      "Epoch 2 Train batch 2500/3877 Loss=4.9706 Acc=27.49%\n",
      "Epoch 2 Train batch 2510/3877 Loss=4.9674 Acc=27.50%\n",
      "Epoch 2 Train batch 2520/3877 Loss=4.9646 Acc=27.52%\n",
      "Epoch 2 Train batch 2530/3877 Loss=4.9616 Acc=27.52%\n",
      "Epoch 2 Train batch 2540/3877 Loss=4.9585 Acc=27.53%\n",
      "Epoch 2 Train batch 2550/3877 Loss=4.9556 Acc=27.54%\n",
      "Epoch 2 Train batch 2560/3877 Loss=4.9521 Acc=27.56%\n",
      "Epoch 2 Train batch 2570/3877 Loss=4.9491 Acc=27.57%\n",
      "Epoch 2 Train batch 2580/3877 Loss=4.9466 Acc=27.58%\n",
      "Epoch 2 Train batch 2590/3877 Loss=4.9435 Acc=27.59%\n",
      "Epoch 2 Train batch 2600/3877 Loss=4.9408 Acc=27.60%\n",
      "Epoch 2 Train batch 2610/3877 Loss=4.9382 Acc=27.61%\n",
      "Epoch 2 Train batch 2620/3877 Loss=4.9355 Acc=27.62%\n",
      "Epoch 2 Train batch 2630/3877 Loss=4.9330 Acc=27.64%\n",
      "Epoch 2 Train batch 2640/3877 Loss=4.9301 Acc=27.65%\n",
      "Epoch 2 Train batch 2650/3877 Loss=4.9270 Acc=27.66%\n",
      "Epoch 2 Train batch 2660/3877 Loss=4.9237 Acc=27.68%\n",
      "Epoch 2 Train batch 2670/3877 Loss=4.9213 Acc=27.69%\n",
      "Epoch 2 Train batch 2680/3877 Loss=4.9188 Acc=27.69%\n",
      "Epoch 2 Train batch 2690/3877 Loss=4.9161 Acc=27.71%\n",
      "Epoch 2 Train batch 2700/3877 Loss=4.9132 Acc=27.73%\n",
      "Epoch 2 Train batch 2710/3877 Loss=4.9107 Acc=27.74%\n",
      "Epoch 2 Train batch 2720/3877 Loss=4.9083 Acc=27.75%\n",
      "Epoch 2 Train batch 2730/3877 Loss=4.9055 Acc=27.76%\n",
      "Epoch 2 Train batch 2740/3877 Loss=4.9026 Acc=27.77%\n",
      "Epoch 2 Train batch 2750/3877 Loss=4.9001 Acc=27.78%\n",
      "Epoch 2 Train batch 2760/3877 Loss=4.8969 Acc=27.79%\n",
      "Epoch 2 Train batch 2770/3877 Loss=4.8942 Acc=27.81%\n",
      "Epoch 2 Train batch 2780/3877 Loss=4.8913 Acc=27.83%\n",
      "Epoch 2 Train batch 2790/3877 Loss=4.8882 Acc=27.84%\n",
      "Epoch 2 Train batch 2800/3877 Loss=4.8856 Acc=27.85%\n",
      "Epoch 2 Train batch 2810/3877 Loss=4.8831 Acc=27.86%\n",
      "Epoch 2 Train batch 2820/3877 Loss=4.8807 Acc=27.87%\n",
      "Epoch 2 Train batch 2830/3877 Loss=4.8783 Acc=27.88%\n",
      "Epoch 2 Train batch 2840/3877 Loss=4.8754 Acc=27.89%\n",
      "Epoch 2 Train batch 2850/3877 Loss=4.8726 Acc=27.91%\n",
      "Epoch 2 Train batch 2860/3877 Loss=4.8699 Acc=27.92%\n",
      "Epoch 2 Train batch 2870/3877 Loss=4.8672 Acc=27.93%\n",
      "Epoch 2 Train batch 2880/3877 Loss=4.8644 Acc=27.95%\n",
      "Epoch 2 Train batch 2890/3877 Loss=4.8616 Acc=27.97%\n",
      "Epoch 2 Train batch 2900/3877 Loss=4.8586 Acc=27.99%\n",
      "Epoch 2 Train batch 2910/3877 Loss=4.8563 Acc=28.00%\n",
      "Epoch 2 Train batch 2920/3877 Loss=4.8533 Acc=28.01%\n",
      "Epoch 2 Train batch 2930/3877 Loss=4.8507 Acc=28.03%\n",
      "Epoch 2 Train batch 2940/3877 Loss=4.8481 Acc=28.04%\n",
      "Epoch 2 Train batch 2950/3877 Loss=4.8455 Acc=28.05%\n",
      "Epoch 2 Train batch 2960/3877 Loss=4.8431 Acc=28.06%\n",
      "Epoch 2 Train batch 2970/3877 Loss=4.8405 Acc=28.08%\n",
      "Epoch 2 Train batch 2980/3877 Loss=4.8381 Acc=28.09%\n",
      "Epoch 2 Train batch 2990/3877 Loss=4.8358 Acc=28.10%\n",
      "Epoch 2 Train batch 3000/3877 Loss=4.8332 Acc=28.11%\n",
      "Epoch 2 Train batch 3010/3877 Loss=4.8305 Acc=28.13%\n",
      "Epoch 2 Train batch 3020/3877 Loss=4.8279 Acc=28.14%\n",
      "Epoch 2 Train batch 3030/3877 Loss=4.8252 Acc=28.15%\n",
      "Epoch 2 Train batch 3040/3877 Loss=4.8228 Acc=28.16%\n",
      "Epoch 2 Train batch 3050/3877 Loss=4.8202 Acc=28.17%\n",
      "Epoch 2 Train batch 3060/3877 Loss=4.8175 Acc=28.19%\n",
      "Epoch 2 Train batch 3070/3877 Loss=4.8154 Acc=28.19%\n",
      "Epoch 2 Train batch 3080/3877 Loss=4.8131 Acc=28.20%\n",
      "Epoch 2 Train batch 3090/3877 Loss=4.8106 Acc=28.22%\n",
      "Epoch 2 Train batch 3100/3877 Loss=4.8082 Acc=28.22%\n",
      "Epoch 2 Train batch 3110/3877 Loss=4.8060 Acc=28.23%\n",
      "Epoch 2 Train batch 3120/3877 Loss=4.8032 Acc=28.25%\n",
      "Epoch 2 Train batch 3130/3877 Loss=4.8009 Acc=28.26%\n",
      "Epoch 2 Train batch 3140/3877 Loss=4.7980 Acc=28.27%\n",
      "Epoch 2 Train batch 3150/3877 Loss=4.7958 Acc=28.28%\n",
      "Epoch 2 Train batch 3160/3877 Loss=4.7929 Acc=28.30%\n",
      "Epoch 2 Train batch 3170/3877 Loss=4.7903 Acc=28.31%\n",
      "Epoch 2 Train batch 3180/3877 Loss=4.7877 Acc=28.32%\n",
      "Epoch 2 Train batch 3190/3877 Loss=4.7853 Acc=28.34%\n",
      "Epoch 2 Train batch 3200/3877 Loss=4.7826 Acc=28.35%\n",
      "Epoch 2 Train batch 3210/3877 Loss=4.7802 Acc=28.36%\n",
      "Epoch 2 Train batch 3220/3877 Loss=4.7781 Acc=28.37%\n",
      "Epoch 2 Train batch 3230/3877 Loss=4.7757 Acc=28.38%\n",
      "Epoch 2 Train batch 3240/3877 Loss=4.7732 Acc=28.39%\n",
      "Epoch 2 Train batch 3250/3877 Loss=4.7706 Acc=28.40%\n",
      "Epoch 2 Train batch 3260/3877 Loss=4.7688 Acc=28.41%\n",
      "Epoch 2 Train batch 3270/3877 Loss=4.7665 Acc=28.42%\n",
      "Epoch 2 Train batch 3280/3877 Loss=4.7642 Acc=28.43%\n",
      "Epoch 2 Train batch 3290/3877 Loss=4.7622 Acc=28.44%\n",
      "Epoch 2 Train batch 3300/3877 Loss=4.7601 Acc=28.45%\n",
      "Epoch 2 Train batch 3310/3877 Loss=4.7577 Acc=28.46%\n",
      "Epoch 2 Train batch 3320/3877 Loss=4.7557 Acc=28.47%\n",
      "Epoch 2 Train batch 3330/3877 Loss=4.7533 Acc=28.48%\n",
      "Epoch 2 Train batch 3340/3877 Loss=4.7511 Acc=28.49%\n",
      "Epoch 2 Train batch 3350/3877 Loss=4.7491 Acc=28.50%\n",
      "Epoch 2 Train batch 3360/3877 Loss=4.7465 Acc=28.52%\n",
      "Epoch 2 Train batch 3370/3877 Loss=4.7442 Acc=28.53%\n",
      "Epoch 2 Train batch 3380/3877 Loss=4.7423 Acc=28.53%\n",
      "Epoch 2 Train batch 3390/3877 Loss=4.7403 Acc=28.54%\n",
      "Epoch 2 Train batch 3400/3877 Loss=4.7381 Acc=28.56%\n",
      "Epoch 2 Train batch 3410/3877 Loss=4.7358 Acc=28.57%\n",
      "Epoch 2 Train batch 3420/3877 Loss=4.7336 Acc=28.58%\n",
      "Epoch 2 Train batch 3430/3877 Loss=4.7315 Acc=28.59%\n",
      "Epoch 2 Train batch 3440/3877 Loss=4.7292 Acc=28.60%\n",
      "Epoch 2 Train batch 3450/3877 Loss=4.7272 Acc=28.61%\n",
      "Epoch 2 Train batch 3460/3877 Loss=4.7252 Acc=28.62%\n",
      "Epoch 2 Train batch 3470/3877 Loss=4.7230 Acc=28.63%\n",
      "Epoch 2 Train batch 3480/3877 Loss=4.7207 Acc=28.64%\n",
      "Epoch 2 Train batch 3490/3877 Loss=4.7184 Acc=28.65%\n",
      "Epoch 2 Train batch 3500/3877 Loss=4.7161 Acc=28.66%\n",
      "Epoch 2 Train batch 3510/3877 Loss=4.7135 Acc=28.67%\n",
      "Epoch 2 Train batch 3520/3877 Loss=4.7113 Acc=28.68%\n",
      "Epoch 2 Train batch 3530/3877 Loss=4.7092 Acc=28.69%\n",
      "Epoch 2 Train batch 3540/3877 Loss=4.7071 Acc=28.70%\n",
      "Epoch 2 Train batch 3550/3877 Loss=4.7048 Acc=28.72%\n",
      "Epoch 2 Train batch 3560/3877 Loss=4.7027 Acc=28.72%\n",
      "Epoch 2 Train batch 3570/3877 Loss=4.7007 Acc=28.74%\n",
      "Epoch 2 Train batch 3580/3877 Loss=4.6987 Acc=28.74%\n",
      "Epoch 2 Train batch 3590/3877 Loss=4.6968 Acc=28.75%\n",
      "Epoch 2 Train batch 3600/3877 Loss=4.6946 Acc=28.76%\n",
      "Epoch 2 Train batch 3610/3877 Loss=4.6926 Acc=28.77%\n",
      "Epoch 2 Train batch 3620/3877 Loss=4.6904 Acc=28.78%\n",
      "Epoch 2 Train batch 3630/3877 Loss=4.6882 Acc=28.79%\n",
      "Epoch 2 Train batch 3640/3877 Loss=4.6859 Acc=28.80%\n",
      "Epoch 2 Train batch 3650/3877 Loss=4.6838 Acc=28.81%\n",
      "Epoch 2 Train batch 3660/3877 Loss=4.6819 Acc=28.82%\n",
      "Epoch 2 Train batch 3670/3877 Loss=4.6798 Acc=28.83%\n",
      "Epoch 2 Train batch 3680/3877 Loss=4.6777 Acc=28.84%\n",
      "Epoch 2 Train batch 3690/3877 Loss=4.6756 Acc=28.85%\n",
      "Epoch 2 Train batch 3700/3877 Loss=4.6735 Acc=28.86%\n",
      "Epoch 2 Train batch 3710/3877 Loss=4.6715 Acc=28.87%\n",
      "Epoch 2 Train batch 3720/3877 Loss=4.6694 Acc=28.88%\n",
      "Epoch 2 Train batch 3730/3877 Loss=4.6676 Acc=28.89%\n",
      "Epoch 2 Train batch 3740/3877 Loss=4.6655 Acc=28.90%\n",
      "Epoch 2 Train batch 3750/3877 Loss=4.6633 Acc=28.91%\n",
      "Epoch 2 Train batch 3760/3877 Loss=4.6610 Acc=28.92%\n",
      "Epoch 2 Train batch 3770/3877 Loss=4.6588 Acc=28.94%\n",
      "Epoch 2 Train batch 3780/3877 Loss=4.6567 Acc=28.95%\n",
      "Epoch 2 Train batch 3790/3877 Loss=4.6546 Acc=28.96%\n",
      "Epoch 2 Train batch 3800/3877 Loss=4.6526 Acc=28.97%\n",
      "Epoch 2 Train batch 3810/3877 Loss=4.6506 Acc=28.98%\n",
      "Epoch 2 Train batch 3820/3877 Loss=4.6486 Acc=28.99%\n",
      "Epoch 2 Train batch 3830/3877 Loss=4.6465 Acc=29.00%\n",
      "Epoch 2 Train batch 3840/3877 Loss=4.6445 Acc=29.01%\n",
      "Epoch 2 Train batch 3850/3877 Loss=4.6423 Acc=29.02%\n",
      "Epoch 2 Train batch 3860/3877 Loss=4.6405 Acc=29.03%\n",
      "Epoch 2 Train batch 3870/3877 Loss=4.6388 Acc=29.04%\n",
      "Epoch 2 Train batch 3877/3877 Loss=4.6373 Acc=29.04%\n",
      "Epoch 2/5 train_loss=4.6373 train_acc=29.04% val_loss=3.6889 val_acc=35.29%\n",
      "Epoch 3 Train batch 10/3877 Loss=3.7074 Acc=33.82%\n",
      "Epoch 3 Train batch 20/3877 Loss=3.8016 Acc=33.13%\n",
      "Epoch 3 Train batch 30/3877 Loss=3.7898 Acc=32.91%\n",
      "Epoch 3 Train batch 40/3877 Loss=3.7977 Acc=32.90%\n",
      "Epoch 3 Train batch 50/3877 Loss=3.7971 Acc=32.75%\n",
      "Epoch 3 Train batch 60/3877 Loss=3.7951 Acc=32.73%\n",
      "Epoch 3 Train batch 70/3877 Loss=3.8044 Acc=32.75%\n",
      "Epoch 3 Train batch 80/3877 Loss=3.8053 Acc=32.86%\n",
      "Epoch 3 Train batch 90/3877 Loss=3.8075 Acc=32.80%\n",
      "Epoch 3 Train batch 100/3877 Loss=3.8202 Acc=32.83%\n",
      "Epoch 3 Train batch 110/3877 Loss=3.8201 Acc=32.87%\n",
      "Epoch 3 Train batch 120/3877 Loss=3.8254 Acc=32.95%\n",
      "Epoch 3 Train batch 130/3877 Loss=3.8263 Acc=32.88%\n",
      "Epoch 3 Train batch 140/3877 Loss=3.8309 Acc=32.81%\n",
      "Epoch 3 Train batch 150/3877 Loss=3.8344 Acc=32.77%\n",
      "Epoch 3 Train batch 160/3877 Loss=3.8343 Acc=32.76%\n",
      "Epoch 3 Train batch 170/3877 Loss=3.8307 Acc=32.81%\n",
      "Epoch 3 Train batch 180/3877 Loss=3.8284 Acc=32.80%\n",
      "Epoch 3 Train batch 190/3877 Loss=3.8305 Acc=32.79%\n",
      "Epoch 3 Train batch 200/3877 Loss=3.8300 Acc=32.78%\n",
      "Epoch 3 Train batch 210/3877 Loss=3.8273 Acc=32.82%\n",
      "Epoch 3 Train batch 220/3877 Loss=3.8277 Acc=32.83%\n",
      "Epoch 3 Train batch 230/3877 Loss=3.8224 Acc=32.95%\n",
      "Epoch 3 Train batch 240/3877 Loss=3.8200 Acc=32.97%\n",
      "Epoch 3 Train batch 250/3877 Loss=3.8237 Acc=32.96%\n",
      "Epoch 3 Train batch 260/3877 Loss=3.8180 Acc=33.02%\n",
      "Epoch 3 Train batch 270/3877 Loss=3.8164 Acc=33.04%\n",
      "Epoch 3 Train batch 280/3877 Loss=3.8133 Acc=33.11%\n",
      "Epoch 3 Train batch 290/3877 Loss=3.8132 Acc=33.14%\n",
      "Epoch 3 Train batch 300/3877 Loss=3.8119 Acc=33.14%\n",
      "Epoch 3 Train batch 310/3877 Loss=3.8109 Acc=33.15%\n",
      "Epoch 3 Train batch 320/3877 Loss=3.8091 Acc=33.16%\n",
      "Epoch 3 Train batch 330/3877 Loss=3.8083 Acc=33.14%\n",
      "Epoch 3 Train batch 340/3877 Loss=3.8075 Acc=33.15%\n",
      "Epoch 3 Train batch 350/3877 Loss=3.8051 Acc=33.17%\n",
      "Epoch 3 Train batch 360/3877 Loss=3.8058 Acc=33.15%\n",
      "Epoch 3 Train batch 370/3877 Loss=3.8050 Acc=33.17%\n",
      "Epoch 3 Train batch 380/3877 Loss=3.8065 Acc=33.16%\n",
      "Epoch 3 Train batch 390/3877 Loss=3.8065 Acc=33.15%\n",
      "Epoch 3 Train batch 400/3877 Loss=3.8048 Acc=33.15%\n",
      "Epoch 3 Train batch 410/3877 Loss=3.8038 Acc=33.15%\n",
      "Epoch 3 Train batch 420/3877 Loss=3.8023 Acc=33.15%\n",
      "Epoch 3 Train batch 430/3877 Loss=3.8037 Acc=33.16%\n",
      "Epoch 3 Train batch 440/3877 Loss=3.8042 Acc=33.18%\n",
      "Epoch 3 Train batch 450/3877 Loss=3.7991 Acc=33.22%\n",
      "Epoch 3 Train batch 460/3877 Loss=3.7986 Acc=33.23%\n",
      "Epoch 3 Train batch 470/3877 Loss=3.7977 Acc=33.23%\n",
      "Epoch 3 Train batch 480/3877 Loss=3.7954 Acc=33.26%\n",
      "Epoch 3 Train batch 490/3877 Loss=3.7937 Acc=33.30%\n",
      "Epoch 3 Train batch 500/3877 Loss=3.7910 Acc=33.32%\n",
      "Epoch 3 Train batch 510/3877 Loss=3.7904 Acc=33.34%\n",
      "Epoch 3 Train batch 520/3877 Loss=3.7905 Acc=33.35%\n",
      "Epoch 3 Train batch 530/3877 Loss=3.7893 Acc=33.36%\n",
      "Epoch 3 Train batch 540/3877 Loss=3.7884 Acc=33.37%\n",
      "Epoch 3 Train batch 550/3877 Loss=3.7881 Acc=33.39%\n",
      "Epoch 3 Train batch 560/3877 Loss=3.7878 Acc=33.40%\n",
      "Epoch 3 Train batch 570/3877 Loss=3.7877 Acc=33.42%\n",
      "Epoch 3 Train batch 580/3877 Loss=3.7888 Acc=33.40%\n",
      "Epoch 3 Train batch 590/3877 Loss=3.7879 Acc=33.43%\n",
      "Epoch 3 Train batch 600/3877 Loss=3.7872 Acc=33.45%\n",
      "Epoch 3 Train batch 610/3877 Loss=3.7869 Acc=33.45%\n",
      "Epoch 3 Train batch 620/3877 Loss=3.7861 Acc=33.46%\n",
      "Epoch 3 Train batch 630/3877 Loss=3.7830 Acc=33.50%\n",
      "Epoch 3 Train batch 640/3877 Loss=3.7812 Acc=33.50%\n",
      "Epoch 3 Train batch 650/3877 Loss=3.7804 Acc=33.50%\n",
      "Epoch 3 Train batch 660/3877 Loss=3.7800 Acc=33.50%\n",
      "Epoch 3 Train batch 670/3877 Loss=3.7814 Acc=33.50%\n",
      "Epoch 3 Train batch 680/3877 Loss=3.7808 Acc=33.51%\n",
      "Epoch 3 Train batch 690/3877 Loss=3.7805 Acc=33.50%\n",
      "Epoch 3 Train batch 700/3877 Loss=3.7786 Acc=33.51%\n",
      "Epoch 3 Train batch 710/3877 Loss=3.7790 Acc=33.50%\n",
      "Epoch 3 Train batch 720/3877 Loss=3.7770 Acc=33.52%\n",
      "Epoch 3 Train batch 730/3877 Loss=3.7751 Acc=33.53%\n",
      "Epoch 3 Train batch 740/3877 Loss=3.7725 Acc=33.55%\n",
      "Epoch 3 Train batch 750/3877 Loss=3.7728 Acc=33.56%\n",
      "Epoch 3 Train batch 760/3877 Loss=3.7725 Acc=33.56%\n",
      "Epoch 3 Train batch 770/3877 Loss=3.7707 Acc=33.58%\n",
      "Epoch 3 Train batch 780/3877 Loss=3.7698 Acc=33.59%\n",
      "Epoch 3 Train batch 790/3877 Loss=3.7703 Acc=33.56%\n",
      "Epoch 3 Train batch 800/3877 Loss=3.7710 Acc=33.56%\n",
      "Epoch 3 Train batch 810/3877 Loss=3.7703 Acc=33.56%\n",
      "Epoch 3 Train batch 820/3877 Loss=3.7691 Acc=33.58%\n",
      "Epoch 3 Train batch 830/3877 Loss=3.7691 Acc=33.60%\n",
      "Epoch 3 Train batch 840/3877 Loss=3.7678 Acc=33.61%\n",
      "Epoch 3 Train batch 850/3877 Loss=3.7667 Acc=33.62%\n",
      "Epoch 3 Train batch 860/3877 Loss=3.7664 Acc=33.62%\n",
      "Epoch 3 Train batch 870/3877 Loss=3.7656 Acc=33.62%\n",
      "Epoch 3 Train batch 880/3877 Loss=3.7645 Acc=33.63%\n",
      "Epoch 3 Train batch 890/3877 Loss=3.7637 Acc=33.63%\n",
      "Epoch 3 Train batch 900/3877 Loss=3.7635 Acc=33.63%\n",
      "Epoch 3 Train batch 910/3877 Loss=3.7625 Acc=33.63%\n",
      "Epoch 3 Train batch 920/3877 Loss=3.7631 Acc=33.63%\n",
      "Epoch 3 Train batch 930/3877 Loss=3.7634 Acc=33.62%\n",
      "Epoch 3 Train batch 940/3877 Loss=3.7628 Acc=33.62%\n",
      "Epoch 3 Train batch 950/3877 Loss=3.7623 Acc=33.61%\n",
      "Epoch 3 Train batch 960/3877 Loss=3.7605 Acc=33.62%\n",
      "Epoch 3 Train batch 970/3877 Loss=3.7593 Acc=33.64%\n",
      "Epoch 3 Train batch 980/3877 Loss=3.7583 Acc=33.66%\n",
      "Epoch 3 Train batch 990/3877 Loss=3.7580 Acc=33.66%\n",
      "Epoch 3 Train batch 1000/3877 Loss=3.7574 Acc=33.67%\n",
      "Epoch 3 Train batch 1010/3877 Loss=3.7570 Acc=33.67%\n",
      "Epoch 3 Train batch 1020/3877 Loss=3.7560 Acc=33.68%\n",
      "Epoch 3 Train batch 1030/3877 Loss=3.7570 Acc=33.68%\n",
      "Epoch 3 Train batch 1040/3877 Loss=3.7567 Acc=33.67%\n",
      "Epoch 3 Train batch 1050/3877 Loss=3.7556 Acc=33.68%\n",
      "Epoch 3 Train batch 1060/3877 Loss=3.7551 Acc=33.68%\n",
      "Epoch 3 Train batch 1070/3877 Loss=3.7560 Acc=33.67%\n",
      "Epoch 3 Train batch 1080/3877 Loss=3.7562 Acc=33.66%\n",
      "Epoch 3 Train batch 1090/3877 Loss=3.7571 Acc=33.65%\n",
      "Epoch 3 Train batch 1100/3877 Loss=3.7562 Acc=33.66%\n",
      "Epoch 3 Train batch 1110/3877 Loss=3.7562 Acc=33.66%\n",
      "Epoch 3 Train batch 1120/3877 Loss=3.7554 Acc=33.67%\n",
      "Epoch 3 Train batch 1130/3877 Loss=3.7553 Acc=33.67%\n",
      "Epoch 3 Train batch 1140/3877 Loss=3.7555 Acc=33.67%\n",
      "Epoch 3 Train batch 1150/3877 Loss=3.7547 Acc=33.68%\n",
      "Epoch 3 Train batch 1160/3877 Loss=3.7541 Acc=33.68%\n",
      "Epoch 3 Train batch 1170/3877 Loss=3.7527 Acc=33.69%\n",
      "Epoch 3 Train batch 1180/3877 Loss=3.7517 Acc=33.69%\n",
      "Epoch 3 Train batch 1190/3877 Loss=3.7506 Acc=33.71%\n",
      "Epoch 3 Train batch 1200/3877 Loss=3.7503 Acc=33.71%\n",
      "Epoch 3 Train batch 1210/3877 Loss=3.7494 Acc=33.71%\n",
      "Epoch 3 Train batch 1220/3877 Loss=3.7482 Acc=33.72%\n",
      "Epoch 3 Train batch 1230/3877 Loss=3.7464 Acc=33.73%\n",
      "Epoch 3 Train batch 1240/3877 Loss=3.7454 Acc=33.74%\n",
      "Epoch 3 Train batch 1250/3877 Loss=3.7452 Acc=33.73%\n",
      "Epoch 3 Train batch 1260/3877 Loss=3.7442 Acc=33.74%\n",
      "Epoch 3 Train batch 1270/3877 Loss=3.7442 Acc=33.74%\n",
      "Epoch 3 Train batch 1280/3877 Loss=3.7434 Acc=33.74%\n",
      "Epoch 3 Train batch 1290/3877 Loss=3.7425 Acc=33.75%\n",
      "Epoch 3 Train batch 1300/3877 Loss=3.7414 Acc=33.77%\n",
      "Epoch 3 Train batch 1310/3877 Loss=3.7415 Acc=33.78%\n",
      "Epoch 3 Train batch 1320/3877 Loss=3.7413 Acc=33.77%\n",
      "Epoch 3 Train batch 1330/3877 Loss=3.7410 Acc=33.77%\n",
      "Epoch 3 Train batch 1340/3877 Loss=3.7408 Acc=33.77%\n",
      "Epoch 3 Train batch 1350/3877 Loss=3.7401 Acc=33.79%\n",
      "Epoch 3 Train batch 1360/3877 Loss=3.7393 Acc=33.79%\n",
      "Epoch 3 Train batch 1370/3877 Loss=3.7379 Acc=33.80%\n",
      "Epoch 3 Train batch 1380/3877 Loss=3.7369 Acc=33.81%\n",
      "Epoch 3 Train batch 1390/3877 Loss=3.7362 Acc=33.81%\n",
      "Epoch 3 Train batch 1400/3877 Loss=3.7358 Acc=33.81%\n",
      "Epoch 3 Train batch 1410/3877 Loss=3.7353 Acc=33.82%\n",
      "Epoch 3 Train batch 1420/3877 Loss=3.7351 Acc=33.83%\n",
      "Epoch 3 Train batch 1430/3877 Loss=3.7342 Acc=33.83%\n",
      "Epoch 3 Train batch 1440/3877 Loss=3.7331 Acc=33.84%\n",
      "Epoch 3 Train batch 1450/3877 Loss=3.7322 Acc=33.85%\n",
      "Epoch 3 Train batch 1460/3877 Loss=3.7315 Acc=33.85%\n",
      "Epoch 3 Train batch 1470/3877 Loss=3.7316 Acc=33.85%\n",
      "Epoch 3 Train batch 1480/3877 Loss=3.7316 Acc=33.85%\n",
      "Epoch 3 Train batch 1490/3877 Loss=3.7316 Acc=33.85%\n",
      "Epoch 3 Train batch 1500/3877 Loss=3.7307 Acc=33.86%\n",
      "Epoch 3 Train batch 1510/3877 Loss=3.7304 Acc=33.87%\n",
      "Epoch 3 Train batch 1520/3877 Loss=3.7305 Acc=33.86%\n",
      "Epoch 3 Train batch 1530/3877 Loss=3.7298 Acc=33.87%\n",
      "Epoch 3 Train batch 1540/3877 Loss=3.7293 Acc=33.87%\n",
      "Epoch 3 Train batch 1550/3877 Loss=3.7284 Acc=33.88%\n",
      "Epoch 3 Train batch 1560/3877 Loss=3.7272 Acc=33.89%\n",
      "Epoch 3 Train batch 1570/3877 Loss=3.7266 Acc=33.90%\n",
      "Epoch 3 Train batch 1580/3877 Loss=3.7257 Acc=33.91%\n",
      "Epoch 3 Train batch 1590/3877 Loss=3.7250 Acc=33.91%\n",
      "Epoch 3 Train batch 1600/3877 Loss=3.7245 Acc=33.91%\n",
      "Epoch 3 Train batch 1610/3877 Loss=3.7237 Acc=33.92%\n",
      "Epoch 3 Train batch 1620/3877 Loss=3.7237 Acc=33.92%\n",
      "Epoch 3 Train batch 1630/3877 Loss=3.7236 Acc=33.92%\n",
      "Epoch 3 Train batch 1640/3877 Loss=3.7228 Acc=33.93%\n",
      "Epoch 3 Train batch 1650/3877 Loss=3.7224 Acc=33.93%\n",
      "Epoch 3 Train batch 1660/3877 Loss=3.7219 Acc=33.94%\n",
      "Epoch 3 Train batch 1670/3877 Loss=3.7204 Acc=33.95%\n",
      "Epoch 3 Train batch 1680/3877 Loss=3.7199 Acc=33.95%\n",
      "Epoch 3 Train batch 1690/3877 Loss=3.7196 Acc=33.95%\n",
      "Epoch 3 Train batch 1700/3877 Loss=3.7185 Acc=33.96%\n",
      "Epoch 3 Train batch 1710/3877 Loss=3.7179 Acc=33.96%\n",
      "Epoch 3 Train batch 1720/3877 Loss=3.7174 Acc=33.96%\n",
      "Epoch 3 Train batch 1730/3877 Loss=3.7166 Acc=33.97%\n",
      "Epoch 3 Train batch 1740/3877 Loss=3.7155 Acc=33.97%\n",
      "Epoch 3 Train batch 1750/3877 Loss=3.7160 Acc=33.97%\n",
      "Epoch 3 Train batch 1760/3877 Loss=3.7156 Acc=33.97%\n",
      "Epoch 3 Train batch 1770/3877 Loss=3.7146 Acc=33.98%\n",
      "Epoch 3 Train batch 1780/3877 Loss=3.7139 Acc=33.99%\n",
      "Epoch 3 Train batch 1790/3877 Loss=3.7142 Acc=33.98%\n",
      "Epoch 3 Train batch 1800/3877 Loss=3.7138 Acc=33.98%\n",
      "Epoch 3 Train batch 1810/3877 Loss=3.7133 Acc=33.99%\n",
      "Epoch 3 Train batch 1820/3877 Loss=3.7127 Acc=33.99%\n",
      "Epoch 3 Train batch 1830/3877 Loss=3.7122 Acc=33.99%\n",
      "Epoch 3 Train batch 1840/3877 Loss=3.7119 Acc=33.99%\n",
      "Epoch 3 Train batch 1850/3877 Loss=3.7113 Acc=34.00%\n",
      "Epoch 3 Train batch 1860/3877 Loss=3.7112 Acc=34.00%\n",
      "Epoch 3 Train batch 1870/3877 Loss=3.7114 Acc=34.00%\n",
      "Epoch 3 Train batch 1880/3877 Loss=3.7106 Acc=34.00%\n",
      "Epoch 3 Train batch 1890/3877 Loss=3.7102 Acc=34.01%\n",
      "Epoch 3 Train batch 1900/3877 Loss=3.7089 Acc=34.02%\n",
      "Epoch 3 Train batch 1910/3877 Loss=3.7085 Acc=34.02%\n",
      "Epoch 3 Train batch 1920/3877 Loss=3.7077 Acc=34.03%\n",
      "Epoch 3 Train batch 1930/3877 Loss=3.7078 Acc=34.03%\n",
      "Epoch 3 Train batch 1940/3877 Loss=3.7071 Acc=34.03%\n",
      "Epoch 3 Train batch 1950/3877 Loss=3.7072 Acc=34.03%\n",
      "Epoch 3 Train batch 1960/3877 Loss=3.7067 Acc=34.03%\n",
      "Epoch 3 Train batch 1970/3877 Loss=3.7065 Acc=34.03%\n",
      "Epoch 3 Train batch 1980/3877 Loss=3.7061 Acc=34.03%\n",
      "Epoch 3 Train batch 1990/3877 Loss=3.7056 Acc=34.04%\n",
      "Epoch 3 Train batch 2000/3877 Loss=3.7051 Acc=34.04%\n",
      "Epoch 3 Train batch 2010/3877 Loss=3.7047 Acc=34.05%\n",
      "Epoch 3 Train batch 2020/3877 Loss=3.7046 Acc=34.05%\n",
      "Epoch 3 Train batch 2030/3877 Loss=3.7050 Acc=34.04%\n",
      "Epoch 3 Train batch 2040/3877 Loss=3.7037 Acc=34.06%\n",
      "Epoch 3 Train batch 2050/3877 Loss=3.7032 Acc=34.06%\n",
      "Epoch 3 Train batch 2060/3877 Loss=3.7030 Acc=34.06%\n",
      "Epoch 3 Train batch 2070/3877 Loss=3.7033 Acc=34.06%\n",
      "Epoch 3 Train batch 2080/3877 Loss=3.7029 Acc=34.06%\n",
      "Epoch 3 Train batch 2090/3877 Loss=3.7023 Acc=34.07%\n",
      "Epoch 3 Train batch 2100/3877 Loss=3.7021 Acc=34.07%\n",
      "Epoch 3 Train batch 2110/3877 Loss=3.7017 Acc=34.08%\n",
      "Epoch 3 Train batch 2120/3877 Loss=3.7017 Acc=34.07%\n",
      "Epoch 3 Train batch 2130/3877 Loss=3.7018 Acc=34.07%\n",
      "Epoch 3 Train batch 2140/3877 Loss=3.7008 Acc=34.08%\n",
      "Epoch 3 Train batch 2150/3877 Loss=3.7001 Acc=34.08%\n",
      "Epoch 3 Train batch 2160/3877 Loss=3.6993 Acc=34.08%\n",
      "Epoch 3 Train batch 2170/3877 Loss=3.6991 Acc=34.08%\n",
      "Epoch 3 Train batch 2180/3877 Loss=3.6986 Acc=34.08%\n",
      "Epoch 3 Train batch 2190/3877 Loss=3.6977 Acc=34.09%\n",
      "Epoch 3 Train batch 2200/3877 Loss=3.6975 Acc=34.09%\n",
      "Epoch 3 Train batch 2210/3877 Loss=3.6974 Acc=34.09%\n",
      "Epoch 3 Train batch 2220/3877 Loss=3.6968 Acc=34.10%\n",
      "Epoch 3 Train batch 2230/3877 Loss=3.6966 Acc=34.11%\n",
      "Epoch 3 Train batch 2240/3877 Loss=3.6959 Acc=34.11%\n",
      "Epoch 3 Train batch 2250/3877 Loss=3.6956 Acc=34.12%\n",
      "Epoch 3 Train batch 2260/3877 Loss=3.6951 Acc=34.12%\n",
      "Epoch 3 Train batch 2270/3877 Loss=3.6949 Acc=34.12%\n",
      "Epoch 3 Train batch 2280/3877 Loss=3.6949 Acc=34.13%\n",
      "Epoch 3 Train batch 2290/3877 Loss=3.6939 Acc=34.14%\n",
      "Epoch 3 Train batch 2300/3877 Loss=3.6936 Acc=34.14%\n",
      "Epoch 3 Train batch 2310/3877 Loss=3.6930 Acc=34.14%\n",
      "Epoch 3 Train batch 2320/3877 Loss=3.6924 Acc=34.15%\n",
      "Epoch 3 Train batch 2330/3877 Loss=3.6920 Acc=34.16%\n",
      "Epoch 3 Train batch 2340/3877 Loss=3.6916 Acc=34.16%\n",
      "Epoch 3 Train batch 2350/3877 Loss=3.6912 Acc=34.16%\n",
      "Epoch 3 Train batch 2360/3877 Loss=3.6903 Acc=34.17%\n",
      "Epoch 3 Train batch 2370/3877 Loss=3.6893 Acc=34.18%\n",
      "Epoch 3 Train batch 2380/3877 Loss=3.6886 Acc=34.19%\n",
      "Epoch 3 Train batch 2390/3877 Loss=3.6885 Acc=34.19%\n",
      "Epoch 3 Train batch 2400/3877 Loss=3.6886 Acc=34.19%\n",
      "Epoch 3 Train batch 2410/3877 Loss=3.6883 Acc=34.20%\n",
      "Epoch 3 Train batch 2420/3877 Loss=3.6875 Acc=34.21%\n",
      "Epoch 3 Train batch 2430/3877 Loss=3.6871 Acc=34.21%\n",
      "Epoch 3 Train batch 2440/3877 Loss=3.6866 Acc=34.22%\n",
      "Epoch 3 Train batch 2450/3877 Loss=3.6863 Acc=34.22%\n",
      "Epoch 3 Train batch 2460/3877 Loss=3.6857 Acc=34.23%\n",
      "Epoch 3 Train batch 2470/3877 Loss=3.6857 Acc=34.23%\n",
      "Epoch 3 Train batch 2480/3877 Loss=3.6856 Acc=34.23%\n",
      "Epoch 3 Train batch 2490/3877 Loss=3.6853 Acc=34.24%\n",
      "Epoch 3 Train batch 2500/3877 Loss=3.6848 Acc=34.24%\n",
      "Epoch 3 Train batch 2510/3877 Loss=3.6842 Acc=34.25%\n",
      "Epoch 3 Train batch 2520/3877 Loss=3.6840 Acc=34.24%\n",
      "Epoch 3 Train batch 2530/3877 Loss=3.6843 Acc=34.24%\n",
      "Epoch 3 Train batch 2540/3877 Loss=3.6834 Acc=34.24%\n",
      "Epoch 3 Train batch 2550/3877 Loss=3.6830 Acc=34.24%\n",
      "Epoch 3 Train batch 2560/3877 Loss=3.6826 Acc=34.25%\n",
      "Epoch 3 Train batch 2570/3877 Loss=3.6821 Acc=34.25%\n",
      "Epoch 3 Train batch 2580/3877 Loss=3.6812 Acc=34.25%\n",
      "Epoch 3 Train batch 2590/3877 Loss=3.6805 Acc=34.26%\n",
      "Epoch 3 Train batch 2600/3877 Loss=3.6799 Acc=34.27%\n",
      "Epoch 3 Train batch 2610/3877 Loss=3.6794 Acc=34.27%\n",
      "Epoch 3 Train batch 2620/3877 Loss=3.6789 Acc=34.27%\n",
      "Epoch 3 Train batch 2630/3877 Loss=3.6784 Acc=34.27%\n",
      "Epoch 3 Train batch 2640/3877 Loss=3.6779 Acc=34.28%\n",
      "Epoch 3 Train batch 2650/3877 Loss=3.6775 Acc=34.28%\n",
      "Epoch 3 Train batch 2660/3877 Loss=3.6768 Acc=34.28%\n",
      "Epoch 3 Train batch 2670/3877 Loss=3.6768 Acc=34.28%\n",
      "Epoch 3 Train batch 2680/3877 Loss=3.6769 Acc=34.28%\n",
      "Epoch 3 Train batch 2690/3877 Loss=3.6771 Acc=34.28%\n",
      "Epoch 3 Train batch 2700/3877 Loss=3.6765 Acc=34.29%\n",
      "Epoch 3 Train batch 2710/3877 Loss=3.6759 Acc=34.29%\n",
      "Epoch 3 Train batch 2720/3877 Loss=3.6755 Acc=34.29%\n",
      "Epoch 3 Train batch 2730/3877 Loss=3.6752 Acc=34.29%\n",
      "Epoch 3 Train batch 2740/3877 Loss=3.6747 Acc=34.29%\n",
      "Epoch 3 Train batch 2750/3877 Loss=3.6744 Acc=34.29%\n",
      "Epoch 3 Train batch 2760/3877 Loss=3.6742 Acc=34.29%\n",
      "Epoch 3 Train batch 2770/3877 Loss=3.6741 Acc=34.30%\n",
      "Epoch 3 Train batch 2780/3877 Loss=3.6736 Acc=34.30%\n",
      "Epoch 3 Train batch 2790/3877 Loss=3.6729 Acc=34.31%\n",
      "Epoch 3 Train batch 2800/3877 Loss=3.6730 Acc=34.31%\n",
      "Epoch 3 Train batch 2810/3877 Loss=3.6726 Acc=34.31%\n",
      "Epoch 3 Train batch 2820/3877 Loss=3.6722 Acc=34.31%\n",
      "Epoch 3 Train batch 2830/3877 Loss=3.6720 Acc=34.31%\n",
      "Epoch 3 Train batch 2840/3877 Loss=3.6713 Acc=34.32%\n",
      "Epoch 3 Train batch 2850/3877 Loss=3.6707 Acc=34.33%\n",
      "Epoch 3 Train batch 2860/3877 Loss=3.6702 Acc=34.33%\n",
      "Epoch 3 Train batch 2870/3877 Loss=3.6698 Acc=34.33%\n",
      "Epoch 3 Train batch 2880/3877 Loss=3.6694 Acc=34.33%\n",
      "Epoch 3 Train batch 2890/3877 Loss=3.6691 Acc=34.33%\n",
      "Epoch 3 Train batch 2900/3877 Loss=3.6681 Acc=34.34%\n",
      "Epoch 3 Train batch 2910/3877 Loss=3.6678 Acc=34.34%\n",
      "Epoch 3 Train batch 2920/3877 Loss=3.6672 Acc=34.34%\n",
      "Epoch 3 Train batch 2930/3877 Loss=3.6670 Acc=34.35%\n",
      "Epoch 3 Train batch 2940/3877 Loss=3.6669 Acc=34.35%\n",
      "Epoch 3 Train batch 2950/3877 Loss=3.6662 Acc=34.35%\n",
      "Epoch 3 Train batch 2960/3877 Loss=3.6658 Acc=34.36%\n",
      "Epoch 3 Train batch 2970/3877 Loss=3.6657 Acc=34.36%\n",
      "Epoch 3 Train batch 2980/3877 Loss=3.6651 Acc=34.36%\n",
      "Epoch 3 Train batch 2990/3877 Loss=3.6641 Acc=34.37%\n",
      "Epoch 3 Train batch 3000/3877 Loss=3.6638 Acc=34.38%\n",
      "Epoch 3 Train batch 3010/3877 Loss=3.6634 Acc=34.39%\n",
      "Epoch 3 Train batch 3020/3877 Loss=3.6631 Acc=34.39%\n",
      "Epoch 3 Train batch 3030/3877 Loss=3.6630 Acc=34.39%\n",
      "Epoch 3 Train batch 3040/3877 Loss=3.6626 Acc=34.39%\n",
      "Epoch 3 Train batch 3050/3877 Loss=3.6622 Acc=34.39%\n",
      "Epoch 3 Train batch 3060/3877 Loss=3.6617 Acc=34.39%\n",
      "Epoch 3 Train batch 3070/3877 Loss=3.6616 Acc=34.40%\n",
      "Epoch 3 Train batch 3080/3877 Loss=3.6613 Acc=34.40%\n",
      "Epoch 3 Train batch 3090/3877 Loss=3.6608 Acc=34.40%\n",
      "Epoch 3 Train batch 3100/3877 Loss=3.6605 Acc=34.41%\n",
      "Epoch 3 Train batch 3110/3877 Loss=3.6603 Acc=34.41%\n",
      "Epoch 3 Train batch 3120/3877 Loss=3.6598 Acc=34.41%\n",
      "Epoch 3 Train batch 3130/3877 Loss=3.6596 Acc=34.41%\n",
      "Epoch 3 Train batch 3140/3877 Loss=3.6589 Acc=34.42%\n",
      "Epoch 3 Train batch 3150/3877 Loss=3.6588 Acc=34.42%\n",
      "Epoch 3 Train batch 3160/3877 Loss=3.6583 Acc=34.42%\n",
      "Epoch 3 Train batch 3170/3877 Loss=3.6578 Acc=34.42%\n",
      "Epoch 3 Train batch 3180/3877 Loss=3.6572 Acc=34.42%\n",
      "Epoch 3 Train batch 3190/3877 Loss=3.6569 Acc=34.42%\n",
      "Epoch 3 Train batch 3200/3877 Loss=3.6564 Acc=34.43%\n",
      "Epoch 3 Train batch 3210/3877 Loss=3.6561 Acc=34.43%\n",
      "Epoch 3 Train batch 3220/3877 Loss=3.6556 Acc=34.43%\n",
      "Epoch 3 Train batch 3230/3877 Loss=3.6553 Acc=34.43%\n",
      "Epoch 3 Train batch 3240/3877 Loss=3.6550 Acc=34.43%\n",
      "Epoch 3 Train batch 3250/3877 Loss=3.6544 Acc=34.43%\n",
      "Epoch 3 Train batch 3260/3877 Loss=3.6538 Acc=34.44%\n",
      "Epoch 3 Train batch 3270/3877 Loss=3.6537 Acc=34.45%\n",
      "Epoch 3 Train batch 3280/3877 Loss=3.6533 Acc=34.45%\n",
      "Epoch 3 Train batch 3290/3877 Loss=3.6530 Acc=34.45%\n",
      "Epoch 3 Train batch 3300/3877 Loss=3.6527 Acc=34.45%\n",
      "Epoch 3 Train batch 3310/3877 Loss=3.6524 Acc=34.46%\n",
      "Epoch 3 Train batch 3320/3877 Loss=3.6522 Acc=34.46%\n",
      "Epoch 3 Train batch 3330/3877 Loss=3.6518 Acc=34.46%\n",
      "Epoch 3 Train batch 3340/3877 Loss=3.6513 Acc=34.46%\n",
      "Epoch 3 Train batch 3350/3877 Loss=3.6509 Acc=34.47%\n",
      "Epoch 3 Train batch 3360/3877 Loss=3.6503 Acc=34.48%\n",
      "Epoch 3 Train batch 3370/3877 Loss=3.6498 Acc=34.48%\n",
      "Epoch 3 Train batch 3380/3877 Loss=3.6493 Acc=34.49%\n",
      "Epoch 3 Train batch 3390/3877 Loss=3.6490 Acc=34.49%\n",
      "Epoch 3 Train batch 3400/3877 Loss=3.6485 Acc=34.49%\n",
      "Epoch 3 Train batch 3410/3877 Loss=3.6480 Acc=34.50%\n",
      "Epoch 3 Train batch 3420/3877 Loss=3.6479 Acc=34.50%\n",
      "Epoch 3 Train batch 3430/3877 Loss=3.6475 Acc=34.50%\n",
      "Epoch 3 Train batch 3440/3877 Loss=3.6470 Acc=34.51%\n",
      "Epoch 3 Train batch 3450/3877 Loss=3.6464 Acc=34.51%\n",
      "Epoch 3 Train batch 3460/3877 Loss=3.6459 Acc=34.52%\n",
      "Epoch 3 Train batch 3470/3877 Loss=3.6456 Acc=34.52%\n",
      "Epoch 3 Train batch 3480/3877 Loss=3.6454 Acc=34.52%\n",
      "Epoch 3 Train batch 3490/3877 Loss=3.6450 Acc=34.52%\n",
      "Epoch 3 Train batch 3500/3877 Loss=3.6448 Acc=34.52%\n",
      "Epoch 3 Train batch 3510/3877 Loss=3.6440 Acc=34.53%\n",
      "Epoch 3 Train batch 3520/3877 Loss=3.6435 Acc=34.54%\n",
      "Epoch 3 Train batch 3530/3877 Loss=3.6432 Acc=34.54%\n",
      "Epoch 3 Train batch 3540/3877 Loss=3.6430 Acc=34.54%\n",
      "Epoch 3 Train batch 3550/3877 Loss=3.6424 Acc=34.54%\n",
      "Epoch 3 Train batch 3560/3877 Loss=3.6423 Acc=34.54%\n",
      "Epoch 3 Train batch 3570/3877 Loss=3.6419 Acc=34.55%\n",
      "Epoch 3 Train batch 3580/3877 Loss=3.6414 Acc=34.55%\n",
      "Epoch 3 Train batch 3590/3877 Loss=3.6412 Acc=34.55%\n",
      "Epoch 3 Train batch 3600/3877 Loss=3.6411 Acc=34.55%\n",
      "Epoch 3 Train batch 3610/3877 Loss=3.6407 Acc=34.56%\n",
      "Epoch 3 Train batch 3620/3877 Loss=3.6404 Acc=34.55%\n",
      "Epoch 3 Train batch 3630/3877 Loss=3.6403 Acc=34.56%\n",
      "Epoch 3 Train batch 3640/3877 Loss=3.6395 Acc=34.56%\n",
      "Epoch 3 Train batch 3650/3877 Loss=3.6393 Acc=34.57%\n",
      "Epoch 3 Train batch 3660/3877 Loss=3.6393 Acc=34.57%\n",
      "Epoch 3 Train batch 3670/3877 Loss=3.6390 Acc=34.57%\n",
      "Epoch 3 Train batch 3680/3877 Loss=3.6383 Acc=34.58%\n",
      "Epoch 3 Train batch 3690/3877 Loss=3.6379 Acc=34.58%\n",
      "Epoch 3 Train batch 3700/3877 Loss=3.6374 Acc=34.59%\n",
      "Epoch 3 Train batch 3710/3877 Loss=3.6372 Acc=34.59%\n",
      "Epoch 3 Train batch 3720/3877 Loss=3.6369 Acc=34.59%\n",
      "Epoch 3 Train batch 3730/3877 Loss=3.6365 Acc=34.60%\n",
      "Epoch 3 Train batch 3740/3877 Loss=3.6359 Acc=34.60%\n",
      "Epoch 3 Train batch 3750/3877 Loss=3.6356 Acc=34.60%\n",
      "Epoch 3 Train batch 3760/3877 Loss=3.6354 Acc=34.61%\n",
      "Epoch 3 Train batch 3770/3877 Loss=3.6349 Acc=34.61%\n",
      "Epoch 3 Train batch 3780/3877 Loss=3.6344 Acc=34.61%\n",
      "Epoch 3 Train batch 3790/3877 Loss=3.6338 Acc=34.62%\n",
      "Epoch 3 Train batch 3800/3877 Loss=3.6334 Acc=34.62%\n",
      "Epoch 3 Train batch 3810/3877 Loss=3.6331 Acc=34.63%\n",
      "Epoch 3 Train batch 3820/3877 Loss=3.6330 Acc=34.63%\n",
      "Epoch 3 Train batch 3830/3877 Loss=3.6327 Acc=34.63%\n",
      "Epoch 3 Train batch 3840/3877 Loss=3.6323 Acc=34.63%\n",
      "Epoch 3 Train batch 3850/3877 Loss=3.6322 Acc=34.63%\n",
      "Epoch 3 Train batch 3860/3877 Loss=3.6321 Acc=34.63%\n",
      "Epoch 3 Train batch 3870/3877 Loss=3.6317 Acc=34.64%\n",
      "Epoch 3 Train batch 3877/3877 Loss=3.6314 Acc=34.64%\n",
      "Epoch 3/5 train_loss=3.6314 train_acc=34.64% val_loss=3.3752 val_acc=37.45%\n",
      "Epoch 4 Train batch 10/3877 Loss=3.3741 Acc=37.17%\n",
      "Epoch 4 Train batch 20/3877 Loss=3.3593 Acc=36.55%\n",
      "Epoch 4 Train batch 30/3877 Loss=3.3471 Acc=36.80%\n",
      "Epoch 4 Train batch 40/3877 Loss=3.3542 Acc=36.69%\n",
      "Epoch 4 Train batch 50/3877 Loss=3.3624 Acc=36.36%\n",
      "Epoch 4 Train batch 60/3877 Loss=3.3712 Acc=36.28%\n",
      "Epoch 4 Train batch 70/3877 Loss=3.3682 Acc=36.46%\n",
      "Epoch 4 Train batch 80/3877 Loss=3.3760 Acc=36.40%\n",
      "Epoch 4 Train batch 90/3877 Loss=3.3769 Acc=36.40%\n",
      "Epoch 4 Train batch 100/3877 Loss=3.3751 Acc=36.44%\n",
      "Epoch 4 Train batch 110/3877 Loss=3.3749 Acc=36.39%\n",
      "Epoch 4 Train batch 120/3877 Loss=3.3832 Acc=36.33%\n",
      "Epoch 4 Train batch 130/3877 Loss=3.3907 Acc=36.34%\n",
      "Epoch 4 Train batch 140/3877 Loss=3.4003 Acc=36.24%\n",
      "Epoch 4 Train batch 150/3877 Loss=3.3962 Acc=36.31%\n",
      "Epoch 4 Train batch 160/3877 Loss=3.3952 Acc=36.37%\n",
      "Epoch 4 Train batch 170/3877 Loss=3.3992 Acc=36.38%\n",
      "Epoch 4 Train batch 180/3877 Loss=3.3990 Acc=36.44%\n",
      "Epoch 4 Train batch 190/3877 Loss=3.4007 Acc=36.39%\n",
      "Epoch 4 Train batch 200/3877 Loss=3.4016 Acc=36.40%\n",
      "Epoch 4 Train batch 210/3877 Loss=3.4032 Acc=36.39%\n",
      "Epoch 4 Train batch 220/3877 Loss=3.4053 Acc=36.35%\n",
      "Epoch 4 Train batch 230/3877 Loss=3.3995 Acc=36.41%\n",
      "Epoch 4 Train batch 240/3877 Loss=3.4046 Acc=36.36%\n",
      "Epoch 4 Train batch 250/3877 Loss=3.4058 Acc=36.33%\n",
      "Epoch 4 Train batch 260/3877 Loss=3.4055 Acc=36.36%\n",
      "Epoch 4 Train batch 270/3877 Loss=3.4032 Acc=36.38%\n",
      "Epoch 4 Train batch 280/3877 Loss=3.4016 Acc=36.38%\n",
      "Epoch 4 Train batch 290/3877 Loss=3.4022 Acc=36.37%\n",
      "Epoch 4 Train batch 300/3877 Loss=3.3966 Acc=36.42%\n",
      "Epoch 4 Train batch 310/3877 Loss=3.3950 Acc=36.45%\n",
      "Epoch 4 Train batch 320/3877 Loss=3.3963 Acc=36.43%\n",
      "Epoch 4 Train batch 330/3877 Loss=3.3955 Acc=36.42%\n",
      "Epoch 4 Train batch 340/3877 Loss=3.3991 Acc=36.38%\n",
      "Epoch 4 Train batch 350/3877 Loss=3.3972 Acc=36.42%\n",
      "Epoch 4 Train batch 360/3877 Loss=3.3984 Acc=36.42%\n",
      "Epoch 4 Train batch 370/3877 Loss=3.3976 Acc=36.45%\n",
      "Epoch 4 Train batch 380/3877 Loss=3.3999 Acc=36.43%\n",
      "Epoch 4 Train batch 390/3877 Loss=3.4024 Acc=36.42%\n",
      "Epoch 4 Train batch 400/3877 Loss=3.4030 Acc=36.44%\n",
      "Epoch 4 Train batch 410/3877 Loss=3.4055 Acc=36.42%\n",
      "Epoch 4 Train batch 420/3877 Loss=3.4039 Acc=36.43%\n",
      "Epoch 4 Train batch 430/3877 Loss=3.4038 Acc=36.45%\n",
      "Epoch 4 Train batch 440/3877 Loss=3.4041 Acc=36.45%\n",
      "Epoch 4 Train batch 450/3877 Loss=3.4034 Acc=36.48%\n",
      "Epoch 4 Train batch 460/3877 Loss=3.4035 Acc=36.48%\n",
      "Epoch 4 Train batch 470/3877 Loss=3.4033 Acc=36.47%\n",
      "Epoch 4 Train batch 480/3877 Loss=3.4025 Acc=36.47%\n",
      "Epoch 4 Train batch 490/3877 Loss=3.4036 Acc=36.46%\n",
      "Epoch 4 Train batch 500/3877 Loss=3.4050 Acc=36.44%\n",
      "Epoch 4 Train batch 510/3877 Loss=3.4034 Acc=36.46%\n",
      "Epoch 4 Train batch 520/3877 Loss=3.4027 Acc=36.45%\n",
      "Epoch 4 Train batch 530/3877 Loss=3.4041 Acc=36.42%\n",
      "Epoch 4 Train batch 540/3877 Loss=3.4038 Acc=36.42%\n",
      "Epoch 4 Train batch 550/3877 Loss=3.4040 Acc=36.42%\n",
      "Epoch 4 Train batch 560/3877 Loss=3.4034 Acc=36.43%\n",
      "Epoch 4 Train batch 570/3877 Loss=3.4015 Acc=36.46%\n",
      "Epoch 4 Train batch 580/3877 Loss=3.4033 Acc=36.45%\n",
      "Epoch 4 Train batch 590/3877 Loss=3.4035 Acc=36.46%\n",
      "Epoch 4 Train batch 600/3877 Loss=3.4031 Acc=36.46%\n",
      "Epoch 4 Train batch 610/3877 Loss=3.4021 Acc=36.45%\n",
      "Epoch 4 Train batch 620/3877 Loss=3.4020 Acc=36.46%\n",
      "Epoch 4 Train batch 630/3877 Loss=3.4039 Acc=36.45%\n",
      "Epoch 4 Train batch 640/3877 Loss=3.4035 Acc=36.46%\n",
      "Epoch 4 Train batch 650/3877 Loss=3.4045 Acc=36.45%\n",
      "Epoch 4 Train batch 660/3877 Loss=3.4048 Acc=36.44%\n",
      "Epoch 4 Train batch 670/3877 Loss=3.4053 Acc=36.43%\n",
      "Epoch 4 Train batch 680/3877 Loss=3.4058 Acc=36.41%\n",
      "Epoch 4 Train batch 690/3877 Loss=3.4056 Acc=36.42%\n",
      "Epoch 4 Train batch 700/3877 Loss=3.4040 Acc=36.44%\n",
      "Epoch 4 Train batch 710/3877 Loss=3.4050 Acc=36.44%\n",
      "Epoch 4 Train batch 720/3877 Loss=3.4039 Acc=36.45%\n",
      "Epoch 4 Train batch 730/3877 Loss=3.4043 Acc=36.44%\n",
      "Epoch 4 Train batch 740/3877 Loss=3.4034 Acc=36.46%\n",
      "Epoch 4 Train batch 750/3877 Loss=3.4047 Acc=36.44%\n",
      "Epoch 4 Train batch 760/3877 Loss=3.4044 Acc=36.44%\n",
      "Epoch 4 Train batch 770/3877 Loss=3.4041 Acc=36.45%\n",
      "Epoch 4 Train batch 780/3877 Loss=3.4056 Acc=36.45%\n",
      "Epoch 4 Train batch 790/3877 Loss=3.4062 Acc=36.46%\n",
      "Epoch 4 Train batch 800/3877 Loss=3.4058 Acc=36.45%\n",
      "Epoch 4 Train batch 810/3877 Loss=3.4057 Acc=36.46%\n",
      "Epoch 4 Train batch 820/3877 Loss=3.4062 Acc=36.45%\n",
      "Epoch 4 Train batch 830/3877 Loss=3.4068 Acc=36.44%\n",
      "Epoch 4 Train batch 840/3877 Loss=3.4071 Acc=36.43%\n",
      "Epoch 4 Train batch 850/3877 Loss=3.4059 Acc=36.44%\n",
      "Epoch 4 Train batch 860/3877 Loss=3.4055 Acc=36.45%\n",
      "Epoch 4 Train batch 870/3877 Loss=3.4059 Acc=36.45%\n",
      "Epoch 4 Train batch 880/3877 Loss=3.4050 Acc=36.45%\n",
      "Epoch 4 Train batch 890/3877 Loss=3.4052 Acc=36.46%\n",
      "Epoch 4 Train batch 900/3877 Loss=3.4044 Acc=36.46%\n",
      "Epoch 4 Train batch 910/3877 Loss=3.4059 Acc=36.45%\n",
      "Epoch 4 Train batch 920/3877 Loss=3.4055 Acc=36.46%\n",
      "Epoch 4 Train batch 930/3877 Loss=3.4043 Acc=36.48%\n",
      "Epoch 4 Train batch 940/3877 Loss=3.4040 Acc=36.47%\n",
      "Epoch 4 Train batch 950/3877 Loss=3.4039 Acc=36.47%\n",
      "Epoch 4 Train batch 960/3877 Loss=3.4038 Acc=36.47%\n",
      "Epoch 4 Train batch 970/3877 Loss=3.4021 Acc=36.49%\n",
      "Epoch 4 Train batch 980/3877 Loss=3.4021 Acc=36.50%\n",
      "Epoch 4 Train batch 990/3877 Loss=3.4024 Acc=36.50%\n",
      "Epoch 4 Train batch 1000/3877 Loss=3.4025 Acc=36.48%\n",
      "Epoch 4 Train batch 1010/3877 Loss=3.4021 Acc=36.48%\n",
      "Epoch 4 Train batch 1020/3877 Loss=3.4029 Acc=36.47%\n",
      "Epoch 4 Train batch 1030/3877 Loss=3.4041 Acc=36.47%\n",
      "Epoch 4 Train batch 1040/3877 Loss=3.4043 Acc=36.46%\n",
      "Epoch 4 Train batch 1050/3877 Loss=3.4040 Acc=36.47%\n",
      "Epoch 4 Train batch 1060/3877 Loss=3.4032 Acc=36.48%\n",
      "Epoch 4 Train batch 1070/3877 Loss=3.4023 Acc=36.49%\n",
      "Epoch 4 Train batch 1080/3877 Loss=3.4023 Acc=36.48%\n",
      "Epoch 4 Train batch 1090/3877 Loss=3.4025 Acc=36.49%\n",
      "Epoch 4 Train batch 1100/3877 Loss=3.4024 Acc=36.49%\n",
      "Epoch 4 Train batch 1110/3877 Loss=3.4031 Acc=36.47%\n",
      "Epoch 4 Train batch 1120/3877 Loss=3.4024 Acc=36.48%\n",
      "Epoch 4 Train batch 1130/3877 Loss=3.4024 Acc=36.48%\n",
      "Epoch 4 Train batch 1140/3877 Loss=3.4026 Acc=36.47%\n",
      "Epoch 4 Train batch 1150/3877 Loss=3.4034 Acc=36.47%\n",
      "Epoch 4 Train batch 1160/3877 Loss=3.4042 Acc=36.47%\n",
      "Epoch 4 Train batch 1170/3877 Loss=3.4040 Acc=36.46%\n",
      "Epoch 4 Train batch 1180/3877 Loss=3.4043 Acc=36.47%\n",
      "Epoch 4 Train batch 1190/3877 Loss=3.4037 Acc=36.46%\n",
      "Epoch 4 Train batch 1200/3877 Loss=3.4035 Acc=36.47%\n",
      "Epoch 4 Train batch 1210/3877 Loss=3.4036 Acc=36.47%\n",
      "Epoch 4 Train batch 1220/3877 Loss=3.4041 Acc=36.47%\n",
      "Epoch 4 Train batch 1230/3877 Loss=3.4042 Acc=36.46%\n",
      "Epoch 4 Train batch 1240/3877 Loss=3.4039 Acc=36.46%\n",
      "Epoch 4 Train batch 1250/3877 Loss=3.4039 Acc=36.46%\n",
      "Epoch 4 Train batch 1260/3877 Loss=3.4033 Acc=36.46%\n",
      "Epoch 4 Train batch 1270/3877 Loss=3.4038 Acc=36.46%\n",
      "Epoch 4 Train batch 1280/3877 Loss=3.4033 Acc=36.46%\n",
      "Epoch 4 Train batch 1290/3877 Loss=3.4035 Acc=36.46%\n",
      "Epoch 4 Train batch 1300/3877 Loss=3.4035 Acc=36.47%\n",
      "Epoch 4 Train batch 1310/3877 Loss=3.4035 Acc=36.47%\n",
      "Epoch 4 Train batch 1320/3877 Loss=3.4029 Acc=36.47%\n",
      "Epoch 4 Train batch 1330/3877 Loss=3.4037 Acc=36.45%\n",
      "Epoch 4 Train batch 1340/3877 Loss=3.4030 Acc=36.47%\n",
      "Epoch 4 Train batch 1350/3877 Loss=3.4026 Acc=36.48%\n",
      "Epoch 4 Train batch 1360/3877 Loss=3.4028 Acc=36.48%\n",
      "Epoch 4 Train batch 1370/3877 Loss=3.4028 Acc=36.47%\n",
      "Epoch 4 Train batch 1380/3877 Loss=3.4030 Acc=36.47%\n",
      "Epoch 4 Train batch 1390/3877 Loss=3.4025 Acc=36.48%\n",
      "Epoch 4 Train batch 1400/3877 Loss=3.4032 Acc=36.47%\n",
      "Epoch 4 Train batch 1410/3877 Loss=3.4026 Acc=36.48%\n",
      "Epoch 4 Train batch 1420/3877 Loss=3.4019 Acc=36.48%\n",
      "Epoch 4 Train batch 1430/3877 Loss=3.4018 Acc=36.49%\n",
      "Epoch 4 Train batch 1440/3877 Loss=3.4013 Acc=36.49%\n",
      "Epoch 4 Train batch 1450/3877 Loss=3.4007 Acc=36.50%\n",
      "Epoch 4 Train batch 1460/3877 Loss=3.4006 Acc=36.50%\n",
      "Epoch 4 Train batch 1470/3877 Loss=3.4005 Acc=36.50%\n",
      "Epoch 4 Train batch 1480/3877 Loss=3.3997 Acc=36.51%\n",
      "Epoch 4 Train batch 1490/3877 Loss=3.3999 Acc=36.51%\n",
      "Epoch 4 Train batch 1500/3877 Loss=3.3996 Acc=36.51%\n",
      "Epoch 4 Train batch 1510/3877 Loss=3.3991 Acc=36.52%\n",
      "Epoch 4 Train batch 1520/3877 Loss=3.3987 Acc=36.51%\n",
      "Epoch 4 Train batch 1530/3877 Loss=3.3988 Acc=36.50%\n",
      "Epoch 4 Train batch 1540/3877 Loss=3.3981 Acc=36.51%\n",
      "Epoch 4 Train batch 1550/3877 Loss=3.3976 Acc=36.53%\n",
      "Epoch 4 Train batch 1560/3877 Loss=3.3971 Acc=36.53%\n",
      "Epoch 4 Train batch 1570/3877 Loss=3.3969 Acc=36.53%\n",
      "Epoch 4 Train batch 1580/3877 Loss=3.3969 Acc=36.53%\n",
      "Epoch 4 Train batch 1590/3877 Loss=3.3965 Acc=36.53%\n",
      "Epoch 4 Train batch 1600/3877 Loss=3.3963 Acc=36.53%\n",
      "Epoch 4 Train batch 1610/3877 Loss=3.3963 Acc=36.53%\n",
      "Epoch 4 Train batch 1620/3877 Loss=3.3963 Acc=36.53%\n",
      "Epoch 4 Train batch 1630/3877 Loss=3.3959 Acc=36.54%\n",
      "Epoch 4 Train batch 1640/3877 Loss=3.3952 Acc=36.54%\n",
      "Epoch 4 Train batch 1650/3877 Loss=3.3944 Acc=36.55%\n",
      "Epoch 4 Train batch 1660/3877 Loss=3.3939 Acc=36.55%\n",
      "Epoch 4 Train batch 1670/3877 Loss=3.3939 Acc=36.54%\n",
      "Epoch 4 Train batch 1680/3877 Loss=3.3937 Acc=36.54%\n",
      "Epoch 4 Train batch 1690/3877 Loss=3.3939 Acc=36.55%\n",
      "Epoch 4 Train batch 1700/3877 Loss=3.3933 Acc=36.56%\n",
      "Epoch 4 Train batch 1710/3877 Loss=3.3935 Acc=36.56%\n",
      "Epoch 4 Train batch 1720/3877 Loss=3.3933 Acc=36.57%\n",
      "Epoch 4 Train batch 1730/3877 Loss=3.3931 Acc=36.56%\n",
      "Epoch 4 Train batch 1740/3877 Loss=3.3931 Acc=36.56%\n",
      "Epoch 4 Train batch 1750/3877 Loss=3.3930 Acc=36.57%\n",
      "Epoch 4 Train batch 1760/3877 Loss=3.3923 Acc=36.58%\n",
      "Epoch 4 Train batch 1770/3877 Loss=3.3924 Acc=36.57%\n",
      "Epoch 4 Train batch 1780/3877 Loss=3.3917 Acc=36.58%\n",
      "Epoch 4 Train batch 1790/3877 Loss=3.3909 Acc=36.59%\n",
      "Epoch 4 Train batch 1800/3877 Loss=3.3907 Acc=36.59%\n",
      "Epoch 4 Train batch 1810/3877 Loss=3.3910 Acc=36.59%\n",
      "Epoch 4 Train batch 1820/3877 Loss=3.3907 Acc=36.60%\n",
      "Epoch 4 Train batch 1830/3877 Loss=3.3904 Acc=36.60%\n",
      "Epoch 4 Train batch 1840/3877 Loss=3.3896 Acc=36.61%\n",
      "Epoch 4 Train batch 1850/3877 Loss=3.3898 Acc=36.60%\n",
      "Epoch 4 Train batch 1860/3877 Loss=3.3897 Acc=36.60%\n",
      "Epoch 4 Train batch 1870/3877 Loss=3.3894 Acc=36.61%\n",
      "Epoch 4 Train batch 1880/3877 Loss=3.3889 Acc=36.62%\n",
      "Epoch 4 Train batch 1890/3877 Loss=3.3888 Acc=36.62%\n",
      "Epoch 4 Train batch 1900/3877 Loss=3.3885 Acc=36.63%\n",
      "Epoch 4 Train batch 1910/3877 Loss=3.3881 Acc=36.62%\n",
      "Epoch 4 Train batch 1920/3877 Loss=3.3881 Acc=36.63%\n",
      "Epoch 4 Train batch 1930/3877 Loss=3.3874 Acc=36.63%\n",
      "Epoch 4 Train batch 1940/3877 Loss=3.3868 Acc=36.64%\n",
      "Epoch 4 Train batch 1950/3877 Loss=3.3868 Acc=36.64%\n",
      "Epoch 4 Train batch 1960/3877 Loss=3.3872 Acc=36.64%\n",
      "Epoch 4 Train batch 1970/3877 Loss=3.3870 Acc=36.64%\n",
      "Epoch 4 Train batch 1980/3877 Loss=3.3872 Acc=36.64%\n",
      "Epoch 4 Train batch 1990/3877 Loss=3.3873 Acc=36.64%\n",
      "Epoch 4 Train batch 2000/3877 Loss=3.3870 Acc=36.64%\n",
      "Epoch 4 Train batch 2010/3877 Loss=3.3871 Acc=36.63%\n",
      "Epoch 4 Train batch 2020/3877 Loss=3.3868 Acc=36.63%\n",
      "Epoch 4 Train batch 2030/3877 Loss=3.3861 Acc=36.64%\n",
      "Epoch 4 Train batch 2040/3877 Loss=3.3858 Acc=36.64%\n",
      "Epoch 4 Train batch 2050/3877 Loss=3.3862 Acc=36.64%\n",
      "Epoch 4 Train batch 2060/3877 Loss=3.3858 Acc=36.64%\n",
      "Epoch 4 Train batch 2070/3877 Loss=3.3853 Acc=36.64%\n",
      "Epoch 4 Train batch 2080/3877 Loss=3.3848 Acc=36.65%\n",
      "Epoch 4 Train batch 2090/3877 Loss=3.3844 Acc=36.65%\n",
      "Epoch 4 Train batch 2100/3877 Loss=3.3841 Acc=36.65%\n",
      "Epoch 4 Train batch 2110/3877 Loss=3.3843 Acc=36.65%\n",
      "Epoch 4 Train batch 2120/3877 Loss=3.3842 Acc=36.66%\n",
      "Epoch 4 Train batch 2130/3877 Loss=3.3845 Acc=36.65%\n",
      "Epoch 4 Train batch 2140/3877 Loss=3.3847 Acc=36.65%\n",
      "Epoch 4 Train batch 2150/3877 Loss=3.3849 Acc=36.66%\n",
      "Epoch 4 Train batch 2160/3877 Loss=3.3846 Acc=36.66%\n",
      "Epoch 4 Train batch 2170/3877 Loss=3.3845 Acc=36.66%\n",
      "Epoch 4 Train batch 2180/3877 Loss=3.3845 Acc=36.66%\n",
      "Epoch 4 Train batch 2190/3877 Loss=3.3842 Acc=36.66%\n",
      "Epoch 4 Train batch 2200/3877 Loss=3.3838 Acc=36.67%\n",
      "Epoch 4 Train batch 2210/3877 Loss=3.3837 Acc=36.67%\n",
      "Epoch 4 Train batch 2220/3877 Loss=3.3835 Acc=36.68%\n",
      "Epoch 4 Train batch 2230/3877 Loss=3.3833 Acc=36.68%\n",
      "Epoch 4 Train batch 2240/3877 Loss=3.3831 Acc=36.68%\n",
      "Epoch 4 Train batch 2250/3877 Loss=3.3823 Acc=36.69%\n",
      "Epoch 4 Train batch 2260/3877 Loss=3.3823 Acc=36.69%\n",
      "Epoch 4 Train batch 2270/3877 Loss=3.3825 Acc=36.69%\n",
      "Epoch 4 Train batch 2280/3877 Loss=3.3818 Acc=36.69%\n",
      "Epoch 4 Train batch 2290/3877 Loss=3.3816 Acc=36.70%\n",
      "Epoch 4 Train batch 2300/3877 Loss=3.3813 Acc=36.70%\n",
      "Epoch 4 Train batch 2310/3877 Loss=3.3813 Acc=36.70%\n",
      "Epoch 4 Train batch 2320/3877 Loss=3.3808 Acc=36.71%\n",
      "Epoch 4 Train batch 2330/3877 Loss=3.3804 Acc=36.71%\n",
      "Epoch 4 Train batch 2340/3877 Loss=3.3805 Acc=36.72%\n",
      "Epoch 4 Train batch 2350/3877 Loss=3.3807 Acc=36.72%\n",
      "Epoch 4 Train batch 2360/3877 Loss=3.3802 Acc=36.72%\n",
      "Epoch 4 Train batch 2370/3877 Loss=3.3798 Acc=36.73%\n",
      "Epoch 4 Train batch 2380/3877 Loss=3.3793 Acc=36.73%\n",
      "Epoch 4 Train batch 2390/3877 Loss=3.3794 Acc=36.73%\n",
      "Epoch 4 Train batch 2400/3877 Loss=3.3793 Acc=36.73%\n",
      "Epoch 4 Train batch 2410/3877 Loss=3.3792 Acc=36.73%\n",
      "Epoch 4 Train batch 2420/3877 Loss=3.3791 Acc=36.73%\n",
      "Epoch 4 Train batch 2430/3877 Loss=3.3787 Acc=36.73%\n",
      "Epoch 4 Train batch 2440/3877 Loss=3.3788 Acc=36.73%\n",
      "Epoch 4 Train batch 2450/3877 Loss=3.3784 Acc=36.74%\n",
      "Epoch 4 Train batch 2460/3877 Loss=3.3785 Acc=36.74%\n",
      "Epoch 4 Train batch 2470/3877 Loss=3.3782 Acc=36.74%\n",
      "Epoch 4 Train batch 2480/3877 Loss=3.3781 Acc=36.74%\n",
      "Epoch 4 Train batch 2490/3877 Loss=3.3778 Acc=36.74%\n",
      "Epoch 4 Train batch 2500/3877 Loss=3.3778 Acc=36.74%\n",
      "Epoch 4 Train batch 2510/3877 Loss=3.3779 Acc=36.73%\n",
      "Epoch 4 Train batch 2520/3877 Loss=3.3776 Acc=36.74%\n",
      "Epoch 4 Train batch 2530/3877 Loss=3.3776 Acc=36.74%\n",
      "Epoch 4 Train batch 2540/3877 Loss=3.3775 Acc=36.74%\n",
      "Epoch 4 Train batch 2550/3877 Loss=3.3774 Acc=36.74%\n",
      "Epoch 4 Train batch 2560/3877 Loss=3.3773 Acc=36.74%\n",
      "Epoch 4 Train batch 2570/3877 Loss=3.3775 Acc=36.74%\n",
      "Epoch 4 Train batch 2580/3877 Loss=3.3773 Acc=36.74%\n",
      "Epoch 4 Train batch 2590/3877 Loss=3.3772 Acc=36.74%\n",
      "Epoch 4 Train batch 2600/3877 Loss=3.3771 Acc=36.75%\n",
      "Epoch 4 Train batch 2610/3877 Loss=3.3770 Acc=36.75%\n",
      "Epoch 4 Train batch 2620/3877 Loss=3.3776 Acc=36.74%\n",
      "Epoch 4 Train batch 2630/3877 Loss=3.3771 Acc=36.74%\n",
      "Epoch 4 Train batch 2640/3877 Loss=3.3769 Acc=36.75%\n",
      "Epoch 4 Train batch 2650/3877 Loss=3.3765 Acc=36.75%\n",
      "Epoch 4 Train batch 2660/3877 Loss=3.3764 Acc=36.75%\n",
      "Epoch 4 Train batch 2670/3877 Loss=3.3763 Acc=36.75%\n",
      "Epoch 4 Train batch 2680/3877 Loss=3.3762 Acc=36.75%\n",
      "Epoch 4 Train batch 2690/3877 Loss=3.3757 Acc=36.76%\n",
      "Epoch 4 Train batch 2700/3877 Loss=3.3756 Acc=36.76%\n",
      "Epoch 4 Train batch 2710/3877 Loss=3.3754 Acc=36.76%\n",
      "Epoch 4 Train batch 2720/3877 Loss=3.3752 Acc=36.76%\n",
      "Epoch 4 Train batch 2730/3877 Loss=3.3746 Acc=36.76%\n",
      "Epoch 4 Train batch 2740/3877 Loss=3.3747 Acc=36.77%\n",
      "Epoch 4 Train batch 2750/3877 Loss=3.3741 Acc=36.77%\n",
      "Epoch 4 Train batch 2760/3877 Loss=3.3743 Acc=36.77%\n",
      "Epoch 4 Train batch 2770/3877 Loss=3.3745 Acc=36.77%\n",
      "Epoch 4 Train batch 2780/3877 Loss=3.3742 Acc=36.76%\n",
      "Epoch 4 Train batch 2790/3877 Loss=3.3742 Acc=36.76%\n",
      "Epoch 4 Train batch 2800/3877 Loss=3.3739 Acc=36.76%\n",
      "Epoch 4 Train batch 2810/3877 Loss=3.3736 Acc=36.76%\n",
      "Epoch 4 Train batch 2820/3877 Loss=3.3734 Acc=36.76%\n",
      "Epoch 4 Train batch 2830/3877 Loss=3.3732 Acc=36.77%\n",
      "Epoch 4 Train batch 2840/3877 Loss=3.3733 Acc=36.77%\n",
      "Epoch 4 Train batch 2850/3877 Loss=3.3730 Acc=36.77%\n",
      "Epoch 4 Train batch 2860/3877 Loss=3.3729 Acc=36.77%\n",
      "Epoch 4 Train batch 2870/3877 Loss=3.3730 Acc=36.77%\n",
      "Epoch 4 Train batch 2880/3877 Loss=3.3728 Acc=36.77%\n",
      "Epoch 4 Train batch 2890/3877 Loss=3.3727 Acc=36.77%\n",
      "Epoch 4 Train batch 2900/3877 Loss=3.3723 Acc=36.78%\n",
      "Epoch 4 Train batch 2910/3877 Loss=3.3723 Acc=36.78%\n",
      "Epoch 4 Train batch 2920/3877 Loss=3.3720 Acc=36.78%\n",
      "Epoch 4 Train batch 2930/3877 Loss=3.3722 Acc=36.78%\n",
      "Epoch 4 Train batch 2940/3877 Loss=3.3718 Acc=36.78%\n",
      "Epoch 4 Train batch 2950/3877 Loss=3.3716 Acc=36.79%\n",
      "Epoch 4 Train batch 2960/3877 Loss=3.3716 Acc=36.79%\n",
      "Epoch 4 Train batch 2970/3877 Loss=3.3712 Acc=36.79%\n",
      "Epoch 4 Train batch 2980/3877 Loss=3.3714 Acc=36.78%\n",
      "Epoch 4 Train batch 2990/3877 Loss=3.3714 Acc=36.78%\n",
      "Epoch 4 Train batch 3000/3877 Loss=3.3711 Acc=36.79%\n",
      "Epoch 4 Train batch 3010/3877 Loss=3.3710 Acc=36.79%\n",
      "Epoch 4 Train batch 3020/3877 Loss=3.3709 Acc=36.79%\n",
      "Epoch 4 Train batch 3030/3877 Loss=3.3708 Acc=36.79%\n",
      "Epoch 4 Train batch 3040/3877 Loss=3.3709 Acc=36.79%\n",
      "Epoch 4 Train batch 3050/3877 Loss=3.3705 Acc=36.80%\n",
      "Epoch 4 Train batch 3060/3877 Loss=3.3703 Acc=36.80%\n",
      "Epoch 4 Train batch 3070/3877 Loss=3.3704 Acc=36.80%\n",
      "Epoch 4 Train batch 3080/3877 Loss=3.3707 Acc=36.80%\n",
      "Epoch 4 Train batch 3090/3877 Loss=3.3707 Acc=36.80%\n",
      "Epoch 4 Train batch 3100/3877 Loss=3.3707 Acc=36.80%\n",
      "Epoch 4 Train batch 3110/3877 Loss=3.3706 Acc=36.80%\n",
      "Epoch 4 Train batch 3120/3877 Loss=3.3703 Acc=36.81%\n",
      "Epoch 4 Train batch 3130/3877 Loss=3.3700 Acc=36.81%\n",
      "Epoch 4 Train batch 3140/3877 Loss=3.3699 Acc=36.81%\n",
      "Epoch 4 Train batch 3150/3877 Loss=3.3699 Acc=36.81%\n",
      "Epoch 4 Train batch 3160/3877 Loss=3.3703 Acc=36.81%\n",
      "Epoch 4 Train batch 3170/3877 Loss=3.3704 Acc=36.81%\n",
      "Epoch 4 Train batch 3180/3877 Loss=3.3701 Acc=36.81%\n",
      "Epoch 4 Train batch 3190/3877 Loss=3.3700 Acc=36.81%\n",
      "Epoch 4 Train batch 3200/3877 Loss=3.3698 Acc=36.82%\n",
      "Epoch 4 Train batch 3210/3877 Loss=3.3699 Acc=36.81%\n",
      "Epoch 4 Train batch 3220/3877 Loss=3.3697 Acc=36.81%\n",
      "Epoch 4 Train batch 3230/3877 Loss=3.3696 Acc=36.81%\n",
      "Epoch 4 Train batch 3240/3877 Loss=3.3695 Acc=36.81%\n",
      "Epoch 4 Train batch 3250/3877 Loss=3.3694 Acc=36.81%\n",
      "Epoch 4 Train batch 3260/3877 Loss=3.3695 Acc=36.81%\n",
      "Epoch 4 Train batch 3270/3877 Loss=3.3691 Acc=36.81%\n",
      "Epoch 4 Train batch 3280/3877 Loss=3.3690 Acc=36.81%\n",
      "Epoch 4 Train batch 3290/3877 Loss=3.3687 Acc=36.81%\n",
      "Epoch 4 Train batch 3300/3877 Loss=3.3680 Acc=36.82%\n",
      "Epoch 4 Train batch 3310/3877 Loss=3.3679 Acc=36.82%\n",
      "Epoch 4 Train batch 3320/3877 Loss=3.3677 Acc=36.82%\n",
      "Epoch 4 Train batch 3330/3877 Loss=3.3676 Acc=36.82%\n",
      "Epoch 4 Train batch 3340/3877 Loss=3.3671 Acc=36.82%\n",
      "Epoch 4 Train batch 3350/3877 Loss=3.3669 Acc=36.82%\n",
      "Epoch 4 Train batch 3360/3877 Loss=3.3668 Acc=36.83%\n",
      "Epoch 4 Train batch 3370/3877 Loss=3.3666 Acc=36.83%\n",
      "Epoch 4 Train batch 3380/3877 Loss=3.3664 Acc=36.83%\n",
      "Epoch 4 Train batch 3390/3877 Loss=3.3665 Acc=36.83%\n",
      "Epoch 4 Train batch 3400/3877 Loss=3.3664 Acc=36.83%\n",
      "Epoch 4 Train batch 3410/3877 Loss=3.3658 Acc=36.84%\n",
      "Epoch 4 Train batch 3420/3877 Loss=3.3656 Acc=36.84%\n",
      "Epoch 4 Train batch 3430/3877 Loss=3.3656 Acc=36.84%\n",
      "Epoch 4 Train batch 3440/3877 Loss=3.3652 Acc=36.85%\n",
      "Epoch 4 Train batch 3450/3877 Loss=3.3648 Acc=36.85%\n",
      "Epoch 4 Train batch 3460/3877 Loss=3.3646 Acc=36.85%\n",
      "Epoch 4 Train batch 3470/3877 Loss=3.3646 Acc=36.85%\n",
      "Epoch 4 Train batch 3480/3877 Loss=3.3645 Acc=36.85%\n",
      "Epoch 4 Train batch 3490/3877 Loss=3.3643 Acc=36.85%\n",
      "Epoch 4 Train batch 3500/3877 Loss=3.3639 Acc=36.85%\n",
      "Epoch 4 Train batch 3510/3877 Loss=3.3637 Acc=36.85%\n",
      "Epoch 4 Train batch 3520/3877 Loss=3.3633 Acc=36.86%\n",
      "Epoch 4 Train batch 3530/3877 Loss=3.3629 Acc=36.86%\n",
      "Epoch 4 Train batch 3540/3877 Loss=3.3626 Acc=36.86%\n",
      "Epoch 4 Train batch 3550/3877 Loss=3.3621 Acc=36.87%\n",
      "Epoch 4 Train batch 3560/3877 Loss=3.3618 Acc=36.87%\n",
      "Epoch 4 Train batch 3570/3877 Loss=3.3615 Acc=36.88%\n",
      "Epoch 4 Train batch 3580/3877 Loss=3.3612 Acc=36.88%\n",
      "Epoch 4 Train batch 3590/3877 Loss=3.3611 Acc=36.88%\n",
      "Epoch 4 Train batch 3600/3877 Loss=3.3612 Acc=36.88%\n",
      "Epoch 4 Train batch 3610/3877 Loss=3.3608 Acc=36.89%\n",
      "Epoch 4 Train batch 3620/3877 Loss=3.3606 Acc=36.89%\n",
      "Epoch 4 Train batch 3630/3877 Loss=3.3605 Acc=36.89%\n",
      "Epoch 4 Train batch 3640/3877 Loss=3.3607 Acc=36.89%\n",
      "Epoch 4 Train batch 3650/3877 Loss=3.3605 Acc=36.89%\n",
      "Epoch 4 Train batch 3660/3877 Loss=3.3607 Acc=36.89%\n",
      "Epoch 4 Train batch 3670/3877 Loss=3.3604 Acc=36.89%\n",
      "Epoch 4 Train batch 3680/3877 Loss=3.3603 Acc=36.89%\n",
      "Epoch 4 Train batch 3690/3877 Loss=3.3599 Acc=36.89%\n",
      "Epoch 4 Train batch 3700/3877 Loss=3.3597 Acc=36.90%\n",
      "Epoch 4 Train batch 3710/3877 Loss=3.3598 Acc=36.89%\n",
      "Epoch 4 Train batch 3720/3877 Loss=3.3600 Acc=36.89%\n",
      "Epoch 4 Train batch 3730/3877 Loss=3.3598 Acc=36.89%\n",
      "Epoch 4 Train batch 3740/3877 Loss=3.3593 Acc=36.90%\n",
      "Epoch 4 Train batch 3750/3877 Loss=3.3593 Acc=36.90%\n",
      "Epoch 4 Train batch 3760/3877 Loss=3.3593 Acc=36.90%\n",
      "Epoch 4 Train batch 3770/3877 Loss=3.3593 Acc=36.90%\n",
      "Epoch 4 Train batch 3780/3877 Loss=3.3589 Acc=36.90%\n",
      "Epoch 4 Train batch 3790/3877 Loss=3.3585 Acc=36.91%\n",
      "Epoch 4 Train batch 3800/3877 Loss=3.3584 Acc=36.91%\n",
      "Epoch 4 Train batch 3810/3877 Loss=3.3581 Acc=36.91%\n",
      "Epoch 4 Train batch 3820/3877 Loss=3.3582 Acc=36.91%\n",
      "Epoch 4 Train batch 3830/3877 Loss=3.3580 Acc=36.91%\n",
      "Epoch 4 Train batch 3840/3877 Loss=3.3578 Acc=36.92%\n",
      "Epoch 4 Train batch 3850/3877 Loss=3.3579 Acc=36.92%\n",
      "Epoch 4 Train batch 3860/3877 Loss=3.3579 Acc=36.91%\n",
      "Epoch 4 Train batch 3870/3877 Loss=3.3580 Acc=36.92%\n",
      "Epoch 4 Train batch 3877/3877 Loss=3.3578 Acc=36.92%\n",
      "Epoch 4/5 train_loss=3.3578 train_acc=36.92% val_loss=3.2262 val_acc=38.71%\n",
      "Epoch 5 Train batch 10/3877 Loss=3.2011 Acc=37.80%\n",
      "Epoch 5 Train batch 20/3877 Loss=3.1855 Acc=38.00%\n",
      "Epoch 5 Train batch 30/3877 Loss=3.1947 Acc=38.13%\n",
      "Epoch 5 Train batch 40/3877 Loss=3.1912 Acc=38.29%\n",
      "Epoch 5 Train batch 50/3877 Loss=3.2071 Acc=37.95%\n",
      "Epoch 5 Train batch 60/3877 Loss=3.2146 Acc=37.89%\n",
      "Epoch 5 Train batch 70/3877 Loss=3.2209 Acc=37.95%\n",
      "Epoch 5 Train batch 80/3877 Loss=3.2118 Acc=37.98%\n",
      "Epoch 5 Train batch 90/3877 Loss=3.2036 Acc=38.03%\n",
      "Epoch 5 Train batch 100/3877 Loss=3.2038 Acc=37.95%\n",
      "Epoch 5 Train batch 110/3877 Loss=3.2059 Acc=37.91%\n",
      "Epoch 5 Train batch 120/3877 Loss=3.2076 Acc=37.91%\n",
      "Epoch 5 Train batch 130/3877 Loss=3.2059 Acc=37.96%\n",
      "Epoch 5 Train batch 140/3877 Loss=3.2109 Acc=37.90%\n",
      "Epoch 5 Train batch 150/3877 Loss=3.2090 Acc=37.95%\n",
      "Epoch 5 Train batch 160/3877 Loss=3.2084 Acc=37.95%\n",
      "Epoch 5 Train batch 170/3877 Loss=3.2020 Acc=38.02%\n",
      "Epoch 5 Train batch 180/3877 Loss=3.2050 Acc=38.01%\n",
      "Epoch 5 Train batch 190/3877 Loss=3.2004 Acc=38.03%\n",
      "Epoch 5 Train batch 200/3877 Loss=3.2016 Acc=38.03%\n",
      "Epoch 5 Train batch 210/3877 Loss=3.2031 Acc=38.07%\n",
      "Epoch 5 Train batch 220/3877 Loss=3.2011 Acc=38.06%\n",
      "Epoch 5 Train batch 230/3877 Loss=3.2046 Acc=38.05%\n",
      "Epoch 5 Train batch 240/3877 Loss=3.2026 Acc=38.07%\n",
      "Epoch 5 Train batch 250/3877 Loss=3.1999 Acc=38.15%\n",
      "Epoch 5 Train batch 260/3877 Loss=3.1998 Acc=38.18%\n",
      "Epoch 5 Train batch 270/3877 Loss=3.2009 Acc=38.19%\n",
      "Epoch 5 Train batch 280/3877 Loss=3.2020 Acc=38.17%\n",
      "Epoch 5 Train batch 290/3877 Loss=3.2002 Acc=38.18%\n",
      "Epoch 5 Train batch 300/3877 Loss=3.2034 Acc=38.14%\n",
      "Epoch 5 Train batch 310/3877 Loss=3.2076 Acc=38.09%\n",
      "Epoch 5 Train batch 320/3877 Loss=3.2100 Acc=38.05%\n",
      "Epoch 5 Train batch 330/3877 Loss=3.2094 Acc=38.03%\n",
      "Epoch 5 Train batch 340/3877 Loss=3.2109 Acc=37.99%\n",
      "Epoch 5 Train batch 350/3877 Loss=3.2123 Acc=37.99%\n",
      "Epoch 5 Train batch 360/3877 Loss=3.2115 Acc=38.01%\n",
      "Epoch 5 Train batch 370/3877 Loss=3.2131 Acc=38.01%\n",
      "Epoch 5 Train batch 380/3877 Loss=3.2133 Acc=38.00%\n",
      "Epoch 5 Train batch 390/3877 Loss=3.2150 Acc=37.98%\n",
      "Epoch 5 Train batch 400/3877 Loss=3.2156 Acc=37.97%\n",
      "Epoch 5 Train batch 410/3877 Loss=3.2145 Acc=37.99%\n",
      "Epoch 5 Train batch 420/3877 Loss=3.2159 Acc=37.96%\n",
      "Epoch 5 Train batch 430/3877 Loss=3.2173 Acc=37.98%\n",
      "Epoch 5 Train batch 440/3877 Loss=3.2181 Acc=37.99%\n",
      "Epoch 5 Train batch 450/3877 Loss=3.2194 Acc=37.99%\n",
      "Epoch 5 Train batch 460/3877 Loss=3.2186 Acc=37.99%\n",
      "Epoch 5 Train batch 470/3877 Loss=3.2175 Acc=37.97%\n",
      "Epoch 5 Train batch 480/3877 Loss=3.2160 Acc=37.98%\n",
      "Epoch 5 Train batch 490/3877 Loss=3.2158 Acc=37.99%\n",
      "Epoch 5 Train batch 500/3877 Loss=3.2165 Acc=37.97%\n",
      "Epoch 5 Train batch 510/3877 Loss=3.2177 Acc=37.96%\n",
      "Epoch 5 Train batch 520/3877 Loss=3.2157 Acc=37.99%\n",
      "Epoch 5 Train batch 530/3877 Loss=3.2158 Acc=37.99%\n",
      "Epoch 5 Train batch 540/3877 Loss=3.2160 Acc=37.97%\n",
      "Epoch 5 Train batch 550/3877 Loss=3.2129 Acc=38.02%\n",
      "Epoch 5 Train batch 560/3877 Loss=3.2140 Acc=38.02%\n",
      "Epoch 5 Train batch 570/3877 Loss=3.2142 Acc=38.01%\n",
      "Epoch 5 Train batch 580/3877 Loss=3.2136 Acc=38.04%\n",
      "Epoch 5 Train batch 590/3877 Loss=3.2148 Acc=38.04%\n",
      "Epoch 5 Train batch 600/3877 Loss=3.2152 Acc=38.04%\n",
      "Epoch 5 Train batch 610/3877 Loss=3.2153 Acc=38.03%\n",
      "Epoch 5 Train batch 620/3877 Loss=3.2150 Acc=38.03%\n",
      "Epoch 5 Train batch 630/3877 Loss=3.2145 Acc=38.05%\n",
      "Epoch 5 Train batch 640/3877 Loss=3.2144 Acc=38.06%\n",
      "Epoch 5 Train batch 650/3877 Loss=3.2130 Acc=38.05%\n",
      "Epoch 5 Train batch 660/3877 Loss=3.2119 Acc=38.07%\n",
      "Epoch 5 Train batch 670/3877 Loss=3.2119 Acc=38.06%\n",
      "Epoch 5 Train batch 680/3877 Loss=3.2112 Acc=38.07%\n",
      "Epoch 5 Train batch 690/3877 Loss=3.2122 Acc=38.07%\n",
      "Epoch 5 Train batch 700/3877 Loss=3.2131 Acc=38.05%\n",
      "Epoch 5 Train batch 710/3877 Loss=3.2153 Acc=38.01%\n",
      "Epoch 5 Train batch 720/3877 Loss=3.2155 Acc=38.01%\n",
      "Epoch 5 Train batch 730/3877 Loss=3.2157 Acc=38.00%\n",
      "Epoch 5 Train batch 740/3877 Loss=3.2157 Acc=38.01%\n",
      "Epoch 5 Train batch 750/3877 Loss=3.2156 Acc=38.00%\n",
      "Epoch 5 Train batch 760/3877 Loss=3.2157 Acc=38.01%\n",
      "Epoch 5 Train batch 770/3877 Loss=3.2162 Acc=38.01%\n",
      "Epoch 5 Train batch 780/3877 Loss=3.2159 Acc=38.00%\n",
      "Epoch 5 Train batch 790/3877 Loss=3.2169 Acc=37.98%\n",
      "Epoch 5 Train batch 800/3877 Loss=3.2170 Acc=37.98%\n",
      "Epoch 5 Train batch 810/3877 Loss=3.2171 Acc=37.97%\n",
      "Epoch 5 Train batch 820/3877 Loss=3.2171 Acc=37.97%\n",
      "Epoch 5 Train batch 830/3877 Loss=3.2168 Acc=37.98%\n",
      "Epoch 5 Train batch 840/3877 Loss=3.2163 Acc=37.99%\n",
      "Epoch 5 Train batch 850/3877 Loss=3.2162 Acc=37.98%\n",
      "Epoch 5 Train batch 860/3877 Loss=3.2160 Acc=38.00%\n",
      "Epoch 5 Train batch 870/3877 Loss=3.2171 Acc=37.98%\n",
      "Epoch 5 Train batch 880/3877 Loss=3.2166 Acc=38.00%\n",
      "Epoch 5 Train batch 890/3877 Loss=3.2167 Acc=37.99%\n",
      "Epoch 5 Train batch 900/3877 Loss=3.2163 Acc=37.99%\n",
      "Epoch 5 Train batch 910/3877 Loss=3.2159 Acc=38.01%\n",
      "Epoch 5 Train batch 920/3877 Loss=3.2156 Acc=38.01%\n",
      "Epoch 5 Train batch 930/3877 Loss=3.2157 Acc=38.02%\n",
      "Epoch 5 Train batch 940/3877 Loss=3.2153 Acc=38.03%\n",
      "Epoch 5 Train batch 950/3877 Loss=3.2143 Acc=38.03%\n",
      "Epoch 5 Train batch 960/3877 Loss=3.2146 Acc=38.03%\n",
      "Epoch 5 Train batch 970/3877 Loss=3.2159 Acc=38.02%\n",
      "Epoch 5 Train batch 980/3877 Loss=3.2155 Acc=38.03%\n",
      "Epoch 5 Train batch 990/3877 Loss=3.2153 Acc=38.03%\n",
      "Epoch 5 Train batch 1000/3877 Loss=3.2147 Acc=38.04%\n",
      "Epoch 5 Train batch 1010/3877 Loss=3.2155 Acc=38.03%\n",
      "Epoch 5 Train batch 1020/3877 Loss=3.2162 Acc=38.03%\n",
      "Epoch 5 Train batch 1030/3877 Loss=3.2160 Acc=38.02%\n",
      "Epoch 5 Train batch 1040/3877 Loss=3.2159 Acc=38.03%\n",
      "Epoch 5 Train batch 1050/3877 Loss=3.2167 Acc=38.01%\n",
      "Epoch 5 Train batch 1060/3877 Loss=3.2161 Acc=38.02%\n",
      "Epoch 5 Train batch 1070/3877 Loss=3.2156 Acc=38.03%\n",
      "Epoch 5 Train batch 1080/3877 Loss=3.2150 Acc=38.03%\n",
      "Epoch 5 Train batch 1090/3877 Loss=3.2150 Acc=38.04%\n",
      "Epoch 5 Train batch 1100/3877 Loss=3.2156 Acc=38.03%\n",
      "Epoch 5 Train batch 1110/3877 Loss=3.2149 Acc=38.03%\n",
      "Epoch 5 Train batch 1120/3877 Loss=3.2153 Acc=38.03%\n",
      "Epoch 5 Train batch 1130/3877 Loss=3.2152 Acc=38.02%\n",
      "Epoch 5 Train batch 1140/3877 Loss=3.2143 Acc=38.04%\n",
      "Epoch 5 Train batch 1150/3877 Loss=3.2133 Acc=38.05%\n",
      "Epoch 5 Train batch 1160/3877 Loss=3.2133 Acc=38.05%\n",
      "Epoch 5 Train batch 1170/3877 Loss=3.2120 Acc=38.07%\n",
      "Epoch 5 Train batch 1180/3877 Loss=3.2125 Acc=38.06%\n",
      "Epoch 5 Train batch 1190/3877 Loss=3.2123 Acc=38.06%\n",
      "Epoch 5 Train batch 1200/3877 Loss=3.2128 Acc=38.06%\n",
      "Epoch 5 Train batch 1210/3877 Loss=3.2138 Acc=38.05%\n",
      "Epoch 5 Train batch 1220/3877 Loss=3.2134 Acc=38.04%\n",
      "Epoch 5 Train batch 1230/3877 Loss=3.2131 Acc=38.04%\n",
      "Epoch 5 Train batch 1240/3877 Loss=3.2135 Acc=38.04%\n",
      "Epoch 5 Train batch 1250/3877 Loss=3.2133 Acc=38.05%\n",
      "Epoch 5 Train batch 1260/3877 Loss=3.2132 Acc=38.05%\n",
      "Epoch 5 Train batch 1270/3877 Loss=3.2132 Acc=38.06%\n",
      "Epoch 5 Train batch 1280/3877 Loss=3.2130 Acc=38.06%\n",
      "Epoch 5 Train batch 1290/3877 Loss=3.2130 Acc=38.06%\n",
      "Epoch 5 Train batch 1300/3877 Loss=3.2130 Acc=38.06%\n",
      "Epoch 5 Train batch 1310/3877 Loss=3.2131 Acc=38.07%\n",
      "Epoch 5 Train batch 1320/3877 Loss=3.2129 Acc=38.06%\n",
      "Epoch 5 Train batch 1330/3877 Loss=3.2124 Acc=38.07%\n",
      "Epoch 5 Train batch 1340/3877 Loss=3.2130 Acc=38.06%\n",
      "Epoch 5 Train batch 1350/3877 Loss=3.2125 Acc=38.07%\n",
      "Epoch 5 Train batch 1360/3877 Loss=3.2118 Acc=38.08%\n",
      "Epoch 5 Train batch 1370/3877 Loss=3.2118 Acc=38.10%\n",
      "Epoch 5 Train batch 1380/3877 Loss=3.2117 Acc=38.10%\n",
      "Epoch 5 Train batch 1390/3877 Loss=3.2115 Acc=38.11%\n",
      "Epoch 5 Train batch 1400/3877 Loss=3.2113 Acc=38.11%\n",
      "Epoch 5 Train batch 1410/3877 Loss=3.2116 Acc=38.10%\n",
      "Epoch 5 Train batch 1420/3877 Loss=3.2120 Acc=38.10%\n",
      "Epoch 5 Train batch 1430/3877 Loss=3.2121 Acc=38.10%\n",
      "Epoch 5 Train batch 1440/3877 Loss=3.2112 Acc=38.11%\n",
      "Epoch 5 Train batch 1450/3877 Loss=3.2112 Acc=38.11%\n",
      "Epoch 5 Train batch 1460/3877 Loss=3.2112 Acc=38.11%\n",
      "Epoch 5 Train batch 1470/3877 Loss=3.2112 Acc=38.10%\n",
      "Epoch 5 Train batch 1480/3877 Loss=3.2112 Acc=38.10%\n",
      "Epoch 5 Train batch 1490/3877 Loss=3.2111 Acc=38.11%\n",
      "Epoch 5 Train batch 1500/3877 Loss=3.2111 Acc=38.12%\n",
      "Epoch 5 Train batch 1510/3877 Loss=3.2112 Acc=38.12%\n",
      "Epoch 5 Train batch 1520/3877 Loss=3.2101 Acc=38.13%\n",
      "Epoch 5 Train batch 1530/3877 Loss=3.2100 Acc=38.13%\n",
      "Epoch 5 Train batch 1540/3877 Loss=3.2101 Acc=38.13%\n",
      "Epoch 5 Train batch 1550/3877 Loss=3.2099 Acc=38.13%\n",
      "Epoch 5 Train batch 1560/3877 Loss=3.2102 Acc=38.12%\n",
      "Epoch 5 Train batch 1570/3877 Loss=3.2104 Acc=38.12%\n",
      "Epoch 5 Train batch 1580/3877 Loss=3.2100 Acc=38.12%\n",
      "Epoch 5 Train batch 1590/3877 Loss=3.2096 Acc=38.12%\n",
      "Epoch 5 Train batch 1600/3877 Loss=3.2091 Acc=38.12%\n",
      "Epoch 5 Train batch 1610/3877 Loss=3.2093 Acc=38.12%\n",
      "Epoch 5 Train batch 1620/3877 Loss=3.2100 Acc=38.11%\n",
      "Epoch 5 Train batch 1630/3877 Loss=3.2098 Acc=38.11%\n",
      "Epoch 5 Train batch 1640/3877 Loss=3.2096 Acc=38.11%\n",
      "Epoch 5 Train batch 1650/3877 Loss=3.2095 Acc=38.11%\n",
      "Epoch 5 Train batch 1660/3877 Loss=3.2092 Acc=38.11%\n",
      "Epoch 5 Train batch 1670/3877 Loss=3.2092 Acc=38.11%\n",
      "Epoch 5 Train batch 1680/3877 Loss=3.2094 Acc=38.10%\n",
      "Epoch 5 Train batch 1690/3877 Loss=3.2095 Acc=38.10%\n",
      "Epoch 5 Train batch 1700/3877 Loss=3.2092 Acc=38.10%\n",
      "Epoch 5 Train batch 1710/3877 Loss=3.2093 Acc=38.10%\n",
      "Epoch 5 Train batch 1720/3877 Loss=3.2092 Acc=38.10%\n",
      "Epoch 5 Train batch 1730/3877 Loss=3.2093 Acc=38.10%\n",
      "Epoch 5 Train batch 1740/3877 Loss=3.2090 Acc=38.10%\n",
      "Epoch 5 Train batch 1750/3877 Loss=3.2085 Acc=38.11%\n",
      "Epoch 5 Train batch 1760/3877 Loss=3.2078 Acc=38.12%\n",
      "Epoch 5 Train batch 1770/3877 Loss=3.2074 Acc=38.13%\n",
      "Epoch 5 Train batch 1780/3877 Loss=3.2066 Acc=38.14%\n",
      "Epoch 5 Train batch 1790/3877 Loss=3.2069 Acc=38.13%\n",
      "Epoch 5 Train batch 1800/3877 Loss=3.2069 Acc=38.14%\n",
      "Epoch 5 Train batch 1810/3877 Loss=3.2077 Acc=38.13%\n",
      "Epoch 5 Train batch 1820/3877 Loss=3.2075 Acc=38.13%\n",
      "Epoch 5 Train batch 1830/3877 Loss=3.2077 Acc=38.13%\n",
      "Epoch 5 Train batch 1840/3877 Loss=3.2073 Acc=38.13%\n",
      "Epoch 5 Train batch 1850/3877 Loss=3.2072 Acc=38.13%\n",
      "Epoch 5 Train batch 1860/3877 Loss=3.2072 Acc=38.13%\n",
      "Epoch 5 Train batch 1870/3877 Loss=3.2071 Acc=38.13%\n",
      "Epoch 5 Train batch 1880/3877 Loss=3.2067 Acc=38.14%\n",
      "Epoch 5 Train batch 1890/3877 Loss=3.2067 Acc=38.14%\n",
      "Epoch 5 Train batch 1900/3877 Loss=3.2069 Acc=38.14%\n",
      "Epoch 5 Train batch 1910/3877 Loss=3.2067 Acc=38.14%\n",
      "Epoch 5 Train batch 1920/3877 Loss=3.2063 Acc=38.15%\n",
      "Epoch 5 Train batch 1930/3877 Loss=3.2062 Acc=38.15%\n",
      "Epoch 5 Train batch 1940/3877 Loss=3.2063 Acc=38.15%\n",
      "Epoch 5 Train batch 1950/3877 Loss=3.2056 Acc=38.16%\n",
      "Epoch 5 Train batch 1960/3877 Loss=3.2056 Acc=38.16%\n",
      "Epoch 5 Train batch 1970/3877 Loss=3.2054 Acc=38.16%\n",
      "Epoch 5 Train batch 1980/3877 Loss=3.2052 Acc=38.16%\n",
      "Epoch 5 Train batch 1990/3877 Loss=3.2056 Acc=38.15%\n",
      "Epoch 5 Train batch 2000/3877 Loss=3.2055 Acc=38.15%\n",
      "Epoch 5 Train batch 2010/3877 Loss=3.2055 Acc=38.15%\n",
      "Epoch 5 Train batch 2020/3877 Loss=3.2053 Acc=38.15%\n",
      "Epoch 5 Train batch 2030/3877 Loss=3.2053 Acc=38.14%\n",
      "Epoch 5 Train batch 2040/3877 Loss=3.2052 Acc=38.15%\n",
      "Epoch 5 Train batch 2050/3877 Loss=3.2046 Acc=38.15%\n",
      "Epoch 5 Train batch 2060/3877 Loss=3.2045 Acc=38.15%\n",
      "Epoch 5 Train batch 2070/3877 Loss=3.2043 Acc=38.15%\n",
      "Epoch 5 Train batch 2080/3877 Loss=3.2039 Acc=38.15%\n",
      "Epoch 5 Train batch 2090/3877 Loss=3.2040 Acc=38.15%\n",
      "Epoch 5 Train batch 2100/3877 Loss=3.2040 Acc=38.15%\n",
      "Epoch 5 Train batch 2110/3877 Loss=3.2036 Acc=38.15%\n",
      "Epoch 5 Train batch 2120/3877 Loss=3.2035 Acc=38.15%\n",
      "Epoch 5 Train batch 2130/3877 Loss=3.2035 Acc=38.16%\n",
      "Epoch 5 Train batch 2140/3877 Loss=3.2034 Acc=38.16%\n",
      "Epoch 5 Train batch 2150/3877 Loss=3.2030 Acc=38.16%\n",
      "Epoch 5 Train batch 2160/3877 Loss=3.2023 Acc=38.17%\n",
      "Epoch 5 Train batch 2170/3877 Loss=3.2017 Acc=38.18%\n",
      "Epoch 5 Train batch 2180/3877 Loss=3.2021 Acc=38.18%\n",
      "Epoch 5 Train batch 2190/3877 Loss=3.2017 Acc=38.18%\n",
      "Epoch 5 Train batch 2200/3877 Loss=3.2017 Acc=38.19%\n",
      "Epoch 5 Train batch 2210/3877 Loss=3.2019 Acc=38.19%\n",
      "Epoch 5 Train batch 2220/3877 Loss=3.2023 Acc=38.18%\n",
      "Epoch 5 Train batch 2230/3877 Loss=3.2022 Acc=38.18%\n",
      "Epoch 5 Train batch 2240/3877 Loss=3.2018 Acc=38.18%\n",
      "Epoch 5 Train batch 2250/3877 Loss=3.2016 Acc=38.19%\n",
      "Epoch 5 Train batch 2260/3877 Loss=3.2015 Acc=38.19%\n",
      "Epoch 5 Train batch 2270/3877 Loss=3.2014 Acc=38.20%\n",
      "Epoch 5 Train batch 2280/3877 Loss=3.2009 Acc=38.20%\n",
      "Epoch 5 Train batch 2290/3877 Loss=3.2009 Acc=38.20%\n",
      "Epoch 5 Train batch 2300/3877 Loss=3.2010 Acc=38.20%\n",
      "Epoch 5 Train batch 2310/3877 Loss=3.2006 Acc=38.21%\n",
      "Epoch 5 Train batch 2320/3877 Loss=3.2005 Acc=38.21%\n",
      "Epoch 5 Train batch 2330/3877 Loss=3.1999 Acc=38.22%\n",
      "Epoch 5 Train batch 2340/3877 Loss=3.1998 Acc=38.22%\n",
      "Epoch 5 Train batch 2350/3877 Loss=3.1999 Acc=38.21%\n",
      "Epoch 5 Train batch 2360/3877 Loss=3.2000 Acc=38.22%\n",
      "Epoch 5 Train batch 2370/3877 Loss=3.2002 Acc=38.22%\n",
      "Epoch 5 Train batch 2380/3877 Loss=3.2000 Acc=38.22%\n",
      "Epoch 5 Train batch 2390/3877 Loss=3.2000 Acc=38.22%\n",
      "Epoch 5 Train batch 2400/3877 Loss=3.1996 Acc=38.22%\n",
      "Epoch 5 Train batch 2410/3877 Loss=3.1999 Acc=38.22%\n",
      "Epoch 5 Train batch 2420/3877 Loss=3.2000 Acc=38.21%\n",
      "Epoch 5 Train batch 2430/3877 Loss=3.2002 Acc=38.21%\n",
      "Epoch 5 Train batch 2440/3877 Loss=3.1999 Acc=38.22%\n",
      "Epoch 5 Train batch 2450/3877 Loss=3.1996 Acc=38.22%\n",
      "Epoch 5 Train batch 2460/3877 Loss=3.1994 Acc=38.23%\n",
      "Epoch 5 Train batch 2470/3877 Loss=3.1992 Acc=38.23%\n",
      "Epoch 5 Train batch 2480/3877 Loss=3.1990 Acc=38.23%\n",
      "Epoch 5 Train batch 2490/3877 Loss=3.1989 Acc=38.23%\n",
      "Epoch 5 Train batch 2500/3877 Loss=3.1988 Acc=38.23%\n",
      "Epoch 5 Train batch 2510/3877 Loss=3.1986 Acc=38.24%\n",
      "Epoch 5 Train batch 2520/3877 Loss=3.1985 Acc=38.24%\n",
      "Epoch 5 Train batch 2530/3877 Loss=3.1985 Acc=38.24%\n",
      "Epoch 5 Train batch 2540/3877 Loss=3.1984 Acc=38.25%\n",
      "Epoch 5 Train batch 2550/3877 Loss=3.1982 Acc=38.25%\n",
      "Epoch 5 Train batch 2560/3877 Loss=3.1981 Acc=38.25%\n",
      "Epoch 5 Train batch 2570/3877 Loss=3.1979 Acc=38.25%\n",
      "Epoch 5 Train batch 2580/3877 Loss=3.1982 Acc=38.25%\n",
      "Epoch 5 Train batch 2590/3877 Loss=3.1984 Acc=38.24%\n",
      "Epoch 5 Train batch 2600/3877 Loss=3.1986 Acc=38.24%\n",
      "Epoch 5 Train batch 2610/3877 Loss=3.1989 Acc=38.24%\n",
      "Epoch 5 Train batch 2620/3877 Loss=3.1988 Acc=38.24%\n",
      "Epoch 5 Train batch 2630/3877 Loss=3.1986 Acc=38.24%\n",
      "Epoch 5 Train batch 2640/3877 Loss=3.1980 Acc=38.25%\n",
      "Epoch 5 Train batch 2650/3877 Loss=3.1979 Acc=38.25%\n",
      "Epoch 5 Train batch 2660/3877 Loss=3.1983 Acc=38.24%\n",
      "Epoch 5 Train batch 2670/3877 Loss=3.1983 Acc=38.24%\n",
      "Epoch 5 Train batch 2680/3877 Loss=3.1983 Acc=38.24%\n",
      "Epoch 5 Train batch 2690/3877 Loss=3.1980 Acc=38.24%\n",
      "Epoch 5 Train batch 2700/3877 Loss=3.1977 Acc=38.25%\n",
      "Epoch 5 Train batch 2710/3877 Loss=3.1972 Acc=38.25%\n",
      "Epoch 5 Train batch 2720/3877 Loss=3.1977 Acc=38.24%\n",
      "Epoch 5 Train batch 2730/3877 Loss=3.1978 Acc=38.24%\n",
      "Epoch 5 Train batch 2740/3877 Loss=3.1976 Acc=38.24%\n",
      "Epoch 5 Train batch 2750/3877 Loss=3.1977 Acc=38.24%\n",
      "Epoch 5 Train batch 2760/3877 Loss=3.1975 Acc=38.25%\n",
      "Epoch 5 Train batch 2770/3877 Loss=3.1971 Acc=38.25%\n",
      "Epoch 5 Train batch 2780/3877 Loss=3.1969 Acc=38.25%\n",
      "Epoch 5 Train batch 2790/3877 Loss=3.1963 Acc=38.26%\n",
      "Epoch 5 Train batch 2800/3877 Loss=3.1961 Acc=38.26%\n",
      "Epoch 5 Train batch 2810/3877 Loss=3.1964 Acc=38.26%\n",
      "Epoch 5 Train batch 2820/3877 Loss=3.1966 Acc=38.26%\n",
      "Epoch 5 Train batch 2830/3877 Loss=3.1964 Acc=38.26%\n",
      "Epoch 5 Train batch 2840/3877 Loss=3.1964 Acc=38.26%\n",
      "Epoch 5 Train batch 2850/3877 Loss=3.1963 Acc=38.26%\n",
      "Epoch 5 Train batch 2860/3877 Loss=3.1965 Acc=38.26%\n",
      "Epoch 5 Train batch 2870/3877 Loss=3.1967 Acc=38.26%\n",
      "Epoch 5 Train batch 2880/3877 Loss=3.1963 Acc=38.26%\n",
      "Epoch 5 Train batch 2890/3877 Loss=3.1961 Acc=38.27%\n",
      "Epoch 5 Train batch 2900/3877 Loss=3.1960 Acc=38.27%\n",
      "Epoch 5 Train batch 2910/3877 Loss=3.1959 Acc=38.27%\n",
      "Epoch 5 Train batch 2920/3877 Loss=3.1957 Acc=38.27%\n",
      "Epoch 5 Train batch 2930/3877 Loss=3.1957 Acc=38.27%\n",
      "Epoch 5 Train batch 2940/3877 Loss=3.1956 Acc=38.27%\n",
      "Epoch 5 Train batch 2950/3877 Loss=3.1953 Acc=38.27%\n",
      "Epoch 5 Train batch 2960/3877 Loss=3.1947 Acc=38.28%\n",
      "Epoch 5 Train batch 2970/3877 Loss=3.1947 Acc=38.28%\n",
      "Epoch 5 Train batch 2980/3877 Loss=3.1947 Acc=38.27%\n",
      "Epoch 5 Train batch 2990/3877 Loss=3.1946 Acc=38.27%\n",
      "Epoch 5 Train batch 3000/3877 Loss=3.1946 Acc=38.27%\n",
      "Epoch 5 Train batch 3010/3877 Loss=3.1946 Acc=38.28%\n",
      "Epoch 5 Train batch 3020/3877 Loss=3.1943 Acc=38.28%\n",
      "Epoch 5 Train batch 3030/3877 Loss=3.1942 Acc=38.28%\n",
      "Epoch 5 Train batch 3040/3877 Loss=3.1939 Acc=38.28%\n",
      "Epoch 5 Train batch 3050/3877 Loss=3.1938 Acc=38.29%\n",
      "Epoch 5 Train batch 3060/3877 Loss=3.1936 Acc=38.29%\n",
      "Epoch 5 Train batch 3070/3877 Loss=3.1936 Acc=38.29%\n",
      "Epoch 5 Train batch 3080/3877 Loss=3.1935 Acc=38.29%\n",
      "Epoch 5 Train batch 3090/3877 Loss=3.1933 Acc=38.29%\n",
      "Epoch 5 Train batch 3100/3877 Loss=3.1933 Acc=38.29%\n",
      "Epoch 5 Train batch 3110/3877 Loss=3.1933 Acc=38.29%\n",
      "Epoch 5 Train batch 3120/3877 Loss=3.1936 Acc=38.29%\n",
      "Epoch 5 Train batch 3130/3877 Loss=3.1934 Acc=38.29%\n",
      "Epoch 5 Train batch 3140/3877 Loss=3.1936 Acc=38.29%\n",
      "Epoch 5 Train batch 3150/3877 Loss=3.1937 Acc=38.29%\n",
      "Epoch 5 Train batch 3160/3877 Loss=3.1935 Acc=38.29%\n",
      "Epoch 5 Train batch 3170/3877 Loss=3.1937 Acc=38.29%\n",
      "Epoch 5 Train batch 3180/3877 Loss=3.1938 Acc=38.28%\n",
      "Epoch 5 Train batch 3190/3877 Loss=3.1938 Acc=38.28%\n",
      "Epoch 5 Train batch 3200/3877 Loss=3.1941 Acc=38.28%\n",
      "Epoch 5 Train batch 3210/3877 Loss=3.1937 Acc=38.29%\n",
      "Epoch 5 Train batch 3220/3877 Loss=3.1935 Acc=38.29%\n",
      "Epoch 5 Train batch 3230/3877 Loss=3.1933 Acc=38.29%\n",
      "Epoch 5 Train batch 3240/3877 Loss=3.1935 Acc=38.29%\n",
      "Epoch 5 Train batch 3250/3877 Loss=3.1936 Acc=38.29%\n",
      "Epoch 5 Train batch 3260/3877 Loss=3.1932 Acc=38.30%\n",
      "Epoch 5 Train batch 3270/3877 Loss=3.1932 Acc=38.30%\n",
      "Epoch 5 Train batch 3280/3877 Loss=3.1927 Acc=38.30%\n",
      "Epoch 5 Train batch 3290/3877 Loss=3.1928 Acc=38.31%\n",
      "Epoch 5 Train batch 3300/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3310/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3320/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3330/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3340/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3350/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3360/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3370/3877 Loss=3.1927 Acc=38.31%\n",
      "Epoch 5 Train batch 3380/3877 Loss=3.1925 Acc=38.31%\n",
      "Epoch 5 Train batch 3390/3877 Loss=3.1926 Acc=38.31%\n",
      "Epoch 5 Train batch 3400/3877 Loss=3.1923 Acc=38.31%\n",
      "Epoch 5 Train batch 3410/3877 Loss=3.1923 Acc=38.31%\n",
      "Epoch 5 Train batch 3420/3877 Loss=3.1923 Acc=38.32%\n",
      "Epoch 5 Train batch 3430/3877 Loss=3.1924 Acc=38.32%\n",
      "Epoch 5 Train batch 3440/3877 Loss=3.1924 Acc=38.32%\n",
      "Epoch 5 Train batch 3450/3877 Loss=3.1923 Acc=38.31%\n",
      "Epoch 5 Train batch 3460/3877 Loss=3.1923 Acc=38.31%\n",
      "Epoch 5 Train batch 3470/3877 Loss=3.1926 Acc=38.31%\n",
      "Epoch 5 Train batch 3480/3877 Loss=3.1922 Acc=38.31%\n",
      "Epoch 5 Train batch 3490/3877 Loss=3.1923 Acc=38.31%\n",
      "Epoch 5 Train batch 3500/3877 Loss=3.1921 Acc=38.32%\n",
      "Epoch 5 Train batch 3510/3877 Loss=3.1919 Acc=38.32%\n",
      "Epoch 5 Train batch 3520/3877 Loss=3.1919 Acc=38.32%\n",
      "Epoch 5 Train batch 3530/3877 Loss=3.1916 Acc=38.32%\n",
      "Epoch 5 Train batch 3540/3877 Loss=3.1916 Acc=38.32%\n",
      "Epoch 5 Train batch 3550/3877 Loss=3.1918 Acc=38.32%\n",
      "Epoch 5 Train batch 3560/3877 Loss=3.1917 Acc=38.32%\n",
      "Epoch 5 Train batch 3570/3877 Loss=3.1919 Acc=38.32%\n",
      "Epoch 5 Train batch 3580/3877 Loss=3.1919 Acc=38.32%\n",
      "Epoch 5 Train batch 3590/3877 Loss=3.1918 Acc=38.32%\n",
      "Epoch 5 Train batch 3600/3877 Loss=3.1919 Acc=38.32%\n",
      "Epoch 5 Train batch 3610/3877 Loss=3.1916 Acc=38.32%\n",
      "Epoch 5 Train batch 3620/3877 Loss=3.1915 Acc=38.32%\n",
      "Epoch 5 Train batch 3630/3877 Loss=3.1913 Acc=38.32%\n",
      "Epoch 5 Train batch 3640/3877 Loss=3.1914 Acc=38.32%\n",
      "Epoch 5 Train batch 3650/3877 Loss=3.1914 Acc=38.32%\n",
      "Epoch 5 Train batch 3660/3877 Loss=3.1916 Acc=38.32%\n",
      "Epoch 5 Train batch 3670/3877 Loss=3.1917 Acc=38.32%\n",
      "Epoch 5 Train batch 3680/3877 Loss=3.1914 Acc=38.32%\n",
      "Epoch 5 Train batch 3690/3877 Loss=3.1913 Acc=38.32%\n",
      "Epoch 5 Train batch 3700/3877 Loss=3.1914 Acc=38.32%\n",
      "Epoch 5 Train batch 3710/3877 Loss=3.1913 Acc=38.33%\n",
      "Epoch 5 Train batch 3720/3877 Loss=3.1913 Acc=38.33%\n",
      "Epoch 5 Train batch 3730/3877 Loss=3.1912 Acc=38.33%\n",
      "Epoch 5 Train batch 3740/3877 Loss=3.1911 Acc=38.33%\n",
      "Epoch 5 Train batch 3750/3877 Loss=3.1911 Acc=38.33%\n",
      "Epoch 5 Train batch 3760/3877 Loss=3.1908 Acc=38.33%\n",
      "Epoch 5 Train batch 3770/3877 Loss=3.1910 Acc=38.33%\n",
      "Epoch 5 Train batch 3780/3877 Loss=3.1908 Acc=38.33%\n",
      "Epoch 5 Train batch 3790/3877 Loss=3.1909 Acc=38.33%\n",
      "Epoch 5 Train batch 3800/3877 Loss=3.1911 Acc=38.33%\n",
      "Epoch 5 Train batch 3810/3877 Loss=3.1907 Acc=38.33%\n",
      "Epoch 5 Train batch 3820/3877 Loss=3.1908 Acc=38.33%\n",
      "Epoch 5 Train batch 3830/3877 Loss=3.1906 Acc=38.33%\n",
      "Epoch 5 Train batch 3840/3877 Loss=3.1906 Acc=38.33%\n",
      "Epoch 5 Train batch 3850/3877 Loss=3.1903 Acc=38.34%\n",
      "Epoch 5 Train batch 3860/3877 Loss=3.1903 Acc=38.34%\n",
      "Epoch 5 Train batch 3870/3877 Loss=3.1904 Acc=38.34%\n",
      "Epoch 5 Train batch 3877/3877 Loss=3.1902 Acc=38.34%\n",
      "Epoch 5/5 train_loss=3.1902 train_acc=38.34% val_loss=3.1183 val_acc=39.72%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq4AAAHWCAYAAAC2Zgs3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAe95JREFUeJzt3Qd4U9UbBvA33bvQltKyStl7T5UlS0CWKCKooP6doLgHggwHOHHjxokDFQRFkCFDAdl771UopdC9k//znTQlaZPSlrQ34/09T21ymyYnpxf79tzvnKMzGAwGEBERERE5OA+tG0BEREREVBIMrkRERETkFBhciYiIiMgpMLgSERERkVNgcCUiIiIip8DgSkREREROgcGViIiIiJwCgysREREROQUGVyIiIiJyCgyuRIQxY8agdu3aZfreKVOmQKfTwZUdO3ZMvccvv/yywl9bXlf62ETaIMekTVciP1P52TrKuUJEdLUYXIkcmASUknysXLlS66a6vUceeUT9LA4dOmTzMc8//7x6zI4dO+DIzpw5o8Lytm3b4Ij27t2r+tHPzw+XLl3SujlEVIEYXIkc2DfffGPx0bt3b6vHGzdufFWv8+mnn2L//v1l+t6JEyciIyMD7m7UqFHq85w5c2w+5vvvv0fz5s3RokWLMr/OHXfcofo7JiYG5Rlcp06dajW4Xs25Yi/ffvstoqKi1O2ff/5Z07YQUcXyquDXI6JSuP322y3ur1+/HkuXLi1yvLD09HQEBASU+HW8vb3L3EYvLy/14e46duyIevXqqXD6wgsvFPn6unXrcPToUcyYMeOqXsfT01N9aOVqzhV7MBgM6o+DkSNHqv787rvv8L///Q+OKC0tDYGBgVo3g8ilcMSVyMl1794dzZo1w+bNm9G1a1cVWCdMmKC+9ttvv2HAgAGoVq0afH19UbduXbz44ovIy8srtm7RVNP5xhtv4JNPPlHfJ9/fvn17bNy48Yo1rnJ/3LhxmD9/vmqbfG/Tpk2xePHiIu2XMod27dqpy77yOh9//HGJ62bXrFmDW265BbVq1VKvUbNmTTz22GNFRoDl/QUFBeH06dMYMmSIul2lShU8+eSTRfpCLj3L40NDQ1GpUiWMHj26xJejZdR137592LJlS5GvSdiS93TbbbchOztbhdu2bduq15Fw06VLF/z9999XfA1rNa4S5l566SXUqFFD/fx79OiB3bt3F/nexMRE9Z5l1Ff6ICQkBP369cP27dstfh7ycxZ33XVXQTmKqb7XWo2rBLQnnnhC9b/8HBo2bKjOHWlXWc8LW/7991/13keMGKE+Vq9ejVOnThV5nF6vxzvvvKPeq5xb8vO+4YYbsGnTpiKjtx06dFD9VrlyZfVv6K+//rJZY2yrftj0c1m1ahUeeughREZGqp+HOH78uDom/eLv74/w8HB13lqrU5ZzTc5heX7pH3mOO++8EwkJCUhNTVXnyvjx44t8n/SB/EEzffr0EvclkTPiMAmRC7hw4YIKIPKLXEZjq1atWvDLVALK448/rj6vWLFCBabk5GS8/vrrV3xeCVspKSm4//771S/l1157DTfddBOOHDlyxZG3f/75B7/++qv6hR0cHIx3330Xw4YNw4kTJ9QvbrF161YVJqKjo9WlaQmR06ZNUyGjJObOnatGlx988EH1nBs2bMB7772nfonL18zJc/ft21eNjEqoWrZsGd58800VluX7hQStwYMHq7Y/8MADqgRj3rx5KryWNLjK+5B+a9OmjcVr//TTTyqcSsiWEPLZZ5+pEHvvvfeqPv78889V++Q9tGrVCqUhP1MJrv3791cfEpz79OmjArI5+blJaJTQFBsbi3Pnzqk/FLp164Y9e/aoP3DkPcvPQJ7zvvvuU20W11xzjdXXlj4bNGiQCt333HOPavuSJUvw1FNPqT8UZs6cWerzojgywio/MwnXEn4lcMoot7yeOWmLnP/y70JGZHNzc9UfOnLVQv5QEvKzklAq703es4+PD/777z/170T6ryzkfcn5K/0ngV7IH3tr165V/z4liEpgnTVrlvqjU/rddHVEgqn0t9Tw3n333eocknNlwYIF6pyWvh06dCh+/PFHvPXWWxYj79IH8rMwlawQuSwDETmNsWPHyhCWxbFu3bqpYx999FGRx6enpxc5dv/99xsCAgIMmZmZBcdGjx5tiImJKbh/9OhR9Zzh4eGGxMTEguO//fabOr5w4cKCY5MnTy7SJrnv4+NjOHToUMGx7du3q+PvvfdewbGBAweqtpw+fbrg2MGDBw1eXl5FntMaa+9v+vTpBp1OZzh+/LjF+5PnmzZtmsVjW7dubWjbtm3B/fnz56vHvfbaawXHcnNzDV26dFHHZ8+efcU2tW/f3lCjRg1DXl5ewbHFixer7//4448LnjMrK8vi+y5evGioWrWq4e6777Y4Lt8nfWwibZBj8jMS8fHxqq8HDBhg0Ov1BY+bMGGCepy8dxP5mZu3S8jz+Pr6WvTNxo0bbb7fwueKqc9eeukli8fdfPPN6udgfg6U9LywJTs7W52Tzz//fMGxkSNHGlq2bGnxuBUrVqjnfOSRR4o8h6mP5Dzz8PAwDB06tEifmPdj4f43kT4w71vTz+W6665TP98rnafr1q1Tj//6668Ljr3wwgvq2K+//mqz3UuWLFGP+fPPPy2+3qJFC/X/AiJXx1IBIhcglxTlsm5hclnSREb1ZPRGRnRklFIuaV/Jrbfeqi6fmphG32Tk7kp69eqlRsZMZEKSXJo2fa+MQsqop1y6l5E+E6kTlVGykjB/fzK6Je9PRs8kb8hobmEyimpO3o/5e1m0aJGq1zWNwAoZ1Xr44YdRUjLiLaNjcgnbREZgZTRPRjpNzyn3TZe05RK+jAjKSKC1MoPiSB/KyKq00by84tFHH7V6nnh4eBT0v4zUy0i8XMIu7eua95m8H1lVwZyUDsjP4c8//yzVeVEceS5ps4xUm8htKXUwL4345ZdfVF9Mnjy5yHOY+khGnqXvZWTU1CeFH1MWMoJeuAbZ/DzNyclR70HOcylFMe93aXfLli3VqKqtdkv/yb8XGXk22bVrl1qp4kq170SugMGVyAVUr169IAiZk1/m8ktQ6iglHMglTNMvt6SkpCs+r1zWNmcKsRcvXiz195q+3/S98fHxqhZVfoEXZu2YNXJ5WeoMw8LCCupW5bK3tfdnqnO01R5TLaKULchzmZNgV1JyOViCi2l1gczMTFVuIGHc/I+Ar776SoU2aZdcIpe2/fHHHyX6uZiTNov69etbHJfnM389IUFNLt3LYyXERkREqMdJ6Cnt65q/vgQpuexvzrTShal9JT0viiP1qFLiIG2XZcfkQ0KwXGo3D3KHDx9WbZLzwhZ5jATWJk2awJ6kfYXJeS4B2VQDbOp3qWc173dpk5Q/FEfaLOUAErzlD1Ah713OI9MfRkSujMGVyAWYj+iYyC9FCXEyGiX1ewsXLlQrErz66qsFIeZKbM1eLzzpxt7fWxIyYijLg0nYe+aZZ9Qvcnl/pklEhd9fRc3El0k50i4ZPZPRNel3Ge02rz2UACaBW0KX1LbK5CRp+/XXX1+in0tZvfLKK6reWSYgSRukFlVeVyZIlefr2uO8kLps6UtZSUCCt+lDgqcEOPlDwV7nVkkUntRX3L9FGQ1/+eWXMXz4cFXrLJO/pN/lD5ay9LtM1pJ6WDnnTass3HjjjeoPVCJXx8lZRC5KZofLJUmZCCNBxUR+8TsCCXgySmRtwf7iFvE32blzJw4cOKBGLuUXuYkEgrKStVGXL1+uQoH5qGtp1y2VkCphVC5tS6iQ0e6BAwcWfF3WHq1Tp4762ZhflrZ2abskbRYHDx5Uz2ly/vz5IqOY8rqy4oCE5cJ/5MgoYFkulcvrS7mChHPzUVdTKYq91puVvpLRa5nUZN5W089H1hOWFQeuu+469QeBhHIpwbA16iqPkdAok6OKmwwno8GFV5WQ0oy4uLgSt136XSb4yWRAE3kvhZ9X2iSX/a9ERmVbt26tRlplspdceZBJiUTugCOuRC7KNLJlPgolv3A//PBDOEr7pF5PRo1kwXvz0Fq4LtLW9xd+f3JblkAqK5mRL7WmEo7MR9ZKGwqkblcuX0tfy3uRlRgkpBfXdpnNLmu9lpb0oazwIG00f7633367yGPldQuPSsrqCzL735xp7dGSLAMmfSZ99P7771scl5IECcAlrVe+EhkhlmAudco333yzxYcs8SV/aJjKBWSVAnmfsmpAYab3Lz8juewuVyMKj3qa95GESfN6ZSFLxNkacbXGWr/Lz6vwc0i75QqJlJbYarf5RhQycis/Zxm5tVc/Ezk6jrgSuSiZpCSjRTLSY9qOVHbZqsjLqVciSxHJL99rr71WTYgyBSAZUbrSdqONGjVSoUJCiwQvGdWUy/MlqZW0RUZFpS3PPvusWrJILkPLSF9p6z8lREkwMtW5Fl6iSC7ryvNK/bGssyuj4B999JF6PRntLQ3TerSyfqc8rwRJmZgmgbnwyKR8XYKaTOST80NGrSXsmY/UCulXmTgkbZJRVAmysoyYtfpN6TMZxZXtbKXPZHKR/ExlDWGZIGY+Eaus5A8bWW6r8AQwE6kblaXEJITL8lrSHgl2cltGomXJNQmnshyWfE3WkpU6ammzrGssk/Tkjwt5Hlm6SupjTeuhylJaEpYlVEoJiARLGc0t3LfFkX6Xf3tyKV9+xvIHioxSF17+S5b0ktFZqVWV5bBknV8ZNZblsORnIX1rIhswPP300yrkyr8drTeGIKooHHElclHyS/H3339Xk43kMqqsXSq/eGUtVkchv5glYEnAnjRpkrqELcGqZ8+eFiOU1sgvaql5lMu8EjJkdE1qHr/++usyt0dG4CQkSNCUET4JNjLxTcoRSssUVqX/pXbVnNS3Sr2phCAJYxKE5PVM64uWlqzhKu9fAquEH5nkI+Gx8K5NsjGFzPaX15NF7GVGu9QIy6Shwn0r71lGCiW0ycx9WVi/uD6TkCrnm3yWy++yTrCsNWoPP/zwgwqe5uUWhcnXpDTGNFo/e/Zs1Qb5o0D6RPpbJkmZr0cr59oXX3yhjsvPWiZQyWQyOf/MVwmQGmoZdZW+k+eTcpTS7IglVwGknEX+SJDnkDIDCa6FJwHKfQnXEkRltQY5N2TUXiYHmjYzMJG1mk1rzUpIJ3IXOlkTS+tGEBGZk9FKWRFBRsuIyDoZsZdR85LUhBO5Co64EpGmCm/PKmFVRptkVyEisk5GbWW0nKOt5G444kpEmpJL6XLpXOos5TKtTIzKyspSl70Lr01K5O6kVEFWT5Atg6UeV8pCoqKitG4WUYXh5Cwi0pRMnJF91s+ePasmx3Tu3FnVIzK0EhUltcYyuU42cpA6ZIZWcjcccSUiIiIip8AaVyIiIiJyCgyuREREROQUXL7GVdb+k8WrZRHt0mxjSEREREQVQypXZeto2QBE1od22+AqobXw4tpERERE5HhOnjxZZMMNtwquMtJq6gjZErK85eTkqB1rZEcTbsF3GfvFNvaNdewX29g31rFfbGPfWMd+cZy+SU5OVgONptzmtsHVVB4gobWigmtAQIB6Lf4juIz9Yhv7xjr2i23sG+vYL7axb6xjvzhe31yprJOTs4iIiIjIKTC4EhEREZFTYHAlIiIiIqfg8jWuJV2CITc3F3l5eXapCfHy8kJmZqZdns9VuFO/eHp6qvfK5deIiIjsy+2Da3Z2NuLi4pCenm63ECx7R8sqBgwu7tsvUtAeHR0NHx8frZtCRETkMtw6uMrmBEePHlUjZLLgrYSMqw1V8pypqakICgoqdgFdd+Mu/SIBXf4YOn/+vDq36tev79Lvl4iIyC2D64wZM/Dcc89h/PjxePvtt9Uxuaz8xBNP4IcffkBWVhb69u2LDz/8EFWrVrXLa0rAkEAl64bJCJk9yPPJ8/r5+TGwuGm/+Pv7q6VDjh8/XvCeiYiI6Oo5RILYuHEjPv74Y7Ro0cLi+GOPPYaFCxdi7ty5WLVqldoF66abbrL767t6kKKKx3OKiIjI/jT/7SqXj0eNGoVPP/0UlStXLjielJSEzz//HG+99Rauv/56tG3bFrNnz8batWuxfv16TdtMRERERG5YKjB27FgMGDAAvXr1wksvvVRwfPPmzWomuhw3adSoEWrVqoV169ahU6dOVp9PSgrkw3wLMSHPJR/m5L7UJMplbPmwB3k+02d7PacrcLd+kfco71XOMamhLo7pvCx8fro79ott7Bvr2C+2sW+sY7/YoM9D3tF/UD1xHfIOBwKx1wEexf8uu1ol/RloGlyldnXLli2qVKCws2fPqslSlSpVsjgu9a3yNVumT5+OqVOnFjku++0WrmOVJYtkpruM+kotYlnl6Q3YcjIZCWnZiAj0QZuaIUhJSYEzkTKNBx98UH2UJ2frl7KS8ykjIwOrV69WS62VxNKlS8u9Xc6I/WIb+8Y69ott7Bvr2C+XRV/aiOanvoN/TiLayYHjs5DhHYadNUYhrlJ7lJeSru6kWXCVZZFkIpacLPacvCITvB5//HGLEVeZfNWnTx+13645mfwl7ZCZ7mVtw+JdZzHt9704m5xZcKxqsA8mD2yCG5pFw96uNHr3wgsvYPLkyaV+XvnjITAw0C6T1L7//nvceeeduP/++/H++++rYzL6KKE1ODjYLZbDknNLJml17dr1iueW/JUp/w569+7NvbLNsF9sY99Yx36xjX1jHfvFkm7f7/D8RX5vG6+SmvjlXET7o+8jb9hsGBrdiPJgukLusMFVSgHi4+PRpk2bgmOyML2MUEnYWbJkiRq1unTpksWo67lz59QoqS2+vr7qozA5IQuflPJ6EqJkIk1ZJtMs3hWHsXO2FvrxAvEp2Rg7Zxtm3e5h9/Aqa86a/Pjjjyqo7t+/v+CY+XJTEhblPcrI8pXYa6UGIbXITz/9tJpwJzXKEtxM5QGm/q4ocg5psZaqvEd5r9bOO1tK81h3wn6xjX1jHfvFNvaNdewXqPIALJ1QJLQKnTqmg9fS54Gmg8qlbKCk/a/Z5KyePXti586d2LZtW8FHu3bt1EQt0215E8uXLy/4HgloJ06cQOfOnculTRL00rNzS/SRkpmDyQt2W/nxXv6RT1mwRz2uJM9nqgG9Egntpo/Q0FAVjkz39+3bp0Y0//zzTzWZTQL8P//8g8OHD2Pw4MEqnEqwbd++PZYtW2bxvLVr1y5YhkzI83722WcYOnSoGoWV9UgXLFhwxfbJ2qUyge7ZZ59FgwYN8OuvvxZ5zBdffIGmTZuq9ski/ePGjSv4mvyhIiO10lYJvM2aNcPvv/9u7M8pU9CqVSuL55I2S9tNxowZgyFDhuDll19Wa/M2bNhQHf/mm2/UOSX9I301cuRI9YeTud27d+PGG29UI/PyuC5duqi+kz+m5FwsXKLy6KOPqscQERE5paxU4PwB4PDfwPJpQPKZYh5sAJJPA8fXQkuajbhKMJBQYk4uVYeHhxccv+eee9Rl/7CwMBUmHn74YRVabU3MuloZOXlo8sISuzyXxFApH2g+5a8SPX7PtL4I8LHPj0NC4xtvvIE6deqolRqkHKJ///4qzElY/PrrrzFw4ED1h4BMdrNFaoVfe+01vP7663jvvffUHxWyNqn8PIobbZXJdhKqb7/9drUyhIREk1mzZuHJJ59U6/b269dPrR7x77//qq/JqKwck5KCb7/9FnXr1sWePXuuWB5RmPyxI+eLec2SXA568cUXVZCVwCrnlYTcRYsWqa+fPn1aXdbv3r07VqxYob5f2iX1qXJc+lLC71NPPVXwfN99953qHyIiIodiMABZycYgKmFTfT4DJJ26fFs+spJK/9yp5+DWqwoUZ+bMmeqS67Bhwyw2IKDiTZs2TdXrmEjQbNmyZcF9CXDz5s1TI6jmo52FSbC77bbb1O1XXnkF7777LjZs2IAbbrjB6uMleH755Zcq5IoRI0aoDSRkFDYmJqbgeeSY1DebyAiwkFFgef69e/eq0VohgbG05A8gGS02LxG4++67C27Lc8p7kdc17eb1wQcfqLAtEwZNlytMbTD9ESWh3BRcZX1hqWMdPnx4qdtHRER0VaE046JZADULpslmwTQ7tWTP5xsKhFQDvP2BM1uu/Pgg+5UWOn1wXblypcV9uVQsgUI+KoK/t6ca+SyJDUcTMWZ20dUQCvvyrvboEBtWote2F7kkbk7CmVxm/+OPP1SNrIwiyox3KbsojvmGEBIGZRSy8OV1czLCmZaWpkZ3RUREhArQUhogo7eyDapsIiFlItZIiUiNGjUsAmNZNG/evEhdq9RUSx9s374dFy9eLKi5lT5o0qSJem257G+rxkZC/MSJE9UawjLiLwFdQqv0CxERkd1CafoFszBqHkrNbueUbAY+/CoBoTWMwVR9VC962zf4co3r282AZJlLY618UWd8fMw10JJDBVetSV1nSS/Xd6lfBdGhfjiblGnrx4uoUD/1OE+Pip1FXzhMyaV5CZVSPlCvXj012/3mm2++4hJghUOc9E9xa7BKWUBiYqJ6fhN5/I4dO9RKB1eaXW/+fdbI6HvhWmBr674Vfv8SpmW0Xj7k8n6VKlVUYJX7pj640mtHRkaq8goZdY2NjVV1xIX/0CIiIrJJfn+mJxS6XG8lmOZdXou+WAHhZgHUFEjNP0cDPqUYXJEJVze8Cvx0Z36KMf99m59jbphR7uu5XgmDaxlJGJUlrx78doutH6/6ekWHVmukVlNGDGWilWkE9tixY3Z9jQsXLuC3335Tl9pl4pWJrGpw3XXXqXV0r7nmGjWRSmpQe/ToYXWE99SpUzhw4IDVUVcJnDJBSsKraUktGSm9Epm0Ju2TulpZGk1s2rSpyGt/9dVXKgjbGnX93//+p0onZFRY6m+vvfbaEvQMERG5PBmtTI0vdLm+cCiNA/Ql3OggMNIyiIaah9NqQLBc2rffUqIFmgwChn8NLH7GcqKWvKaEVvm6xhhcr4IsdTXr9jaYunAP4pIur+MaqdZxbVou67iWhawIILP7ZcRQAt+kSZPsvnuVTFySiXVy+bzwOq1SOiDlAhJcZfmuhx56SI1gmiZiSbCWiXfdunVTE6GkplmW0ZLRYQmd8nxSVysTp6TcQCZEyYjx4sWL1chn4fV5C5MJaFI6ILW3DzzwAHbt2qXqfM1Jra98XepyZS1gqXeVsoAOHToUrEwgI7TyWrLDm9QRExGRG8jLBVLPXg6gSVYu46fEAYa8EjyZDgiOsnLp3iyYBkcDXhW/jGMBCaeNBiD3yGpsW7MErbr0hVedrpqPtJowuF4lCae9m0Spmtf4lExUCfJBwzAvVK4UCkchIVAmJ0lwlLrTZ555psQL/ZaUBFMZ0bW2uYAE0TvuuEONeo4ePVpdnpeJd1LCIO2REGryyy+/qOMysimX+CW8ykipaNy4sZqcJxO8JHjK88pjP/nkk2LbJiO1UpM6YcIENSlL1g6WsolBgy7/5SihW1YTkMlXEqBlJQNZest8VFVKFWTkWl5fNlggIiInl5cN/6zz0J1cD6Sdsz7JSWbRG0ow2KPzNIZOW/WkMmoqE5s8nWC9WA9PGGKuw+ndyWgZU/7bvZaGzlDSBUSdlAQ0GT2TZZes7ZwlM96lZtFeu3fJSKa8prxWRS607+hcpV9kdQEZ9b3SmralObekPEGW5ZKRabdfANsM+8U29o117Bfb3LJvcjKBFLOln6wtC5Vme8KxBQ8v4+V5dcne2mhpNWModaCA52znTHF5zRxHXIlKQP4hyYYZc+bMKdFGDEREVI6y0wuF0cL1pKeNs/NLIE/nBY/QGtBZ1JEWCqiBVeSyW7m/LboyBleiEpCdx2SNWamRNV8jl4iIymE3J1P4tDX7XtYxLQkv/6IjpGYBNcc/EotW/of+Awa4z0i0k2NwJSoBLn1FRGTH3ZzUBCdro6Wl2M3JO9Ds0r35CKnZuqX+lWUtR9vPIUsqFvd1cjgMrkRERK5Mnwfd8X9QPXEddMdDgPKYIV5kNycba5WWeDenEOuL5ZuPmspjGDrdDoMrERGRq9qzQK3J6ZV8BmpPxeOz8tfkfLXka3Ja283J2pJQuRkl383JNMveWj2pzMz3K36ZQ3JfDK5ERESuGlrVLkiFFg+ShfDluCw03+hGIO28jXpS0+X8OO12cyIqhMGViIjIFXdykt2PrG5Knn9s7hjjgviGXMfezYnIDIMrERGRs5LL+DJimngUSDwCXJTPR4G47ZZbdlr93jzn2c2JKB+DKxERkaNvOZp08nIoLfh8zPg5J63sz93vNaDd3c6xmxNVmDy9Af8dTcTmBB3Cjyaic71IeHo4xkQ4Bld7XZI5vta4LZxcSqnUFI6ue/fuakvTt99+W+umEBFRdtrlIFo4oEpo1Rd3OV8HhNYAKtcGwmKByrFAbiaw6tUrv25kE4ZWsrB4VxymLtyDuKRMAJ74+uAmRIf6YfLAJmqbe60xuNppxqbpkozsqxESFA30exVoOtjuLzdw4EC1DdvixYuLfG3NmjXo2rUrtm/fjhYtWtjl9TIyMlC9enW1Tevp06fh6+trl+clInIramZ+YqFQeuTybRn4KI6nr2UwNf9cqRbg5Vt0QGXrN8aJVVbrXHXGEoCYa+z6Nsn5Q+uD324pcsacTcpUx2fd3kbz8MrgWg4zNnWpZ4G5owHd1yVfbqSE7rnnHgwbNgynTp1CjRo1LL42e/ZstGvXzm6hVfzyyy9o2rQpDAYD5s+fj1tvvRVakTbk5eXBy4unLRE5IAmLMgvf2qipjKbK4vvF8QstGkpNn2XiU2m2HJV1WmXJK/U7Slfo91T+Jd8bZth/PVdy6vKAqQv32JzOJ2eNfL13kyhNywa48W7hv4jlck1JPjKTgT+ftvqXrM50TEZi5XEleT557RK48cYbUaVKFXz55ZcWx1NTUzF37lwVbC9cuIDbbrtNjZQGBASgefPm+P7778vUJZ9//jluv/129SG3C9u9e7dqU0hICIKDg9GlSxccPny44OtffPGFCr7+/v5o1KgRHn74YXX82LFj0Ol02LZtW8FjL126pI6ZdqmSz3L/zz//RNu2bdVo7z///KOeX7ZgrVq1KoKCgtC+fXssW7bMol1ZWVl45plnULNmTfV99erVU+2X8Cu333jjDYvHSzvktQ4dOlSmfiIiN5GTCcTvA/b/Caz7EFj0FPDtzcB7bYGXo4C3mwNfDwIWjgf+fRvY8xtwdsfl0CoBNOZaoNXtwPUTgWGfA/euAJ4+Cjx7Arh/FXDLl0CvyUCbO4HYLsYygNKEVhMZOJElr2QJKnMy0jrc/gMr5Nz+OXQ+vzzAOkkp8vUNRxOhJQ5dmctJB16pZpenUuFVygdm1CzZN0w4U6K17WS08c4771TB9fnnn1dhS0holdFICawSYiXoSXCTQPnHH3/gjjvuQN26ddGhQ4cSvwcJiOvWrcOvv/6qAt9jjz2G48ePIyYmRn1dSgekNEHqZVesWKFe699//0VurrEWa9asWXj88ccxY8YM9O3bF2fOnFFlDKX17LPPqqBZp04dVK5cGSdPnkT//v3x8ssvq1D69ddfqxKK/fv3o1atWup7pI+k7e+++y5atmyJo0ePIiEhQfXX3XffrUann3zyyYLXkPvyXiTUEpGbkx2gioyaHjN+VmVhxQw0eHgbL92H1Sk6clo5BvD2r8h3YgynjQYg98hqbFuzBK269IVXeeycRU7BYDAgPiULh+NTcTghDUfOp+LweePnUxdLtoFEfIrtcFsRGFydkASv119/HatWrVKh0RS8pIQgNDRUfZiHMhnlXLJkCX766adSBVcZLe3Xr58Ki0LCp7zOlClT1P0PPvhAvdYPP/wAb29jcX+DBg0Kvv+ll17CE088gfHjx0Ov1yMqKqqgvaUxbdo09O7du+B+WFiYCqMmL774IubNm4cFCxZg3LhxOHDggHqvS5cuRa9evdRjJPSajBkzBi+88AI2bNig+kNqhufMmVNkFJaIXJReD6TEWbmcn/8581Lx3+8TDITVtn5ZX42OOlgo9PCEIeY6nN6djJYx1zle+8juMnPycOxCGg7Hm8JpKo6ooJqG1KwSrttrQ2Swtmv1Mria8w4wjnyWhKwi8N3NV37cqJ9LVvwur11Ccsn9mmuuUcFSgqBc3paJWRLwhIy8vvLKKyq8yahodna2unQuZQMlJc/x1Vdf4Z133ik4JuUCEogl9MlkLbm8LqUBptBqLj4+Xo2w9uzZE1dL6nbNyYiyhGcZSY6Li1MjvDKJ7MSJE+rr0i5PT09069bN6vNVq1YNAwYMUP0nwXXhwoWqf2655ZarbisROYjcbODSCeuToS4dN866L05QVdv1prI7VP7VLiItR0/Py+jp+TRjMDV9TjCOntqqQJTy1FphAahbJQh1qgTmfw5C7fAADPrgX5xLyrQ1nQ9RoX7oEBsGLTG4mpP/EZV0K7q61xvrhGzM2DRAB518XR5XDn/dSi2rjKTKqKeMgkoZgCmoyWisBE5Z6krqWwMDA/Hoo4+qAFtSMkIrobfwZCwJtMuXL1cjoFK3aktxXxMSfE3/8Exk5NMaab85Cc8ymiojpHJpX17r5ptvLnh/V3pt8b///U+VT8ycOVP1n7zP0gR7InIAMofALJh6JhzGNYc2w+v9542TpAx629+r8wQq1TRe0i8SUGtzW1JyqNHT4xfS88NpasGlfQmqKcWMnob4eaFuZBDqRAShbmSg8XOVQNQKD4Cvl/VcMmVgE7V6gI3pfGpJLK3Xc2VwLatiZmxKaC3vGZvDhw9Xl+DlErfUeD744IMF9a5SZyqTl2SEVMhlerl83qRJkxI/v0xkGjFihKqjNSd1pfI1Ca6yeoGMykrgLDzqKhO1ateurUJujx49ijy/TDATMmLaunVrddt8olZx5P3J5f6hQ4cWjMDKZC8TCevynqWUwlQqUJjUyEogljpcWVps9erVJXptIqpA8oetLBNldZb+USD9gsXD5c9h4/9ZzK5kmYdR83AaWpPrl5JjjZ6mZl0eNTX7fPJierGjpzVNo6cRgflB1fg5PNCnIBeUlCx1JUteXV7H1UhGWrmOqyswzdg0W8dVGIKi1DquunKcsSmz6WWU8LnnnkNycrIKcib169fHzz//jLVr16r61Lfeegvnzp0rcXA9f/68unwuNaPNmjWz+JpMepLAmJiYqOpJ33vvPRVwpR1S77p+/Xp1+b1hw4bqcv4DDzyAyMhIVR979uxZNTnrkUceUaOinTp1UhO3YmNjVWnBxIkTS9Q+eX8yYUwmZMk/ykmTJqmgaiKBefTo0aoW2DQ5SyaVyWtI4BdSSiB9Ju2W5+vcuXMJe56I7Covx7jAvrUdoeTzlXaFCogoCKR5obWw/UQyWnQfDK8q9YGgSF7SJ4eSlZs/ehpvrDk1nySVkml79DRYRk/NLu3XrRKoLu/HFDN6WlYSTmXJq3WH4vHXmv/Qp0tH7pzlUvJnbJp2ztIHRiK5UlOEVDJOaCpPUi4go58yeih1myYSAI8cOaLColz+vu+++zBkyBAkJSWV6HllBFdGI63Vp8oxCZ3ffvutCqCymsBTTz2lyhQkDMpuXNdee616rITHzMxMdTleLu+Hh4erS/omUmMq70FWQJCg+9prr6FPnz5XbJ8EcQmlUucbERGhVk+Q8G5ORlInTJiAhx56SC0PJqsNyP3C/Se1wHfddVeJ+oWIykiW/LM1anrpJGDIs/29Og8gpIYxnBaZpV8b8AspeKg+JwcnFy1C85odASu190QVNXqakJptcVnfNDnqZGI69MWMntaoHFAQSs2DakRQ6UdPr4aE1I6xYbiw16A+O0poFQyu9iDlALLWnpCRv0IhqrzIKKF5jaj5rHvZLKA4prVSrZGVAOTDGh8fH1y8eLHgvpQLSD2sLffff7/6kBFRCZeyZJZJ48aN1aiwOfP3IxPPrL0/GVGVwGxu7NixFvf9/PxUwJUPW6SGV0ocZBSZiK52V6gLVsJp/mSotPjiv9/LzxhCrU2GUrtC+VTUOyEq1ejpifzaU2NANV3eT0VycaOnvl5mk6IuT46S0VM/b674cCUMruR2ZAUBKYeQUgZZSUA2MiCiEuwKlXTKxhJSx4DslOK/37+y7Vn6Ul5VlgX2icqZDJ5cSMu2CKWmUdQTxYyeyuBojcr++bWnZpOjIgNRJci3QkdPXQ2DK7kd2UVMygSkrEHKIoicij4PuuP/oHriOuiOhwD2XEw+JwO4eNw4Ulo4oMrSUnrrK38Y6YwrrahZ+oUmQsln/0r2aSNROcjO1eP4xZSiS0udT0NShu3zPsh89NQ0OapKIGqHB3L0tJwwuJLbkUlZ5pPZiJzGngVqMqhX8hmo1Y2PzzKGRVnhpKSTQdMTre8IJfdTrrCOtacPUCnG+qipHPfWdmFyoiuNnibK6Gn+pCj5fPBcMnYd98Tj/y1Hno3hUxkcrV4pf/S00CX+yGCOnlY0BlciImcJrWr5vUK/XGUtaTlu2nte7Qp1xvZkqMwrTNL0DbW9K5SEZO66RA4uJ0+vZu5bmxx1Kd3a6KlxSctAH8/8SVGWk6NiIzh66kgYXAtNCCKyB55TZPf6Ull2z+p+NvnHfr0XWD7NeEk/L6v455OaUjVL38ri+1KLyhEkcgJq9NTssr557WluMaOn1UL9C9Y7rR3mh/NHdmPEgB6oERbE0VMn4NbB1bRofnp6eol2WyIqKTmnhLXtcIlKbf9ii7WirZItTC8cNN728DLOxrc2aqp2heIuceQ8o6cSRK1NjrpodfTUKECNngYWmRwlo6f+PpdHT2UDnUUXdiEqxI+h1Um4dXCVdUcrVaqkFqYXsubp1Z64suyTbD0q65eatjUl9+kXGWmV0CrnlJxbco4RlYqM1svC+yfWAyfWGT8n7C/Z9173ONB2tHHdU0+3/t87OZmLqvY0tcjkKFluytboqZDa06JLSwUyiLowt/8/W1RUlPpsCq/2CC4ZGRlqBJf/aNy3XyS0ms4tomLl5QLndlkG1dSzZXuuutcbR1SJHFBukdHT/M8Jaeqyvy3+3majp2afZfQ0wMftY4zbcfufuISo6OhotS2pXDK4WvIcsu99165deZnYTftF3h9HWqnYXaRObbocVE9tBLJTLR/j4Q1UbwPU6gTU7ARUbwd82s04EctqnWv+UlQx11TUuyCy6VJ6ttmkqMuToyS05uTZHj2tFupndXKUjJ56ONDOTaQttw+uJhI07BE25Dlyc3PVzk2uHtBKg/1Cbis1Pj+k5gfVuO1FtziVmfy1OhqDaq3OQLXWgHehuntZ8kqtKmCcAX1Z/i/0G2Zwxj9ZJcs8/Xc0EZsTdAg/mmiXfedl9PTkxQyLyVGmz7Jgf3GjpzJSapocZfosAZWjp1QSPEuIiOxZn3rh8OVL/vI58XDRx0kNakzny0G1SuMr7xwlS13JkleyuoD5RC21juuMkq/jSm5l8a44TF24B3FJmTKEgK8PbkJ0qB8mD2yCG5pFX/H7k9JzcDjBPJwaR1GPX0grdvRUXqPowvxBiOboKV0lBlciorLKywHiduQH1fywmp5Q6EE6oGrTyyG1ZkegUs2yvZ6E00YDkHtkNbatWYJWXfrCy547Z5HLhdYHv91SpLjkbFKmOj7r9jYqvMqI7KmL6RajpqZL/AmptkdP/bw9EBthfmnfGFRlRDXQl/GCygfPLCKikspMNtakFtSnbgJyMywf4+kL1Gh3OajWaG/f7U49PGGIuQ6ndyejZcx1DK1klYRRGWktZuVfPPrDNtSovB8nEjOQnae3+VxSY2ptcpSsh8rRU6poDK5ERLbIJXnz+lSZ/W8o9AteFuyXgGoKqtEtAS9frVpMpGw4eiG/PMC2zFw9Dp1PU7d9vWT01HhJv27E5clRsVUCEcTRU3IgPBuJiIRslZpwwLI+9dLxoo+T5abMg2p4/SvXpxKVM73egAPxKfjvSCLWH7mANQfPl+j7HuhWF6M61lLroXL0lJwBgysRuafcLODMtstB9eR6IOOi5WN0HkBU88tBVZamCrnyhBaiigiq+86mqJD639EL2HA0sdidpGzp1qAKaoZxJzVyHgyuROQeJJSe3Hg5qJ7eDORlWT7GOyC/PjU/qEp9qm+wVi0msqhZ3RuXnB9UE1VQTcrIKbLUVLvaldExNgzta4dh/A/bcC4509bKv4gK9UOH2LAKew9E9sDgSkSu6dJJy92o4vcUXbw/sMrlS/7yOaoF4Ml1hkl7sk7qnrjkgkv/G44lIiUz1+IxgT6eaFs7DJ3qhKFjbDha1AiFt+flspUpg5qo1QNsrPyrlsS62vVciSoagysROT99HhC/16w+dT2QfKro48LrmQXVzkBYHdk+T4sWExUJqrvO5I+oHrmATccuIiXLMqjKJKn2MqJaJ1yNqjarbhlUC5OlrmTJq8vruBpFlWIdVyJHw+BKRM4nJwM4vcWsPnUDkJVk+RgPL+MM/4L61I5AUKRWLSaykJOnx45TSao+VUZVNx1LRFq25Y5qwX5e6FA7DB3ryKhqOJpEh8CrmKBqjYTT3k2isO5QPP5a8x/6dOlol52ziLTC4EpEji/tgnHylGk09cxWQF9oIopPEFCzw+WgWr0t4BOoVYuJLGTnSlC9pOpTZVR18/GLSC8UVEMkqMaGq0v/ElQbR4fYJWDKc8gI7YW9BvWZoZWcGYMrETnetqkXj0F39B+0PDEXXh+9CFw4WPRxQVH526bmB9XIpoAn/5dGjiErNw/bTyapy/7rjxqDamaO5RrAlQK8VZCU+lQZVW0UZZ+gSuTK+H95ItJWXq5xYX/ziVSpZ9X/nGqbP65KI8uJVJViWJ9KDiMzJw/bTl7Kr1FNxJYTF5GVaxlUwwJ98oNqGDrVDUeDyGCunUpUSgyuRFSxslKB05suX/aXLVSzUy0f4+ENfbXWOJxdBbHdboNX7LVAAJftIccKqluOX8T6/Ev/ElqlHMBcRJAEVeOlf5lQVT8yCDr+sUV0VRhciah8pcZb7kYVtwMwWNb2wTcUqNXx8ohqtdbIgxf2LFqE2g1uALy5RBVpKz07F1uOS43qBRVUpQwgO88yqFYJ9jWOptYxhlXZMpVBlci+GFyJyL71qRcO5wfV/I/EI0UfF1ozP6TmB9UqjYtum5pT+l2AiOwlLStX1aWaFvzffvIScvWW6wBXDfFVIdU0qhobEcigSlTOGFyJqOzycowjqAVBdT2QnlDoQTqgatPLIVWWpapUU6MGE1mXmpWLjccSCxb833U6qUhQjQ71KxhNlbAaEx7AoEpUwRhciajkMpOBUxvM6lM3AbkZlo/x9M3fNjU/qMq2qf6VtGoxkVXJmTlq7dSCoHomWW2raq56Jf+CNVQ7xYajZpg/gyqRxhhcici25DOW9anndgMGy7o++Fe+PNNfPsui/16+WrWYyKqkjBxszJ9IJZf+d59JQqGcqoKpBFTTzlQ1wwK0ai4R2cDgSkRGej2QcMAsqK4FLp0o+rjKtS2Danj9ovWpRBq7lJ6tAqqMqMqEqj1xyaoE21zt8ABjfWpd46X/apX8tWouEZUQgyuRu8rNMu5AVbBt6n9AxkXLx+g8gKjmZtumdgJCuL85OZ7EtGysPRiPX456YNb7a7E/PrVIUK0TEVhw6V+CalSon1bNJaIyYnAlchcSSk9uuBxUT28B8rIsH+MdkF+fmh9UpT7VN1irFhPZlJCahQ1qRFWWp0rE/nMp+V+R0X/jusB1qwQaQ6qqUQ1DZAiDKpGzY3AlclVymd98N6r4PUUfE1jFcjeqqBaAJ9dMJcdzPiWrYA1Vufx/ML7QphUA6kcGoqouBbd0a4Vr6keqdVWJyLUwuBK5An2eMZiaB9Xk00UfF17PLKh2BsLqcNtUckjnkjMLJlLJ5yPn04o8plFUcMGC/x1iwxDi64FFixahf/MoeHPTCiKXxOBKpCV9HnTH/0H1xHXQHQ8B6nQFPDyv/H05GcDpzWb1qRuArGTLx3h4GWf4m9enBlUpt7dCdDXikjIKJlLJpf+jCZZBVf6+ahQVUrCGqgTVsEAfi8fkcNMKIpfH4EqklT0LgMXPwCv5DNrJ/eOzgJBqwA2vAk0GWT427QJw0mw09cw2QF/ol7RPEFCzw+WgWr0t4BNYke+IqMROX5Kgmn/p/2gijl9ILxJUm1YLUSFVRlUlqFYKsAyqROR+GFyJtAqtP90pe6RaHk+OMx7v/7oxdJqCqixTVVhQFBDT+XJQjWwKePKfNDmmk4npFpf+T1203LjCQwc0qx5acOm/Xe0whPrzcj8RWeJvOSIt6lEXP1M0tCr5xxY9WfRLVRpZTqSqFMP6VHJIBoMBJxMzVEBdf9Q4mUpGWM15euhUUJVL/7Lof9valRHix6BKRMVjcCWqaMfXGnekupIqjYEGfYxBtWZHICCsIlpHVKageuxCusWl/7ikTIvHeHno0LyGBFXjpX8ZUQ3y5a8gIiod/l+DqKJHWw/8VbLHdn0SaH5zebeIqExB9UhCWsHSVPI5PsVyTWBvTx1a1qhUsOB/m1qVEcigSkRXSdP/i8yaNUt9HDt2TN1v2rQpXnjhBfTr10/d7969O1atWmXxPffffz8++ugjTdpLVGZpCcDWb4BNX1jfRtWaoKrl3SqiEgfVw+dTsU5m/eePqMq6quZ8PD3QqqZlUPX3KcEKGUREzhJca9SogRkzZqB+/frqf4xfffUVBg8ejK1bt6oQK+69915Mmzat4HsCAgI0bDFRKch+k6c2ARs/A3b/CuRlG4/7VQL0uUB2mo06V51xdYGYayq6xUSKXm9QC/ybFvyXHaoSUvPP33w+Xh5oXbNS/s5UYSqo+nkzqBKRCwfXgQMHWtx/+eWX1Qjs+vXrC4KrBNWoqKgSP2dWVpb6MElOTi5Y368i1vgzvQbXE3TjfslJh27XL/Dc/AV053YWHNZHt4K+7T0wNBkC3eHl8PzlLhVSdWbh1SChFUBe75dhyNMD8uGm3Oqc0bhvJKgeiE/FhmMX1WjqxmMXcTHd8rl9vTzQplYltK9dGR1jK6Nl9VD4WgRVPXJytD1fec7Yxr6xjv3iOH1T0tfRGWSo0wHk5eVh7ty5GD16tBpxbdKkiSoV2L17txqNlfAqQXfSpEnFjrpOmTIFU6dOLXJ8zpw5HK2lchWYGYfYhBWombgGPnnGNSnzdN44XbkTjkb0xKXAOhaPj760Ec1PfQf/nMSCY+neYdhVYxTiKrWv8PaT+9AbgDPpwKFkHQ4l6XA4RYf0XMsVKnw8DKgdbEC9EONHTJBMsNKsyUTk4tLT0zFy5EgkJSUhJCTEcYPrzp070blzZ2RmZiIoKEgFzP79+6uvffLJJ4iJiUG1atWwY8cOPPPMM+jQoQN+/fXXUo241qxZEwkJCcV2hD3/Yli6dCl69+7NLQfdoV/0udAd/Asem7+Ax9GVBYcNlWpD33YM9C1GFr8agD4PeUf/wa51y9Cscy94xl5Xsp2z3IDLnjNXKU9vwPrD57Fi3WZc37ktOtWtopaWutL37I1LwYZjiWpUVUZUkzNzLR4T4OOpRlQ71q6sFvtvVi1ElQM4E54ztrFvrGO/OE7fSF6LiIi4YnDVfIpnw4YNsW3bNtXQn3/+WY24yoQsGXG97777Ch7XvHlzREdHo2fPnjh8+DDq1q1r9fl8fX3VR2HS6RV5Ulb06zkLl+mX1Hhgy1fApi+B5FP5B3VAg75A+/9BV7cnPD08cOUI6g3U7YbT+9PQsm431+gbO3OZc8YOFu+Kw9SFe/KXmvLE1we3ITrUD5MHNsENzaILHpebp8fuM8kF26duPJaIlEJBNdDHE+1jjdunSo1q8+qh8PZ0rqBqC88Z29g31rFftO+bkr6G5sHVx8cH9erVU7fbtm2LjRs34p133sHHH39c5LEdO3ZUnw8dOmQzuBKVG7k4IbtYbfzUuPOVacvVgHCg9R1Au7uByjFat5JcOLQ++O2WItP5ziZlquNP9W0InU6nwuqmYxeRmmUZVIN9vfKDqnHWv2yn6uUiQZWI3IfmwbUwvV5vcanfnIzMChl5JaowWanAjh+BjZ8D8bsvH6/RHmh/L9BkMODtp2ULycXJpX4ZaS1mrzW8tmS/xfEQPy91yd+44H84mlQLuWJJARGRo9M0uD733HNqzdZatWohJSVF1beuXLkSS5YsUeUApnrX8PBwVeP62GOPoWvXrmjRooWWzSZ3Eb8P2PQ5sO17IDvFeMzLH2hxC9DuHqBaK61bSG5ig5WdqKxpF1MZ/ZpHq1HVxtEMqkTkejQNrvHx8bjzzjsRFxeH0NBQFUgltEoh8MmTJ7Fs2TK8/fbbSEtLUxOshg0bhokTJ2rZZHJ1eTnAvj+Ma68eW3P5eFhdVbuKVrcB/pW1bCG5ofiUK4dWcUfnGAxuVb3c20NE5JbB9fPPP7f5NQmqhXfNIio3yXHGyVabvwRS4ozHdB5Aw/5A+3uA2O6AB+sBqeJdSs/Gwu1nSvTYyGCWrBCRa3O4GleiCp1sdewf42Srvb8Dhjzj8cAqQJvRQLu7gNAaWreS3JRsCvDLllOY/uc+JKZZ7lpVmBQERIX6qZpWIiJXxuBK7iczGdj+g7EcIMFsQkutzsZygMaDAC8fLVtIbm7f2WRMmr9Lrbcq6kcGYWDLapi59IC6bz5Jy1TFKktisaaViFwdgyu5j3O7jWF1+49ATprxmHcg0PJW42SrqGZat5DcnCxh9fbSA5i99phaScDf2xOP9qqPu6+LVWusNqgaZLaOq1GUlXVciYhcFYMrubbcbGDvAuNSVifWXj4e0dA4uiqh1S9UyxYSqW2t/9gZhxd/34NzycblAPs1i8KkG5ugWiX/gsdJOO3dJArrDsXjrzX/oU+XjuhcL5IjrUTkNhhcyTUlnTJOtNr8FZAWbzym8wQa32gMrLW7ADr+siftHTmfiskLdmPNwQR1PyY8AFMHNUX3hpFWHy8hVZa7urDXoD4ztBKRO2FwJdeh1wNHVxnLAfYvAgx64/GgKKDtGKDtaCCkmtatJFIyc/Lwwd+H8PGqI8jO08PHywMPda+LB7rVhZ/3lTcLJiJyRwyu5PwyLgHb5hg3C7hw6PJxGVWV0dVGAwBP7kFNjmP53nOYsnA3TiZmqPvdGlTBtMFNERMeqHXTiIgcGoMrOa+47cbR1R1zgVxjAIBPsHGTAJlsFdlI6xYSWTh1MV1Nrlq655y6H50/sapv0yjoWLpCRHRFDK7kXHIygT2/GddePbXx8vHIJsbR1RbDAd9gLVtIVER2rh6frjmC91YcRGaOHl4eOtzTJRaPXF8fgb783zARUUnx/5jkHC4eBzbPBrZ8DaRfMB7z8AaaDDIGVlmDlSNW5IDWHkrApN924fB54xJsMqHqxSHN0KAq/8AiIiotBldy7MlWh1cYR1cPLLm87HpIdaDtXUCbO4Hgqlq3ksiq+ORMvPTHXizI3641IsgHzw9ojCGtqrMsgIiojBhcyfGkJwJbvzVOtrp47PLxOt2B9vcCDW4APHnqkmPKzdPj63XH8dbSA2pDAVmt6o5OMXi8T0OE+nOSIBHR1eBvf3IcpzcbNwrY9QuQm78zkG8o0HoU0O5uIKK+1i0kKtbm4xcxcf4u7I1LVvdb1qyEl4c0Q7Pq3OSCiMgeGFxJWzkZwK5fjeUAZ7ZePh7V3Di62vxmwIdLBJFjS0zLxqt/7sOPm06q+zKy+swNjTCifU14cIMAIiK7YXAlbSQeATZ9YSwJyLhoPObpAzQdapxsVaM9J1uRw9PrDfhp00nMWLwPl9Jz1LHh7Wqo0Boe5Kt184iIXA6DK1Ucgx66A4uBrV8Ch5ZdPh5aC2iXP9kqMELLFhKV2K7TSWq1gK0nLqn7jaKC8dKQZmhXO0zrphERuSwGVyp/aQnw2PQleu+ZBa9txv3YlXq9jOUA9XsDHtzikpxDcmYO3vrrAL5edwx6AxDo46kmXo3uHAMvTw+tm0dE5NIYXKl8GAzGDQJkZ6vd8+CZl40AOexfGbrWtxsnW4XV0bqVRCVmMBjw27YzaomrhNQsdWxgy2qYOKAxqob4ad08IiK3wOBK9pWdBuz82RhYz+4oOKyPbo1t3u3QfMQL8A4I0bSJRKV1KD5FrRaw/kiiul8nIhDTBjfDdfVZ2kJEVJEYXMk+Eg4Zw+q2OUBWkvGYlx/QbBjQ/h7kRbbAyUWL0NzbX+uWEpVYenYu3l1+CJ+tOYJcvQF+3h54+Pr6+F+XWPh6sbyFiKiiMbhS2eXlAjLZSpayOrLy8vHKtYF29wBSEhCQP1ElxzjjmshZygL+2nMO0xbuwelLGepYr8ZVMXlgE9QMk6IXIiLSAoMrlV7KOWDL18Dm2UDy6fyDOqBBX+Nkq7rXAx6cpELO6cSFdExesAt/7z+v7lev5I8pg5qidxNuL0xEpDUGVyr5ZKsT64zlAHsWAPr8EdSAcOMyVm3vAirHaN1KojLLzMnDJ6uP4IO/DyErVw9vTx3u71oXY3vUg78PywKIiBwBgysVLysF2PGTcSvW+N2Xj9foYNwooMlgwJszqsm5rT5wHi/8tgvHLqSr+9fWC1eTr+pWCdK6aUREZIbBlayL3wds+hzY9j2QnWI85uUPtLjFWL9arZXWLSS6anFJGXjx9z1YtPOsuh8Z7ItJNzbBjS2ioePObUREDofBlS7LywH2/W4cXT225vLxsLrG0dVWtwH+lbVsIZFd5OTpMfvfo3h72UGkZ+fB00OH0Z1r47He9RHs561184iIyAYGVwKS44DNXxo/Uo0jT9B5AA37GwNrbDdOtiKXseFoIibO34kD51LV/bYxlfHi4GZoUo3rCxMROToGV3eebCWjqjLZau/vgCHPeDwwEmg7Gmg7BgitoXUriexGdrt6ZdFe/LrFuBJGWKAPnr2hEW5uWwMeHiwLICJyBgyu7iYzCdj+ozGwJuy/fLzWNWqjADQeBHj5aNlCIrvK0xswZ8MJvL54H5IzcyGlq7d1qIWn+zZEpQCe60REzoTB1V2c3WUMq7JCQE6a8Zh3INDyVuNkq6hmWreQyO62n7yESb/two5Txt3cmlYLwUtDmqF1LdZqExE5IwZXV5abDexdYAyssgarSURDY+1qyxGAH+v6yPUkpefg9b/24bv/TqiqmGA/LzzVtyFGdYxRE7GIiMg5Mbi6oksnjROttnwFpBl3/4HOE2h8o3Fnq9rXQV0vJXLBrVp/2XIa0xftxYW0bHVsaOvqeK5/I0QGc71hIiJnx+DqKvR64OhK41JW+xcBBr3xeFAU0O4uoM1oICRa61YSlZt9Z5Mxaf4ubDx2Ud2vHxmkNhHoXDdc66YREZGdMLg6u4yLxk0CpBwg8fDl47W7GMsBGg0APLkuJbmu1KxcvL30AGavPaYmYvl7e2J8r/q4+9pY+HhxGTciIlfC4Oqs4rYDGz4Fdv4M5GYYj/kEGzcJkMlWkY20biFRuZcFyI5X037fjXPJWerYDU2jMGlgE1Sv5K9184iIqBwwuDqTnExgz3zj6OqpjZePRzY1LmXV4lbAl3urk+s7cj4VkxfsxpqDCep+THgApgxqih4NI7VuGhERlSMGV2dw8RiwaTaw9Rsg/YLxmIc30GSQcbJVrU6cbEVuITMnDx/+fQgfrTqC7Dy9KgV4qHtdPNCtLvy8PbVuHhERlTMGV0eebHV4uXF09cASuTBqPB5S3TjZqvWdQHBVrVtJVGFW7DunRllPJhpLY7o1qIKpg5qidkSg1k0jIqIKwuDqaNITga3fAps+N460mtTpYZxs1eAGwJM/NnIfpy9l4JU/t+OvPefU/ehQP0we2AR9m0ZBxysNRERuhQnIUZzeDGz4DNj1C5BnnGgC31Cg9Sig3d1ARH2tW0hUobJz9Vh6Wodn3v0XmTl6eHnocM91sXikZ30E+vJ/XURE7oj/99dSToYxqEo5wJmtl49HNTfWrja/GfDhZVByP2sPJ2DivF04kiB1q3p0iA1TW7U2qBqsddOIiEhDDK5auHAY2PSFsSQg85LxmKcP0HSoMbDWaMfJVuSW4pMz8fKivfht2xl1P8jbgMmDmuPmdrVYFkBERAyudqXPg+74P6ieuA664yFAna6AR/5MZ30ecPAv49qrMunKJLQW0P5uoPUdQGCEZk0n0lJunh7frD+Ot/46gJSsXHjogJEdaqKp/iiGtKrG0EpERAqDq73sWQAsfgZeyWfQTu4fnwWEVAN6TARSzxmXs0o6kf9gHVCvl3GyVf3el8MtkRvacuKiKgvYE5es7resWQkvDW6GRlUDsGjRUa2bR0REDoTB1V6h9ac7Ly9ZZZJ8Bvjtocv3/SsDrW83TrYKq1PhzSRyJBfTsvHq4n34YeNJdT/U3xvP3NAII9rXhIeHDjk5OVo3kYiIHAyD69WSEoDFzxQNreZks4AbZxonW3lzK0pyb3q9AT9tOqlC68V0Yzi9pW0NPNuvEcKDfLVuHhEROTAG16t1fK1xZLU4+hygcm2GVnJ7u88kYeL8Xdh6wjgpsVFUsFotoF3tMK2bRkREToDB9WpJ/ao9H0fkgpIzc9TEq6/XHYPeAAT6eOKx3g0w5pra8PL00Lp5RETkJBhcr1ZQVfs+jsiFGAwGLNh+Bi/9sRfnU4wba9zYIhoTBzRBVKif1s0jIiInw+B6tWKuMa4ekBxno85VZ/y6PI7IjRyKT8Gk+bux7sgFdb9ORCCmDW6G6+pz2TciIiobBterJUtZ3fBq/qoCukLhNX/tyRtmcMkrchvp2bl4b8UhfLbmCHLyDPD18sDD19fDvV3rwNeL/w6IiKjsGFztockgYPjXxtUFzCdqyUirhFb5OpEblAX8teccpi3cg9OXMtSxXo0jMXlgU9QMC9C6eURE5G7BVa/XY9WqVVizZg2OHz+O9PR0VKlSBa1bt0avXr1Qs2ZNuC0Jp40GIPfIamxbswStuvSFl/nOWUQu7MSFdExZuBsr9sWr+9Ur+WPKoKbo3YS13UREZD8lms6bkZGBl156SQXT/v37488//8SlS5fg6emJQ4cOYfLkyYiNjVVfW79+PdyWhycMMdfhdFhn9ZmhlVxdVm4e3l1+EL1nrlKh1dtTh7E96mLZ490YWomISJsR1wYNGqBz58749NNP0bt3b3h7exd5jIzAzpkzByNGjMDzzz+Pe++91/6tJSKHsfrAeUxesBtHE9LU/WvrhWPqoGaoFxmkddOIiMidg+tff/2Fxo0bF/uYmJgYPPfcc3jyySdx4sQJe7WPiBzM2aRMvPj7HvyxU1bSACKDfTHxxiYY2CIaOl3+hEQiIiKtguuVQqs5GY2tW7fu1bSJiBxQTp4eX609hplLDyAtOw8eOmDMNbF4rHd9BPsVvQpDRETkMKsK5Obm4uOPP8bKlSuRl5eHa6+9FmPHjoWfHxcVJ3I1G48lYuK8Xdh/LkXdb1OrEl4c0gxNq4Vq3TQiInIjZQ6ujzzyCA4cOICbbroJOTk5+Prrr7Fp0yZ8//339m0hEWkmITUL0xftwy9bTqn7lQO88Vy/xri5bQ14yJArERGRIwbXefPmYejQoRZ1r/v371crC4i+ffuiU6dO5dNKIqpQeXoD5mw4gdcX70NyZi6kdHVE+1p4um9DVA700bp5RETkpkocXL/44gt89dVX+PDDD1GtWjW0adMGDzzwAIYNG6ZGXGXFgfbt25dva4mo3O04dQkT5+/CjlNJ6n7TaiF4aUgztK5VWeumERGRmytxcF24cCF+/PFHdO/eHQ8//DA++eQTvPjii2rpK1ON65QpU8q3tURUbpLSc/D6X/vw3X8nYDAAwb5eeLJvQ9zeKQaeLAsgIiJnq3G99dZbVUnA008/rT5/9NFHePPNN8uvdURUIVu1/rrlNF5ZtBcX0rLVsaGtq+O5/o0QGczJlkRE5MSTsypVqqRGW1evXo0777wTN9xwgxp55WoCRM5n/9kUTJq/CxuOJar7snnAi4OboXPdcK2bRkREVLYtX4VsKjB8+HA0b94co0aNQv369bF582YEBASgZcuWahtYInIOaVm5aoS1/7trVGj19/bEs/0aYdEjXRhaiYjI+YOrjK56eHjg9ddfR2RkJO6//374+Phg6tSpmD9/PqZPn66CbWnMmjULLVq0QEhIiPqQbWXNA3BmZqZaGzY8PBxBQUFqIti5c+dK9w6JyKIsYNHOOPR8cxU+WX1ErR7Qt2lVLHuiGx7oVhc+XiX+XwIREZHjlgrIGq3bt29Xu2JJfWtsbKzFzlpSOiAlBKVRo0YNzJgxQ43eyi9UWbVg8ODB2Lp1K5o2bYrHHnsMf/zxB+bOnYvQ0FCMGzdOrRv777//lu5dEhGOJqThhd92Yc3BBHW/VlgApg5qih6NIrVuGhERkX2Da9u2bfHCCy9g9OjRWLZsmSoZKOy+++5DaQwcONDi/ssvv6xGYdevX69C7eeff445c+bg+uuvV1+fPXu2Csnyda4ZS1QymTl5+PDvQ/ho1RFk5+nVqOqD3eriwe514edtXIeZiIjIpYKr7Iz1xBNPqFHQVq1aqe1e7UmW1JKR1bS0NFUyIPWzsj5sr169Ch7TqFEj1KpVC+vWrbMZXLOystSHSXJysvoszyUf5c30GhXxWs6E/aJN36w8cB5Tf9+HUxcz1P0u9cIx+cbGiAkPAKBHTo4ejornjG3sG+vYL7axb6xjvzhO35T0dXQGuUavoZ07d6qgKvWsUscqI6z9+/dXn++66y6LECo6dOiAHj164NVXX7X6fLKWrNTdFibPJxPJiNxBYhbw61EP7LxorFkN9THgptp6tAwzqF2wiIiIHEl6ejpGjhyJpKQkNe/pqkZcZRQ0MDCwxC9emsc3bNgQ27ZtUw39+eefVSnCqlWrUFbPPfccHn/8cYsR15o1a6JPnz7FdoQ9/2JYunQpevfuDW9v73J/PWfBfqmYvsnO1WP22uP4YNNhZOTo4eWhw5hrYjCuex0E+pZ69TtN8ZyxjX1jHfvFNvaNdewXx+kb0xXyKynRb7J69eph/PjxKlRGR0dbfYwM3Ert61tvvYWuXbuqAFkSsjKBPL+pjnbjxo1455131GYH2dnZuHTpklo71kRWFYiKirL5fL6+vuqjMOn0ijwpK/r1nAX7pfz6Zu3hBLzw224cik9V9zvUDsOLQ5qhYVQwnBnPGdvYN9axX2xj31jHftG+b0r6GiUKritXrsSECRPUZXhZs7Vdu3aoVq2a2nTg4sWL2LNnj6o79fLyUoFVlsoqK71er8oDJMTKm1i+fLlaBkvs379frScrpQVEZBSfkolX/tiL+dvOqPsRQT6Y0L+x2v1Kx7oAIiJyIV4lvZz/yy+/qNAoE6jWrFmDtWvXIiMjAxEREWjdujU+/fRT9OvXD56eJZ+lLCFXvkcmXKWkpKg6VAnJS5YsUctf3XPPPeqyf1hYmLrM//DDD6vQyhUFiIDcPD2+XX8cb/51AClZuap29Y5OMXiiT0OE+nPkgIiIXE+pit4kYMrKAvJhD/Hx8Wpjg7i4OBVUZTMCCa1STyFmzpypNj2QEVcZhZX1Yz/88EO7vDaRM9ty4qLaqnX3GWNNUMsaoXhpSHM0rxGqddOIiIjKjaazNWSd1uJIKcIHH3ygPogIuJiWjdeW7MP3G06q+zKy+vQNDTGifS14erAsgIiIXJtzTTMmclN6vQFzN5/EjD/34WK6ca27W9rWwLP9GiE8qOhkRCIiIlfE4Erk4HafSVJlAVtOXFL3G0UFq9UC2tcO07ppREREFYrBlchBpWTm4K2lB/DV2mPQG4BAH0881rsBRl9TG96exo0FiIiI3AmDK5GG8vQG/Hc0EZsTdAg/mojO9SIhpaoLtp/BS3/sxfkU485xA1pEY9KAJogK9dO6yURERM4TXGvXro27774bY8aMUasMEFHZLN4Vh6kL9yAuKROAJ74+uEmtwRoe6IP954ybCMRGBGLa4KboUr+K1s0lIiLSXKmvNz766KP49ddfUadOHbVs1Q8//KCWqiKi0oXWB7/dkh9aL0tIzVahVbZqfaJ3Ayx+tAtDKxER0dUE123btmHDhg1o3Lix2hRAtoEdN24ctmzZUtqnI3LL8gAZaTUU85iwQB881KMefL1KvqEHERGRqyvzDI82bdrg3XffxZkzZzB58mR89tlnaN++PVq1aoUvvvgCBkNxv5aJ3NeGo4lFRloLi0/JUo8jIiIiO0zOysnJwbx58zB79mwsXbpUbcMqW7SeOnUKEyZMwLJly9QWrkRkKT4l066PIyIichelDq5SDiBh9fvvv1fbscqWrbI1a6NGjQoeM3ToUDX6SkRFRQb72fVxRERE7qLUwVUCqUzKmjVrFoYMGQJvb+8ij4mNjcWIESPs1UYil9IhNgwBPp5Iz86z+nXZuFWWvZLHERER0VUE1yNHjiAmJqbYxwQGBqpRWSIqaumes8WGVjF5YBN4yoKuREREVPbJWfHx8fjvv/+KHJdjmzZtKu3TEbmV4xfS8NTPO9Tt3k2qIrrQhgIy0jrr9ja4oVm0Ri0kIiJyoRHXsWPH4umnn0bHjh0tjp8+fRqvvvqq1VBLREBmTh7GztmClMxctIupjA9HtYGHTod1h+Lx15r/0KdLR7VzFkdaiYiI7BRc9+zZo5bCKqx169bqa0Rk3ct/7MWu08lqjdb3RraGt6fxgkfH2DBc2GtQnxlaiYiI7Fgq4Ovri3PnzhU5HhcXBy+vMq+uReTSFm4/g2/WH1e33xreEtGh/lo3iYiIyPWDa58+ffDcc88hKSmp4NilS5fU2q2y2gARWTpyPhXP/mKsax3Xox66N4zUuklEREROqdRDpG+88Qa6du2qVhaQ8gAhW8BWrVoV33zzTXm0kcip61of+m4L0rLzVCnAo73qa90kIiIi9wmu1atXx44dO/Ddd99h+/bt8Pf3x1133YXbbrvN6pquRO5syoLd2Hc2BRFBPnjvttbwyq9rJSIiotIrU1GqrNN63333leVbidzGvK2n8MPGk9DpgHdGtEZkCHfCIiIiuhplnk0lKwicOHEC2dnZFscHDRp0VQ0icgUHz6Vgwq+71O3xPevj2noRWjeJiIjIPXfOGjp0KHbu3AmdTgeDwaCOy22Rl2d9RyAid5GenavqWjNy8nBdvQg8fD3rWomIiOyh1AV348ePR2xsrNpBKyAgALt378bq1avRrl07rFy50i6NInJW8ofcxPm7cDA+FZHBvnh7RCuuzUpERKTViOu6deuwYsUKREREwMPDQ31cd911mD59Oh555BFs3brVXm0jcjpzN53Cr1tOQ7Lqu7e1RkSQr9ZNIiIict8RVykFCA4OVrclvJ45c0bdluWx9u/fb/8WEjmJfWeTMek3Y13rE30aolOdcK2bRERE5N4jrs2aNVPLYEm5QMeOHfHaa6/Bx8cHn3zyCerUqVM+rSRycKlZxrrWrFw9ujesgge71dW6SURERC6n1MF14sSJSEtLU7enTZuGG2+8EV26dEF4eDh+/PHH8mgjkcPXtU74dSeOnE9DdKgf3hreCh6sayUiItI+uPbt27fgdr169bBv3z4kJiaicuXKBSsLELmTORtOYMH2M/Dy0OH9ka0RFuijdZOIiIhcUqlqXHNycuDl5YVdu4x1fCZhYWEMreSWdp1OwtSFe9Ttp29oiLYxYVo3iYiIyGWVKrjKlq61atXiWq1EAJIzczB2zhZk5+rRq3Ek7u3CGm8iIiKHWlXg+eefx4QJE1R5AJE717U++8sOHL+QjuqV/PHmLa141YGIiMjRalzff/99HDp0CNWqVVNLYAUGBlp8fcuWLfZsH5FD+mrtMSzaeRbenjp8MKoNQgO8tW4SERGRyyt1cB0yZEj5tITISWw/eQkvL9qrbk/o3xitalbSuklERERuodTBdfLkyeXTEiInkJSeo9ZrzckzoF+zKIy5prbWTSIiInIbpa5xJXLnutYnf96O05cyUCssAK/e3IJ1rURERI484urh4VHsL2uuOECu6vN/jmLpnnPw8fTAh6PaIMSPda1EREQOHVznzZtXZG3XrVu34quvvsLUqVPt2TYih7H5+EXM+HOfuj1pYBM0qx6qdZOIiIjcTqmD6+DBg4scu/nmm9G0aVO15es999xjr7YROYSLadl4eM4W5OoNGNiyGm7vWEvrJhEREbklu9W4durUCcuXL7fX0xE5BL3egMd/2oYzSZmoExGI6Tc1Z10rERGRMwfXjIwMvPvuu6hevbo9no7IYXy0+jD+3n8evl4ear3WIN9SX6QgIiIiOyn1b+HKlStbjDjJTOuUlBQEBATg22+/tVe7iDT335ELePOvA+r2tMFN0Tg6ROsmERERubVSB9eZM2daBFdZZaBKlSro2LGjCrVEriAhNQsPf78VeXoDbmpdHcPb1dS6SURERG6v1MF1zJgx5dMSIgchYfWxH7chPiUL9SOD8NLQZqxrJSIicsYa19mzZ2Pu3LlFjssxWRKLyNl98PchrDmYAH9vT7Vea4AP61qJiIicMrhOnz4dERERRY5HRkbilVdesVe7iDSx9lACZi4z1rW+PLQZ6lcN1rpJREREVNbgeuLECcTGxhY5HhMTo75G5KzikzPxyA/bYDAAt7ariZva1NC6SURERHQ1wVVGVnfs2FHk+Pbt2xEeHl7apyNyCLl5ejzyw1Y1KatRVDCmDm6qdZOIiIjoaoPrbbfdhkceeQR///038vLy1MeKFSswfvx4jBgxorRPR+QQ3ll+EOuPJCLQx1Ot1+rn7al1k4iIiKiQUs86efHFF3Hs2DH07NkTXl7Gb9fr9bjzzjtZ40pOadWB83j/70Pq9vRhLVC3SpDWTSIiIiJ7BFcfHx/8+OOPeOmll7Bt2zb4+/ujefPmqsaVyNnEJWWopa+krnVUx1oY1LKa1k0iIiIiG8q8zk/9+vXVB5GzypG61u+3IjEtG02rhWDSjU20bhIRERHZs8Z12LBhePXVV4scf+2113DLLbeU9umINPPGX/ux8dhFBPt6qfVaWddKRETkYsF19erV6N+/f5Hj/fr1U18jcgbL957Dx6uOqNuv3dwCMeGBWjeJiIiI7B1cU1NTVZ1rYd7e3khOTi7t0xFVuFMX0/H4T9vV7THX1Ea/5tFaN4mIiIjKI7jKRCyZnFXYDz/8gCZNWCNIji07V49xc7YiKSMHLWuEYkL/xlo3iYiIiMprctakSZNw00034fDhw7j++uvVseXLl+P777/H3LlzS/t0RBXq1cX7sO3kJYT4eeH9kW3g41Xqv92IiIjIWYLrwIEDMX/+fLVm688//6yWw2rRogWWLVuGbt26lU8riexg8a6z+Pyfo+r2m8NboWZYgNZNIiIiovJeDmvAgAHqo7Bdu3ahWbNmZXlKonJ14kI6nvrZWNd6X9c66N2kqtZNIiIiolK66uukKSkp+OSTT9ChQwe0bNnyap+OyO6ycvMwds4WpGTmom1MZTzVt6HWTSIiIqKKDK6y9JVs8xodHY033nhD1buuX7++rE9HVG5e/mMvdp5OQuUAb7x3W2t4e7KulYiIyOVLBc6ePYsvv/wSn3/+uVr6avjw4cjKylI1r1xRgBzR7zvO4Ot1x9Xtt25thWqV/LVuEhEREZWRR2kmZTVs2BA7duzA22+/jTNnzuC9994r6+sSlbujCWl49ped6vZD3euiR8NIrZtEREREFTHi+ueff+KRRx7Bgw8+iPr161/NaxKVu8ycPDz03RakZuWiQ2wYHu/dQOsmERERUUWNuP7zzz9qIlbbtm3RsWNHvP/++0hISLja1ycqF1MX7sHeuGSEB/qoulYv1rUSERE5vRL/Nu/UqRM+/fRTxMXF4f7771c7ZVWrVg16vR5Lly5VoZbIEczfehrfbzgBnQ54Z0RrVA3x07pJREREZAelHoYKDAzE3XffrUZgd+7ciSeeeAIzZsxAZGQkBg0aZI82EZXZofhUTJhnrGt95Pr6uK5+hNZNIiIiIju5quunMlnrtddew6lTp9SWr6U1ffp0tG/fHsHBwSr4DhkyBPv377d4TPfu3aHT6Sw+HnjggatpNrmojGypa92M9Ow8XFsvHI/0ZC02ERGRK7FL4Z+np6cKnQsWLCjV961atQpjx45V679KuUFOTg769OmDtLQ0i8fde++9qkTB9CFhmaiwSb/twoFzqagS7Iu3b20NTw+d1k0iIiIirbd8tZfFixdb3Jc1YmXkdfPmzejatWvB8YCAAERFRWnQQnIWP206iZ83n4Jk1XdHtFbhlYiIiFyLpsG1sKSkJPU5LCzM4vh3332Hb7/9VoVXWU920qRJKsxaIxsiyIeJbJQgZDRXPsqb6TUq4rWcSXn2y4FzKXjht13q9vjr66FdrRCn6n+eM9axX2xj31jHfrGNfWMd+8Vx+qakr6MzGAwGOABZnUAmd126dElN/DL55JNPEBMTo1YwkM0PnnnmGXTo0AG//vqr1eeZMmUKpk6dWuT4nDlzbIZdcl5ZecCbOz1xLkOHRqF63N9Yr0ZdiYiIyHmkp6dj5MiRahAzJCTE8YOrbGwgmxxIaK1Ro4bNx61YsQI9e/bEoUOHULdu3RKNuNasWVOtOVtcR9jzLwap1+3duze8vb3L/fWcRXn0i5y6T/y8Ewt3nEXVEF8seKgzwgJ94Gx4zljHfrGNfWMd+8U29o117BfH6RvJaxEREVcMrg5RKjBu3Dj8/vvvWL16dbGhVcjmB8JWcPX19VUfhUmnV+RJWdGv5yzs2S9z/juhQqtMwvpgZBtUrRQIZ8Zzxjr2i23sG+vYL7axb6xjv2jfNyV9DU2Dq4yYPfzww5g3bx5WrlyJ2NjYK37Ptm3b1Ofo6OgKaCE5qt1nkjBl4W51++m+DdGutmVdNBEREbkeTYOrLIUltae//fabWsv17Nmz6nhoaCj8/f1x+PBh9fX+/fsjPDxc1bg+9thjasWBFi1aaNl00lBKZg7GfrcF2bl69GwUiXu71NG6SUREROTqwXXWrFkFmwyYmz17NsaMGQMfHx8sW7YMb7/9tlrbVWpVhw0bhokTJ2rUYtKajNI/+8tOHLuQjuqV/PHm8Jbw4GwsIiIit6B5qUBxJKjKJgVEJt+sP44/dsbB21OH90e2RqUA55uMRURERBrunEVUEXacuoQXf9+jbj/brzFa16qsdZOIiIioAjG4klNIysjB2DlbkJNnQN+mVXH3tbW1bhIRERFVMAZXcnhSUvLU3O04mZiBmmH+eO3mltDpWNdKRETkbhhcyeF9/s9R/LXnHHw8PfDhyLYI9edae0RERO6IwZUc2pYTFzHjz33q9qQbG6N5jVCtm0REREQaYXAlh3UxLRvjvtuCXL0BN7aIxu2dYrRuEhEREWmIwZUckl5vwBNzt+NMUiZiIwIx/abmrGslIiJycwyu5JA+WXMEK/bFw8fLAx+MbINgP9a1EhERuTsGV3I4G48l4vUl+9XtqYOaokm1EK2bRERERA6AwZUcyoXULIybswV5egOGtq6OEe1rat0kIiIichAMruRQda2P/rgN55KzULdKIF4a0ox1rURERFSAwZUcxgd/H8Kagwnw8/bAh6PaItDXS+smERERkQNhcCWHsPZwAmYuO6BuvzSkORpGBWvdJCIiInIwDK6kufiUTDzy/TboDcAtbWvg5rY1tG4SEREROSAGV9KUTMIa//02JKRmoWHVYEwb3EzrJhEREZGDYnAlTb2z/CDWHbmAAB9PfDCqDfx9PLVuEhERETkoBlfSzOoD5/HeioPqtuyMVS8ySOsmERERkQNjcCVNnE3KxGM/boPBAIzsWAuDW1XXuklERETk4BhcqcLl5unxyPdbcSEtG02iQ/DCjU20bhIRERE5AQZXqnBvLj2ADccSEeTrhQ9HtYGfN+taiYiI6MoYXKlCrTxwHrNWHla3Xx3WArUjArVuEhERETkJBleqMIlZwFM/71K3R3eOwYAW0Vo3iYiIiJwIgytViOxcPb464IlLGTloUSMUEwY01rpJRERE5GQYXKlCvLn0II6l6hDi54UPRraBrxfrWomIiKh0GFyp3P21+yy+WHtc3Z4xtBlqhgVo3SQiIiJyQgyuVK5OJqbjybnb1e3u0Xr0bhKpdZOIiIjISTG4UrnJys3D2DlbkJyZi1Y1QzGoll7rJhEREZETY3ClcjN90T7sOJWESgHeeGd4C3jybCMiIqKrwChB5eKPHXH4cu0xdXvm8FaoVslf6yYRERGRk2NwJbs7lpCGZ37ZoW4/0K0uejRiXSsRERFdPQZXsqvMnDw89N0WpGblokPtMDzZp4HWTSIiIiIXweBKdjXt9z3YE5eM8EAfvHtba3ixsJWIiIjshKmC7Oa3bacx578T0OmAmbe2QlSon9ZNIiIiIhfC4Ep2cSg+Fc/9ulPdfrhHPXRtUEXrJhEREZGLYXClq5aRnYex321BenYeOtcJx/herGslIiIi+2Nwpas2ecEu7D+XgoggX7xzWyt4eui0bhIRERG5IAZXuio/bz6FnzadgmTVd29rhchg1rUSERFR+WBwpTI7cC4FE+cb61of7dUA19SN0LpJRERE5MIYXKlM0rJy1XqtmTl6dKkfgXE96mndJCIiInJxDK5UagaDARPn71IrCVQN8cXbt7aCB+taiYiIqJwxuFKp/bjxJOZtPa0mYb13WxuEB/lq3SQiIiJyAwyuVCp7ziTjhQW71e0n+zREh9gwrZtEREREboLBlUosJTMHY+dsQXauHj0aVsH9Xeto3SQiIiJyIwyuVOK6VtkZ62hCGqqF+uGt4axrJSIioorF4Eol8u364/h9Rxy8PHR4f1QbVA700bpJRERE5GYYXOmKdp5Kwou/71W3n+3XCG1qVda6SUREROSGGFypWEkZOXhozmZk5+nRu0lV3HNdrNZNIiIiIjfF4ErF1rU+/fN2nEzMQI3K/njj5pbQ6VjXSkRERNpgcCWbZv97DEt2n4OPpwc+HNUGoQHeWjeJiIiI3BiDK1m19cRFvLLIWNf6/IDGaFGjktZNIiIiIjfH4EpFXErPxrg5W5GrN2BA82jc2TlG6yYRERERMbiSJb3egCd+2o7TlzJQOzwA04c1Z10rEREROQQGV7Lw6ZojWL4vHj5eHvhgVBuE+LGulYiIiBwDgysV2HQsEa8t2a9uTxnYFE2rhWrdJCIiIqICDK6kXEjNUnWteXoDBreqhts61NS6SUREREQWGFxJ1bU+9tN2nE3ORJ0qgXhlKOtaiYiIyPEwuBJmrTqM1QfOw8/buF5roK+X1k0iIiIiKoLB1c2tO3wBb/5lrGudNrgZGkWFaN0kIiIiIqsYXN3Y+ZQsPPLDVugNwLA2NTC8HetaiYiIyHExuLopmYQ1/oetKrw2qBqEF4c01bpJRERERMVicHVT7y4/iLWHLyDAx1PVtQb4sK6ViIiIHBuDqxv652AC3l1xUN1+eWgz1IsM1rpJRERERFfE4OpmziVnqhIBgwFqrdahrWto3SQiIiKiEmFwdSO5eXo8/P1WXEjLRuPoEEweyLpWIiIich4Mrm7kraUHsOFoIoJ8vfDByNbw8/bUuklEREREzhFcp0+fjvbt2yM4OBiRkZEYMmQI9u83rilqkpmZibFjxyI8PBxBQUEYNmwYzp07p1mbndXf++Px4crD6vaMYc1Rp0qQ1k0iIiIicp7gumrVKhVK169fj6VLlyInJwd9+vRBWlpawWMee+wxLFy4EHPnzlWPP3PmDG666SYtm+10zlzKwOM/blO37+gUgxtbVNO6SURERESlpukaSIsXL7a4/+WXX6qR182bN6Nr165ISkrC559/jjlz5uD6669Xj5k9ezYaN26swm6nTp00arnzyMnTY9ycLbiYnoPm1UMx8cbGWjeJiIiIqEwcavFOCaoiLCxMfZYAK6OwvXr1KnhMo0aNUKtWLaxbt85qcM3KylIfJsnJyeqzPI98lDfTa1TEa5XEjMX7seXEJQT7eeHt4c3hYdAjJ0df4e1wtH5xJOwb69gvtrFvrGO/2Ma+sY794jh9U9LX0RkMsjCS9vR6PQYNGoRLly7hn3/+UcdkpPWuu+6yCKKiQ4cO6NGjB1599dUizzNlyhRMnTq1yHF5roCAALiTnYk6fLbfOAHr7gZ5aBnuED9qIiIiIgvp6ekYOXKkGsQMCQmBw4+4Sq3rrl27CkJrWT333HN4/PHHLUZca9asqWpni+sIe/7FIPW6vXv3hre3N7Ry6mIGJn24ThbBwpjOtfBc/0bQkqP0iyNi31jHfrGNfWMd+8U29o117BfH6RvTFfIrcYjgOm7cOPz+++9YvXo1atS4vCB+VFQUsrOz1ShspUqVCo7LqgLyNWt8fX3VR2HS6RV5Ulb065nLztXj0Z92IDkzFy1rVsKEAU3h7eUYK59p2S+Ojn1jHfvFNvaNdewX29g31rFftO+bkr6GpmlGqhQktM6bNw8rVqxAbGysxdfbtm2r3sjy5csLjslyWSdOnEDnzp01aLFzeGXRXmw/lYRQf2+1XquPg4RWIiIioqvhpXV5gNSe/vbbb2ot17Nnz6rjoaGh8Pf3V5/vuecedelfJmzJpf6HH35YhVauKGDdnzvj8OXaY+r2W8NbokZl96rrJSIiItelaXCdNWuW+ty9e3eL47Lk1ZgxY9TtmTNnwsPDQ208IJO0+vbtiw8//FCT9jq64xfS8PTPO9Tt+7vVQc/GVbVuEhEREZFrBNeSLGjg5+eHDz74QH2QbZk5eXjouy1IycpFu5jKeLJPQ62bRERERGRXLH50ES/9sQe7zyQjLNAH741sDW9P/miJiIjItTDduIDftp3Gt+tPQKcz1rVGh/pr3SQiIiIiu2NwdXKHz6diwq871e2x3euhe8NIrZtEREREVC4YXJ28rnXsd1uQlp2HjrFheLRXfa2bRERERFRuGFyd2OTfdmPf2RREBPngvdtaw4t1rUREROTCmHSc1C+bT+HHTSdVXes7I1ojMsRP6yYRERERlSsGVyd08FwKJs7fpW4/2rMBrq0XoXWTiIiIiModg6uTSc/OVeu1ZuTk4bp6ERh3fT2tm0RERERUIRhcnYhs2DBx3i4cjE9FZLAv3h7RCp4eOq2bRURERFQhGFydyE+bTuLXrachWVUmY0UE+WrdJCIiIqIKw+DqJPbGJeOF33ar20/0aYiOdcK1bhIRERFRhWJwdQKpWblqvdasXD26N6yCB7vV1bpJRERERBWOwdUJ6lqf+3UnjiSkITrUD28NbwUP1rUSERGRG2JwdXDf/XcCC7efgZeHDu+PbI2wQB+tm0RERESkCQZXB7brdBKmLdyjbj9zQyO0jQnTuklEREREmmFwdVDJmTlqvdbsPD16Na6K/3WJ1bpJRERERJpicHXQutZnft6BE4npqF7JH2/e0hI62duViIiIyI0xuDqgL9cew5+7zsLbU4cPRrVBaIC31k0iIiIi0hyDq4PZdvISXlm0V92e0L8xWtWspHWTiIiIiBwCg6sDuZSerdZrzckzoF+zKIy5prbWTSIiIiJyGAyuDlTX+uTcHTh9KQMx4QF49eYWrGslIiIiMsPg6iA+W3MUy/aeg4+nBz4Y2QYhfqxrJSIiIjLH4OoANh9PxIzF+9TtFwY2QbPqoVo3iYiIiMjhMLhqLDEtG+PmbEWe3oCBLathVMdaWjeJiIiIyCExuGpIrzfgsR+3IS4pE3UiAjH9puasayUiIiKygcFVQ7NWHcaqA+fh6+Wh1msN8vXSuklEREREDovBVSPrj1zAm3/tV7enDW6KxtEhWjeJiIiIyKExuGrgfEoWHvl+K/QG4KY21TG8XU2tm0RERETk8BhcK1hefl1rfEoW6kcG4aUhzVjXSkRERFQCDK4V7P0Vh/DPoQT4e3viw1FtEODDulYiIiKikmBwrUD/HkrA28sPqNsvD22G+lWDtW4SERERkdNgcK0g8cmZGP/DVhgMwK3tauKmNjW0bhIRERGRU2FwrQC5eXo8/P1WJKRmo1FUMKYObqp1k4iIiIicDoNrBXh72UH8dzQRgT7GulY/b0+tm0RERETkdBhcy9nK/fF4/+9D6vb0YS1Qp0qQ1k0iIiIickoMruUoLilDLX0lbu9UC4NaVtO6SUREREROi2sx2XmNVikJ2JygQ+ihBLz39xFcTM9B02ohmDigidbNIyIiInJqDK52snhXHKYu3IO4pEwAnvj64BZ13M/Lg3WtRERERHbAUgE7hdYHv92SH1otZebqsTcuWZN2EREREbkSBlc7lAfISKvBxtdlM1f5ujyOiIiIiMqOwfUqbTiaaHWk1UTiqnxdHkdEREREZcfgepXiUzLt+jgiIiIiso7B9SpFBvvZ9XFEREREZB2D61XqEBuG6FA/VctqjRyXr8vjiIiIiKjsGFyvkqeHDpMHGtdoLRxeTffl6/I4IiIiIio7Blc7uKFZNGbd3gZRoZblAHJfjsvXiYiIiOjqcAMCO5Fw2rtJFNYdisdfa/5Dny4d0bleJEdaiYiIiOyEwdWOJKR2jA3Dhb0G9ZmhlYiIiMh+WCpARERERE6BwZWIiIiInAKDKxERERE5BQZXIiIiInIKDK5ERERE5BQYXImIiIjIKTC4EhEREZFTYHAlIiIiIqfA4EpEREREToHBlYiIiIicgstv+WowGNTn5OTkCnm9nJwcpKenq9fz9vaukNd0BuwX29g31rFfbGPfWMd+sY19Yx37xXH6xpTTTLnNbYNrSkqK+lyzZk2tm0JEREREV8htoaGhNr+uM1wp2jo5vV6PM2fOIDg4GDqdrkL+YpCQfPLkSYSEhJT76zkL9ott7Bvr2C+2sW+sY7/Yxr6xjv3iOH0jcVRCa7Vq1eDh4eG+I67y5mvUqFHhrys/ZP4jKIr9Yhv7xjr2i23sG+vYL7axb6xjvzhG3xQ30mrCyVlERERE5BQYXImIiIjIKTC42pmvry8mT56sPtNl7Bfb2DfWsV9sY99Yx36xjX1jHfvF+frG5SdnEREREZFr4IgrERERETkFBlciIiIicgoMrkRERETkFBhciYiIiMgpMLiW0urVqzFw4EC1s4PsxDV//vwrfs/KlSvRpk0bNTOvXr16+PLLL+Hu/SJ9Io8r/HH27Fm4kunTp6N9+/Zq57bIyEgMGTIE+/fvv+L3zZ07F40aNYKfnx+aN2+ORYsWwd37Rf7dFD5fpH9czaxZs9CiRYuCRb87d+6MP//8063Pl7L0i7ucL4XNmDFDvddHH30U7n7OlKVv3OW8mTJlSpH3KeeDM5wzDK6llJaWhpYtW+KDDz4o0eOPHj2KAQMGoEePHti2bZv6B/O///0PS5YsgTv3i4mElbi4uIIPCTGuZNWqVRg7dizWr1+PpUuXIicnB3369FH9ZcvatWtx22234Z577sHWrVtVqJOPXbt2wZ37RUhgMT9fjh8/DlcjO/3JL9jNmzdj06ZNuP766zF48GDs3r3bbc+XsvSLu5wv5jZu3IiPP/5YBfziuMs5U5a+cafzpmnTphbv859//nGOc0aWw6Kyke6bN29esY95+umnDU2bNrU4duuttxr69u1rcOd++fvvv9XjLl68aHAn8fHx6n2vWrXK5mOGDx9uGDBggMWxjh07Gu6//36DO/fL7NmzDaGhoQZ3VLlyZcNnn31m9WvueL6UpF/c7XxJSUkx1K9f37B06VJDt27dDOPHj7f5WHc7Z0rTN+5y3kyePNnQsmXLEj/ekc4ZjriWs3Xr1qFXr14Wx/r27auOE9CqVStER0ejd+/e+Pfff+HqkpKS1OewsDCbj3HHc6Yk/SJSU1MRExODmjVrXnG0zRXk5eXhhx9+UCPRcmncGnc8X0rSL+52vsgVDLm6V/hcsMbdzpnS9I07nTcHDx5U5X116tTBqFGjcOLECac4Z7wq/BXdjNRsVq1a1eKY3E9OTkZGRgb8/f3hjiSsfvTRR2jXrh2ysrLw2WefoXv37vjvv/9UPbAr0uv1qlTk2muvRbNmzUp9zrha/W9p+6Vhw4b44osv1KU+CbpvvPEGrrnmGvVLRS4ju5KdO3eqQJaZmYmgoCDMmzcPTZo0gbufL6XpF3c6XyTEb9myRV0OLwl3OmdK2zfuct507NhR1fPK+5UygalTp6JLly7q0r/MPXDkc4bBlTQh/1jkw0T+x3D48GHMnDkT33zzDVz1r375n0JxdUTuqKT9IoHFfHRNzpnGjRururUXX3wRrkT+bUhNvPzi/PnnnzF69GhVF2wrpLmL0vSLu5wvJ0+exPjx41WtuCtOIqrovnGX86Zfv34FtyWkS5CVUeaffvpJ1bE6MgbXchYVFYVz585ZHJP7UvztrqOttnTo0MFlQ924cePw+++/q9UXrvRXu61zRo67c78U5u3tjdatW+PQoUNwNT4+PmoFEtG2bVs1WvTOO++oX57ufL6Upl/c5XyRyWrx8fEWV6qklEL+Tb3//vvqipanp6dbnjNl6Rt3OW8Kq1SpEho0aGDzfTrSOcMa13Imf7ktX77c4pj89VdcXZa7kpEUKSFwJTJXTcKZXNJcsWIFYmNjr/g97nDOlKVfCpNfQHLp2NXOGVvlFPJL1l3Pl7L0i7ucLz179lTvS/7/afqQEiypWZTb1oKZu5wzZekbdzlvrNX1ylVPW+/Toc6ZCp8O5gKzE7du3ao+pPveeustdfv48ePq688++6zhjjvuKHj8kSNHDAEBAYannnrKsHfvXsMHH3xg8PT0NCxevNjgzv0yc+ZMw/z58w0HDx407Ny5U83y9PDwMCxbtszgSh588EE1Q3XlypWGuLi4go/09PSCx0i/SP+Y/PvvvwYvLy/DG2+8oc4Zmf3p7e2t+smd+2Xq1KmGJUuWGA4fPmzYvHmzYcSIEQY/Pz/D7t27Da5E3rOsrnD06FHDjh071H2dTmf466+/3PZ8KUu/uMv5Yk3hmfPues6UpW/c5bx54okn1P9/5d+TnA+9evUyREREqBVeHP2cYXAtJdMyToU/Ro8erb4un+UfRuHvadWqlcHHx8dQp04dtdyGu/fLq6++aqhbt676H0JYWJihe/fuhhUrVhhcjbU+kQ/zc0D6xdRPJj/99JOhQYMG6pyR5dT++OMPg7v3y6OPPmqoVauW6pOqVasa+vfvb9iyZYvB1dx9992GmJgY9T6rVKli6NmzZ0E4c9fzpSz94i7nS0nCmbueM2XpG3c5b2699VZDdHS0ep/Vq1dX9w8dOuQU54xO/lPx47xERERERKXDGlciIiIicgoMrkRERETkFBhciYiIiMgpMLgSERERkVNgcCUiIiIip8DgSkREREROgcGViIiIiJwCgysREREROQUGVyIiN6HT6TB//nytm0FEVGYMrkREFWDMmDEqOBb+uOGGG7RuGhGR0/DSugFERO5CQurs2bMtjvn6+mrWHiIiZ8MRVyKiCiIhNSoqyuKjcuXK6msy+jpr1iz069cP/v7+qFOnDn7++WeL79+5cyeuv/569fXw8HDcd999SE1NtXjMF198gaZNm6rXio6Oxrhx4yy+npCQgKFDhyIgIAD169fHggULKuCdExHZB4MrEZGDmDRpEoYNG4bt27dj1KhRGDFiBPbu3au+lpaWhr59+6qgu3HjRsydOxfLli2zCKYSfMeOHasCrYRcCaX16tWzeI2pU6di+PDh2LFjB/r3769eJzExscLfKxFRWegMBoOhTN9JRESlqnH99ttv4efnZ3F8woQJ6kNGXB944AEVPk06deqENm3a4MMPP8Snn36KZ555BidPnkRgYKD6+qJFizBw4ECcOXMGVatWRfXq1XHXXXfhpZdestoGeY2JEyfixRdfLAjDQUFB+PPPP1lrS0ROgTWuREQVpEePHhbBVISFhRXc7ty5s8XX5P62bdvUbRl5bdmyZUFoFddeey30ej3279+vQqkE2J49exbbhhYtWhTclucKCQlBfHz8Vb83IqKKwOBKRFRBJCgWvnRvL1L3WhLe3t4W9yXwSvglInIGrHElInIQ69evL3K/cePG6rZ8ltpXubxv8u+//8LDwwMNGzZEcHAwateujeXLl1d4u4mIKgpHXImIKkhWVhbOnj1rcczLywsRERHqtky4ateuHa677jp899132LBhAz7//HP1NZlENXnyZIwePRpTpkzB+fPn8fDDD+OOO+5Q9a1CjkudbGRkpFqdICUlRYVbeRwRkStgcCUiqiCLFy9WS1SZk9HSffv2Fcz4/+GHH/DQQw+px33//fdo0qSJ+posX7VkyRKMHz8e7du3V/dlBYK33nqr4Lkk1GZmZmLmzJl48sknVSC++eabK/hdEhGVH64qQETkAKTWdN68eRgyZIjWTSEicliscSUiIiIip8DgSkREREROgTWuREQOgFVbRERXxhFXIiIiInIKDK5ERERE5BQYXImIiIjIKTC4EhEREZFTYHAlIiIiIqfA4EpEREREToHBlYiIiIicAoMrEREREcEZ/B8EdqC6mykK7AAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 4: BLIP",
   "id": "ffb5ade2a676abc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:18:16.189412Z",
     "start_time": "2025-10-21T16:18:16.182408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform_blip = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "])\n",
    "val_transform_blip = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "])\n",
    "\n",
    "def collate_blip(batch):\n",
    "    images, captions, _ = zip(*batch)\n",
    "    return list(images), list(captions)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "ENCODER_DECODER_LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_RATIO = 0.05\n",
    "USE_AMP = True\n",
    "MAX_GEN_LEN = 30\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model.to(DEVICE)\n",
    "blip_model.train()\n",
    "\n",
    "train_ds_blip = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=train_transform_blip, split='train', return_raw_caption=True)\n",
    "val_ds_blip = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=train_ds_blip.vocab, transform=val_transform_blip, split='val', return_raw_caption=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds_blip, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_blip, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds_blip,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_blip, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "optimizer = torch.optim.AdamW(blip_model.parameters(), lr=ENCODER_DECODER_LR, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "def _move_inputs_to_device(inputs, device):\n",
    "    return {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in inputs.items()}\n",
    "\n",
    "def train_epoch_blip(model, processor, loader, optimizer, scheduler, device, scaler, print_every=PRINT_EVERY):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    for batch_idx, (images, captions) in enumerate(loader, 1):\n",
    "        inputs = processor(images=images, text=captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].clone()\n",
    "        pad_token_id = processor.tokenizer.pad_token_id\n",
    "        input_ids[input_ids == pad_token_id] = -100\n",
    "        inputs[\"labels\"] = input_ids\n",
    "\n",
    "        inputs = _move_inputs_to_device(inputs, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "        if batch_idx % print_every == 0 or batch_idx == len(loader):\n",
    "            print(f\"[Train] Batch {batch_idx}/{len(loader)}  loss={running_loss / n:.4f}\")\n",
    "\n",
    "    return running_loss / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_blip(model, processor, loader, device, max_display=5):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    examples = []\n",
    "    for images, captions in loader:\n",
    "        inputs = processor(images=images, text=captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].clone()\n",
    "        pad_token_id = processor.tokenizer.pad_token_id\n",
    "        input_ids[input_ids == pad_token_id] = -100\n",
    "        inputs[\"labels\"] = input_ids\n",
    "\n",
    "        inputs = _move_inputs_to_device(inputs, device)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        if len(examples) < max_display:\n",
    "            gen_inputs = processor(images=images[:3], return_tensors=\"pt\")\n",
    "            gen_inputs = _move_inputs_to_device(gen_inputs, device)\n",
    "            gen_ids = model.generate(**gen_inputs, max_length=MAX_GEN_LEN, num_beams=3, early_stopping=True)\n",
    "            for gid, ref in zip(gen_ids, captions[:len(gen_ids)]):\n",
    "                pred = processor.tokenizer.decode(gid, skip_special_tokens=True)\n",
    "                examples.append((pred, ref))\n",
    "\n",
    "    avg_loss = running_loss / max(1, n)\n",
    "    return avg_loss, examples\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "best_val_loss = float(\"inf\")\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} ===\")\n",
    "    train_loss = train_epoch_blip(blip_model, processor, train_loader, optimizer, scheduler, DEVICE, scaler, print_every=PRINT_EVERY)\n",
    "    val_loss, val_examples = validate_blip(blip_model, processor, val_loader, DEVICE)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} summary: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}\")\n",
    "    print(\"Sample predictions (pred => ref):\")\n",
    "    for p, r in val_examples[:5]:\n",
    "        print(\" P:\", p)\n",
    "        print(\" R:\", r)\n",
    "        print(\"----\")\n",
    "\n",
    "    ckpt = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': blip_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'history': history,\n",
    "    }\n",
    "    torch.save(ckpt, os.path.join(OUTPUT_DIR, f\"blip_epoch_{epoch}.pt\"))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(blip_model.state_dict(), os.path.join(OUTPUT_DIR, \"best_blip.pt\"))\n",
    "\n",
    "print(\"Training finished.\")"
   ],
   "id": "3dfad255719581d9",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:18:16.200925Z",
     "start_time": "2025-10-21T16:18:16.197925Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f65feb11045b3b75",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
