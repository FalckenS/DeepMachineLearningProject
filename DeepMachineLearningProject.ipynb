{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Machine Learning Project (SSY340)\n",
    "\n",
    "Project Group 92"
   ],
   "id": "755ce3c68a828da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment setup:",
   "id": "7ab7239d695adf25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports and CUDA setup:",
   "id": "6ac8efba5d72a9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:29.825017Z",
     "start_time": "2025-10-20T23:46:26.943896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, math, random, torch, gc, ast, re\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from transformers import AutoModel\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Method for clearing cache and GPU memory\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "81b8df0429de0732",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\python312\\lib\\site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
      "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (2.8.0+cu129)\n",
      "Requirement already satisfied: torchvision in c:\\python312\\lib\\site-packages (0.23.0+cu129)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python312\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "PyTorch: 2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA version: 12.9\n",
      "Device name: NVIDIA GeForce RTX 2070\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Constants:",
   "id": "33e675f07eca55ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:31.754059Z",
     "start_time": "2025-10-20T23:46:31.749729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "IMAGES_DIR = \"flickr30k-images\"\n",
    "CSV_PATH = \"flickr_annotations_30k.csv\"\n",
    "CAPTIONS_FILE = \"Flickr30k.token.txt\"\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "MIN_FREQ = 5 # Minimum frequency for vocab, lower value means slower training but bigger vocabulary\n",
    "MAX_LEN = 100\n",
    "NUM_EPOCHS = 5\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1"
   ],
   "id": "6aef934d3f1eddd2",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup Flickr30k.token.txt (captions):",
   "id": "50c62e26d9f6e174"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:33.790887Z",
     "start_time": "2025-10-20T23:46:33.782351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _get_image_name(row, df):\n",
    "    for col in ('file_name','filename','image','img','image_filename','img_name','image_name','img_id','image_id','path'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            return os.path.basename(str(val))\n",
    "    return f\"{row.name}.jpg\"\n",
    "\n",
    "def _get_captions(row, df):\n",
    "    for col in ('raw','captions','sentences','sentence','caption','raw_captions','sentids'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return [str(x).strip() for x in val if str(x).strip()]\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, (list, tuple)):\n",
    "                        return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "                    if isinstance(parsed, dict) and 'raw' in parsed:\n",
    "                        r = parsed['raw']\n",
    "                        if isinstance(r, (list, tuple)):\n",
    "                            return [str(x).strip() for x in r if str(x).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                for sep in ('|||', '||', '\\n'):\n",
    "                    if sep in val:\n",
    "                        return [s.strip() for s in val.split(sep) if s.strip()]\n",
    "                return [val.strip()]\n",
    "    return []\n",
    "\n",
    "def generate_token_file_from_csv(csv_path, captions_file):\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(\"CSV columns:\", list(df.columns))\n",
    "    with open(captions_file, 'w', encoding='utf-8') as fout:\n",
    "        for _, row in df.iterrows():\n",
    "            img_name = _get_image_name(row, df)\n",
    "            caps = _get_captions(row, df)\n",
    "            if not caps:\n",
    "                continue\n",
    "            for i, c in enumerate(caps):\n",
    "                fout.write(f\"{img_name}#{i}\\t{c}\\n\")\n",
    "    print(\"Wrote token file:\", captions_file)\n",
    "\n",
    "# If you already have CAPTIONS_FILE from earlier step, skip this call.\n",
    "if os.path.exists(CSV_PATH) and not os.path.exists(CAPTIONS_FILE):\n",
    "    generate_token_file_from_csv(CSV_PATH, CAPTIONS_FILE)"
   ],
   "id": "8eb6d435fed5dbeb",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup tokenizer/vocab:",
   "id": "ba2fcb2a0232f7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:35.938968Z",
     "start_time": "2025-10-20T23:46:35.932787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq=5, reserved=None):\n",
    "        if reserved is None:\n",
    "            reserved = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        self.min_freq = min_freq\n",
    "        self.reserved = reserved\n",
    "        self.freq = Counter()\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "\n",
    "    def build(self, token_lists):\n",
    "        for t in token_lists:\n",
    "            self.freq.update(t)\n",
    "        self.itos = list(self.reserved)\n",
    "        for tok, cnt in self.freq.most_common():\n",
    "            if cnt < self.min_freq:\n",
    "                continue\n",
    "            if tok in self.reserved:\n",
    "                continue\n",
    "            self.itos.append(tok)\n",
    "        self.stoi = {tok:i for i,tok in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.stoi.get(t, self.stoi['<unk>']) for t in tokens]\n",
    "\n",
    "\n",
    "def tokenize_caption(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ],
   "id": "b9bb3bc066e6ff1",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datasets:",
   "id": "cd6c39a10b09d502"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:39.466089Z",
     "start_time": "2025-10-20T23:46:38.029750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, images_dir, captions_file, vocab=None, transform=None, split='train', seed=SEED):\n",
    "        self.images_dir = str(images_dir)\n",
    "        self.transform = transform\n",
    "        image_to_captions = defaultdict(list)\n",
    "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                img_token, cap = parts\n",
    "                img_name = img_token.split('#')[0]\n",
    "                image_to_captions[img_name].append(cap)\n",
    "\n",
    "        available = set(os.listdir(self.images_dir))\n",
    "        self.entries = []\n",
    "        for img, caps in image_to_captions.items():\n",
    "            if img not in available:\n",
    "                continue\n",
    "            for c in caps:\n",
    "                self.entries.append((img, c))\n",
    "\n",
    "        # Split images at the image level\n",
    "        images = sorted(list({e[0] for e in self.entries}))\n",
    "        random.Random(seed).shuffle(images)\n",
    "        n_train = int(len(images) * TRAIN_RATIO)\n",
    "        n_val = int(len(images) * VAL_RATIO)\n",
    "        train_images = set(images[:n_train])\n",
    "        val_images = set(images[n_train:n_train+n_val])\n",
    "        test_images = set(images[n_train+n_val:])\n",
    "\n",
    "        if split == 'train':\n",
    "            self.entries = [e for e in self.entries if e[0] in train_images]\n",
    "        elif split == 'val':\n",
    "            self.entries = [e for e in self.entries if e[0] in val_images]\n",
    "        elif split == 'test':\n",
    "            self.entries = [e for e in self.entries if e[0] in test_images]\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train', 'val', or 'test'\")\n",
    "\n",
    "        if vocab is None and split=='train':\n",
    "            token_lists = [tokenize_caption(c) for _, c in self.entries]\n",
    "            self.vocab = Vocab(min_freq=MIN_FREQ)\n",
    "            self.vocab.build(token_lists)\n",
    "        elif vocab is not None:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            raise ValueError(\"Provide vocab for val/test split\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, cap = self.entries[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tokens = tokenize_caption(cap)\n",
    "        num_caption = torch.tensor([self.vocab.stoi['<start>']] + self.vocab.numericalize(tokens) + [self.vocab.stoi['<end>']])\n",
    "        return image, num_caption\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, caps = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    lengths = [c.size(0) for c in caps]\n",
    "    caps_padded = nn.utils.rnn.pad_sequence(caps, batch_first=True, padding_value=0)\n",
    "    return images, caps_padded, lengths\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=train_transform, split='train')\n",
    "vocab = train_ds.vocab\n",
    "val_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='val')\n",
    "test_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=val_transform, split='test')"
   ],
   "id": "bbccdb64a818704",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training utilities:",
   "id": "f804f60af8e30afc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:41.519019Z",
     "start_time": "2025-10-20T23:46:41.508370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(enc, dec, loader, criterion, enc_opt, dec_opt, device, print_every=PRINT_EVERY):\n",
    "    enc.train(); dec.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, caps, lengths in loader:\n",
    "        images = images.to(device)\n",
    "        caps = caps.to(device)\n",
    "\n",
    "        feats = enc(images)\n",
    "\n",
    "        # Clip captions to decoder's max length\n",
    "        caps_input = caps[:, :-1]             # (B, L-1)\n",
    "        caps_input = caps_input[:, :dec.max_len]  # clip to max_len\n",
    "        logits = dec(feats, caps_input)\n",
    "\n",
    "        targets = caps[:, 1:]                 # (B, L-1)\n",
    "        targets = targets[:, :dec.max_len]   # same length as logits\n",
    "        logits = logits[:, :targets.size(1), :]  # make shapes match\n",
    "\n",
    "        logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
    "        targets_flat = targets.contiguous().view(-1)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "        if enc_opt: enc_opt.zero_grad()\n",
    "        dec_opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "        if enc_opt: nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "\n",
    "        if enc_opt: enc_opt.step()\n",
    "        dec_opt.step()\n",
    "\n",
    "        # compute token-level accuracy ignoring <pad>\n",
    "        pred_tokens = logits.argmax(dim=2)  # (B, L)\n",
    "        mask = targets != 0  # ignore <pad>\n",
    "        correct = (pred_tokens == targets) & mask\n",
    "        correct_tokens += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(1, n_batches), 100*correct_tokens/total_tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(enc, dec, loader, criterion, device, vocab=None, print_every=PRINT_EVERY):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    n_batches = 0\n",
    "    pad_idx = vocab.stoi['<pad>'] if vocab else 0\n",
    "\n",
    "    for batch_idx, (images, caps, _) in enumerate(loader, 1):\n",
    "        images, caps = images.to(device), caps.to(device)\n",
    "        feats = enc(images)\n",
    "\n",
    "        # Always slice teacher-forcing input\n",
    "        logits = dec(feats, caps[:, :-1])\n",
    "        targets = caps[:, 1:]\n",
    "\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if vocab is not None:\n",
    "            pred_tokens = logits.argmax(dim=2)\n",
    "            mask = targets != pad_idx\n",
    "            correct_tokens += ((pred_tokens == targets) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    avg_acc = 100*correct_tokens/total_tokens if total_tokens > 0 else None\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(enc, dec, img_path, transform, vocab, device, max_len=30):\n",
    "    enc.eval(); dec.eval()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    feats = enc(x)  # (1, E)\n",
    "    start_id = vocab.stoi.get('<start>', None)\n",
    "    # pass start_id for transformer decoder; RNN ignores it\n",
    "    gen = dec.sample(feats, start_id=start_id, max_len=max_len)  # (1, max_len)\n",
    "    gen = gen[0].cpu().tolist()\n",
    "    words=[]\n",
    "    for idx in gen:\n",
    "        if idx < len(vocab.itos):\n",
    "            tok = vocab.itos[idx]\n",
    "        else:\n",
    "            tok = '<unk>'\n",
    "        if tok == '<end>': break\n",
    "        if tok not in ('<pad>','<start>'):\n",
    "            words.append(tok)\n",
    "    return ' '.join(words)"
   ],
   "id": "1033064fef890217",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fit and plot model functions:",
   "id": "846fe48b7a6b84f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:43.576484Z",
     "start_time": "2025-10-20T23:46:43.561768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # --------------- Plot loss: ---------------\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --------------- Plot accuracy: ---------------\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, history['train_acc'], label='Train Accuracy', marker='o')\n",
    "    plt.plot(epochs, history['val_acc'], label='Val Accuracy', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "CEL = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
    "\n",
    "def fit_model(\n",
    "    enc, dec,\n",
    "    train_loader, val_loader,\n",
    "    enc_opt, dec_opt,\n",
    "    device,\n",
    "    vocab,\n",
    "    output_dir,\n",
    "    num_epochs,\n",
    "    criterion=CEL,\n",
    "    print_every=PRINT_EVERY,\n",
    "    train_encoder_only=False\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    enc.to(device)\n",
    "    dec.to(device)\n",
    "\n",
    "    # Freeze decoder\n",
    "    if train_encoder_only:\n",
    "        for p in dec.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    best_val = float('inf')\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    pad_idx = vocab.stoi['<pad>'] if vocab else 0\n",
    "\n",
    "    def compute_accuracy(logits, targets):\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = targets != pad_idx\n",
    "        correct = (preds == targets) & mask\n",
    "        total = mask.sum().item()\n",
    "        return correct.sum().item() / total if total > 0 else 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        enc.train(); dec.train()\n",
    "\n",
    "        # --------------- Train: ---------------\n",
    "\n",
    "        train_loss_accum, train_acc_accum, steps = 0.0, 0.0, 0\n",
    "        for batch_idx, (images, caps, _) in enumerate(train_loader, 1):\n",
    "            images, caps = images.to(device), caps.to(device)\n",
    "            features = enc(images)\n",
    "            logits = dec(features, caps[:, :-1])\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "\n",
    "            if enc_opt: enc_opt.zero_grad()\n",
    "            if not train_encoder_only and dec_opt: dec_opt.zero_grad()  # only if decoder is trainable\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            if enc_opt: torch.nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "            if not train_encoder_only and dec_opt: torch.nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "\n",
    "            if enc_opt: enc_opt.step()\n",
    "            if not train_encoder_only and dec_opt: dec_opt.step()\n",
    "\n",
    "            train_loss_accum += loss.item()\n",
    "            train_acc_accum += compute_accuracy(logits, caps[:, 1:])\n",
    "            steps += 1\n",
    "            if batch_idx % print_every == 0 or batch_idx == len(train_loader):\n",
    "                print(f\"Epoch {epoch} Train batch {batch_idx}/{len(train_loader)} \"\n",
    "                      f\"Loss={train_loss_accum/steps:.4f} Acc={100*train_acc_accum/steps:.2f}%\")\n",
    "\n",
    "        train_loss = train_loss_accum / steps\n",
    "        train_acc = 100 * train_acc_accum / steps\n",
    "\n",
    "        # --------------- Validate: ---------------\n",
    "\n",
    "        enc.eval(); dec.eval()\n",
    "        val_loss_accum, val_acc_accum, steps = 0.0, 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, caps, _) in enumerate(val_loader, 1):\n",
    "                images, caps = images.to(device), caps.to(device)\n",
    "                features = enc(images)\n",
    "                logits = dec(features, caps[:, :-1])\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "\n",
    "                val_loss_accum += loss.item()\n",
    "                pred_tokens = logits.argmax(dim=2)\n",
    "                mask = caps[:, 1:] != pad_idx\n",
    "                correct = (pred_tokens == caps[:, 1:]) & mask\n",
    "                val_acc_accum += correct.sum().item() / mask.sum().item()\n",
    "                steps += 1\n",
    "\n",
    "        val_loss = val_loss_accum / steps\n",
    "        val_acc = 100 * val_acc_accum / steps\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} train_loss={train_loss:.4f} train_acc={train_acc:.2f}% \"\n",
    "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.2f}%\")\n",
    "\n",
    "        # --------------- Save checkpoint: ---------------\n",
    "\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': enc.state_dict(),\n",
    "            'decoder_state_dict': dec.state_dict(),\n",
    "            'vocab': getattr(vocab, 'itos', vocab),\n",
    "            'history': history,\n",
    "            'enc_optimizer_state_dict': enc_opt.state_dict() if enc_opt else None,\n",
    "            'dec_optimizer_state_dict': dec_opt.state_dict() if dec_opt else None,\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(output_dir, f\"ckpt_epoch_{epoch}.pth\"))\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(ckpt, os.path.join(output_dir, \"best.pth\"))\n",
    "\n",
    "    plot_training_history(history)\n",
    "    return history"
   ],
   "id": "35c183f0c3620a27",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decoder base class:",
   "id": "a8a625cdfdeeb08f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:46:46.638573Z",
     "start_time": "2025-10-20T23:46:46.634669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBase(nn.Module):\n",
    "    def forward(self, features, captions):\n",
    "        raise NotImplementedError\n",
    "    def sample(self, features, start_id=None, max_len=30):\n",
    "        raise NotImplementedError"
   ],
   "id": "5c3e4c2346443dac",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1: CNN-RNN",
   "id": "3c2c59171b681b76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Simple CNN->RNN image caption baseline model.\n",
    "\n",
    "Encoder: CNN with transfer learning from ResNet18.\n",
    "\n",
    "Decoder: Text RNN, no transfer learning."
   ],
   "id": "6e0057fff467073f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T00:47:55.882154Z",
     "start_time": "2025-10-20T23:46:51.959067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, fine_tune=False):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1]  # remove fc\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(512, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fine_tune(fine_tune)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)               # (B, 512, 1, 1)\n",
    "        feat = feat.view(feat.size(0), -1)    # (B, 512)\n",
    "        feat = self.fc(feat)                  # (B, embed)\n",
    "        feat = self.relu(feat)\n",
    "        return feat\n",
    "\n",
    "    def fine_tune(self, fine):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = fine\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "class RNNDecoder(DecoderBase):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        h0 = self.init_h(features).unsqueeze(0)\n",
    "        c0 = self.init_c(features).unsqueeze(0)\n",
    "        outputs, _ = self.lstm(embeddings, (h0, c0))\n",
    "        outputs = self.dropout(outputs)\n",
    "        logits = self.linear(outputs)\n",
    "        return logits\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "ENC_LR = 1e-3\n",
    "DEC_LR = 1e-3\n",
    "FINE_TUNE = False\n",
    "\n",
    "DROPOUT = 0.3\n",
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 1024\n",
    "\n",
    "OUTPUT_DIR = \"./models_cnn_rnn\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Instantiate models\n",
    "enc = CNNEncoder(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    fine_tune=FINE_TUNE\n",
    ")\n",
    "dec = RNNDecoder(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Optimizers and loss\n",
    "enc_opt = optim.Adam(enc.parameters(), lr=ENC_LR) if FINE_TUNE else None\n",
    "dec_opt = optim.Adam(dec.parameters(), lr=DEC_LR)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Train\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "4a815cba02749e22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 6989\n",
      "Epoch 1 Train batch 10/1939 Loss=7.0176 Acc=12.15%\n",
      "Epoch 1 Train batch 20/1939 Loss=6.2733 Acc=14.30%\n",
      "Epoch 1 Train batch 30/1939 Loss=5.8936 Acc=16.75%\n",
      "Epoch 1 Train batch 40/1939 Loss=5.6474 Acc=18.44%\n",
      "Epoch 1 Train batch 50/1939 Loss=5.4779 Acc=19.68%\n",
      "Epoch 1 Train batch 60/1939 Loss=5.3441 Acc=20.53%\n",
      "Epoch 1 Train batch 70/1939 Loss=5.2288 Acc=21.35%\n",
      "Epoch 1 Train batch 80/1939 Loss=5.1415 Acc=22.05%\n",
      "Epoch 1 Train batch 90/1939 Loss=5.0624 Acc=22.57%\n",
      "Epoch 1 Train batch 100/1939 Loss=4.9965 Acc=23.03%\n",
      "Epoch 1 Train batch 110/1939 Loss=4.9341 Acc=23.45%\n",
      "Epoch 1 Train batch 120/1939 Loss=4.8803 Acc=23.84%\n",
      "Epoch 1 Train batch 130/1939 Loss=4.8406 Acc=24.10%\n",
      "Epoch 1 Train batch 140/1939 Loss=4.7950 Acc=24.40%\n",
      "Epoch 1 Train batch 150/1939 Loss=4.7592 Acc=24.66%\n",
      "Epoch 1 Train batch 160/1939 Loss=4.7244 Acc=24.89%\n",
      "Epoch 1 Train batch 170/1939 Loss=4.6899 Acc=25.17%\n",
      "Epoch 1 Train batch 180/1939 Loss=4.6646 Acc=25.40%\n",
      "Epoch 1 Train batch 190/1939 Loss=4.6373 Acc=25.61%\n",
      "Epoch 1 Train batch 200/1939 Loss=4.6130 Acc=25.78%\n",
      "Epoch 1 Train batch 210/1939 Loss=4.5903 Acc=25.92%\n",
      "Epoch 1 Train batch 220/1939 Loss=4.5694 Acc=26.07%\n",
      "Epoch 1 Train batch 230/1939 Loss=4.5468 Acc=26.26%\n",
      "Epoch 1 Train batch 240/1939 Loss=4.5257 Acc=26.40%\n",
      "Epoch 1 Train batch 250/1939 Loss=4.5038 Acc=26.56%\n",
      "Epoch 1 Train batch 260/1939 Loss=4.4863 Acc=26.70%\n",
      "Epoch 1 Train batch 270/1939 Loss=4.4673 Acc=26.86%\n",
      "Epoch 1 Train batch 280/1939 Loss=4.4512 Acc=26.99%\n",
      "Epoch 1 Train batch 290/1939 Loss=4.4320 Acc=27.15%\n",
      "Epoch 1 Train batch 300/1939 Loss=4.4149 Acc=27.28%\n",
      "Epoch 1 Train batch 310/1939 Loss=4.4015 Acc=27.37%\n",
      "Epoch 1 Train batch 320/1939 Loss=4.3886 Acc=27.47%\n",
      "Epoch 1 Train batch 330/1939 Loss=4.3744 Acc=27.58%\n",
      "Epoch 1 Train batch 340/1939 Loss=4.3615 Acc=27.65%\n",
      "Epoch 1 Train batch 350/1939 Loss=4.3469 Acc=27.77%\n",
      "Epoch 1 Train batch 360/1939 Loss=4.3332 Acc=27.89%\n",
      "Epoch 1 Train batch 370/1939 Loss=4.3199 Acc=27.99%\n",
      "Epoch 1 Train batch 380/1939 Loss=4.3070 Acc=28.10%\n",
      "Epoch 1 Train batch 390/1939 Loss=4.2954 Acc=28.19%\n",
      "Epoch 1 Train batch 400/1939 Loss=4.2837 Acc=28.28%\n",
      "Epoch 1 Train batch 410/1939 Loss=4.2725 Acc=28.36%\n",
      "Epoch 1 Train batch 420/1939 Loss=4.2609 Acc=28.46%\n",
      "Epoch 1 Train batch 430/1939 Loss=4.2511 Acc=28.53%\n",
      "Epoch 1 Train batch 440/1939 Loss=4.2416 Acc=28.60%\n",
      "Epoch 1 Train batch 450/1939 Loss=4.2318 Acc=28.67%\n",
      "Epoch 1 Train batch 460/1939 Loss=4.2224 Acc=28.74%\n",
      "Epoch 1 Train batch 470/1939 Loss=4.2105 Acc=28.86%\n",
      "Epoch 1 Train batch 480/1939 Loss=4.2007 Acc=28.93%\n",
      "Epoch 1 Train batch 490/1939 Loss=4.1910 Acc=29.00%\n",
      "Epoch 1 Train batch 500/1939 Loss=4.1812 Acc=29.09%\n",
      "Epoch 1 Train batch 510/1939 Loss=4.1713 Acc=29.17%\n",
      "Epoch 1 Train batch 520/1939 Loss=4.1627 Acc=29.24%\n",
      "Epoch 1 Train batch 530/1939 Loss=4.1540 Acc=29.31%\n",
      "Epoch 1 Train batch 540/1939 Loss=4.1472 Acc=29.36%\n",
      "Epoch 1 Train batch 550/1939 Loss=4.1388 Acc=29.41%\n",
      "Epoch 1 Train batch 560/1939 Loss=4.1304 Acc=29.48%\n",
      "Epoch 1 Train batch 570/1939 Loss=4.1234 Acc=29.53%\n",
      "Epoch 1 Train batch 580/1939 Loss=4.1158 Acc=29.58%\n",
      "Epoch 1 Train batch 590/1939 Loss=4.1091 Acc=29.64%\n",
      "Epoch 1 Train batch 600/1939 Loss=4.1022 Acc=29.69%\n",
      "Epoch 1 Train batch 610/1939 Loss=4.0952 Acc=29.74%\n",
      "Epoch 1 Train batch 620/1939 Loss=4.0875 Acc=29.79%\n",
      "Epoch 1 Train batch 630/1939 Loss=4.0806 Acc=29.84%\n",
      "Epoch 1 Train batch 640/1939 Loss=4.0730 Acc=29.90%\n",
      "Epoch 1 Train batch 650/1939 Loss=4.0662 Acc=29.96%\n",
      "Epoch 1 Train batch 660/1939 Loss=4.0604 Acc=30.00%\n",
      "Epoch 1 Train batch 670/1939 Loss=4.0539 Acc=30.05%\n",
      "Epoch 1 Train batch 680/1939 Loss=4.0476 Acc=30.09%\n",
      "Epoch 1 Train batch 690/1939 Loss=4.0413 Acc=30.14%\n",
      "Epoch 1 Train batch 700/1939 Loss=4.0359 Acc=30.19%\n",
      "Epoch 1 Train batch 710/1939 Loss=4.0298 Acc=30.24%\n",
      "Epoch 1 Train batch 720/1939 Loss=4.0234 Acc=30.29%\n",
      "Epoch 1 Train batch 730/1939 Loss=4.0183 Acc=30.34%\n",
      "Epoch 1 Train batch 740/1939 Loss=4.0136 Acc=30.37%\n",
      "Epoch 1 Train batch 750/1939 Loss=4.0085 Acc=30.40%\n",
      "Epoch 1 Train batch 760/1939 Loss=4.0025 Acc=30.44%\n",
      "Epoch 1 Train batch 770/1939 Loss=3.9972 Acc=30.49%\n",
      "Epoch 1 Train batch 780/1939 Loss=3.9927 Acc=30.52%\n",
      "Epoch 1 Train batch 790/1939 Loss=3.9878 Acc=30.56%\n",
      "Epoch 1 Train batch 800/1939 Loss=3.9832 Acc=30.60%\n",
      "Epoch 1 Train batch 810/1939 Loss=3.9786 Acc=30.63%\n",
      "Epoch 1 Train batch 820/1939 Loss=3.9737 Acc=30.68%\n",
      "Epoch 1 Train batch 830/1939 Loss=3.9689 Acc=30.71%\n",
      "Epoch 1 Train batch 840/1939 Loss=3.9634 Acc=30.76%\n",
      "Epoch 1 Train batch 850/1939 Loss=3.9584 Acc=30.80%\n",
      "Epoch 1 Train batch 860/1939 Loss=3.9528 Acc=30.85%\n",
      "Epoch 1 Train batch 870/1939 Loss=3.9483 Acc=30.88%\n",
      "Epoch 1 Train batch 880/1939 Loss=3.9442 Acc=30.91%\n",
      "Epoch 1 Train batch 890/1939 Loss=3.9404 Acc=30.93%\n",
      "Epoch 1 Train batch 900/1939 Loss=3.9358 Acc=30.97%\n",
      "Epoch 1 Train batch 910/1939 Loss=3.9314 Acc=31.01%\n",
      "Epoch 1 Train batch 920/1939 Loss=3.9271 Acc=31.04%\n",
      "Epoch 1 Train batch 930/1939 Loss=3.9223 Acc=31.08%\n",
      "Epoch 1 Train batch 940/1939 Loss=3.9184 Acc=31.11%\n",
      "Epoch 1 Train batch 950/1939 Loss=3.9144 Acc=31.14%\n",
      "Epoch 1 Train batch 960/1939 Loss=3.9097 Acc=31.17%\n",
      "Epoch 1 Train batch 970/1939 Loss=3.9067 Acc=31.20%\n",
      "Epoch 1 Train batch 980/1939 Loss=3.9031 Acc=31.23%\n",
      "Epoch 1 Train batch 990/1939 Loss=3.8993 Acc=31.25%\n",
      "Epoch 1 Train batch 1000/1939 Loss=3.8951 Acc=31.28%\n",
      "Epoch 1 Train batch 1010/1939 Loss=3.8914 Acc=31.31%\n",
      "Epoch 1 Train batch 1020/1939 Loss=3.8876 Acc=31.33%\n",
      "Epoch 1 Train batch 1030/1939 Loss=3.8837 Acc=31.36%\n",
      "Epoch 1 Train batch 1040/1939 Loss=3.8802 Acc=31.39%\n",
      "Epoch 1 Train batch 1050/1939 Loss=3.8759 Acc=31.43%\n",
      "Epoch 1 Train batch 1060/1939 Loss=3.8726 Acc=31.45%\n",
      "Epoch 1 Train batch 1070/1939 Loss=3.8686 Acc=31.48%\n",
      "Epoch 1 Train batch 1080/1939 Loss=3.8650 Acc=31.51%\n",
      "Epoch 1 Train batch 1090/1939 Loss=3.8612 Acc=31.54%\n",
      "Epoch 1 Train batch 1100/1939 Loss=3.8575 Acc=31.57%\n",
      "Epoch 1 Train batch 1110/1939 Loss=3.8540 Acc=31.60%\n",
      "Epoch 1 Train batch 1120/1939 Loss=3.8502 Acc=31.63%\n",
      "Epoch 1 Train batch 1130/1939 Loss=3.8468 Acc=31.66%\n",
      "Epoch 1 Train batch 1140/1939 Loss=3.8438 Acc=31.68%\n",
      "Epoch 1 Train batch 1150/1939 Loss=3.8400 Acc=31.71%\n",
      "Epoch 1 Train batch 1160/1939 Loss=3.8365 Acc=31.75%\n",
      "Epoch 1 Train batch 1170/1939 Loss=3.8332 Acc=31.77%\n",
      "Epoch 1 Train batch 1180/1939 Loss=3.8299 Acc=31.80%\n",
      "Epoch 1 Train batch 1190/1939 Loss=3.8271 Acc=31.81%\n",
      "Epoch 1 Train batch 1200/1939 Loss=3.8237 Acc=31.84%\n",
      "Epoch 1 Train batch 1210/1939 Loss=3.8210 Acc=31.86%\n",
      "Epoch 1 Train batch 1220/1939 Loss=3.8175 Acc=31.90%\n",
      "Epoch 1 Train batch 1230/1939 Loss=3.8146 Acc=31.92%\n",
      "Epoch 1 Train batch 1240/1939 Loss=3.8117 Acc=31.94%\n",
      "Epoch 1 Train batch 1250/1939 Loss=3.8094 Acc=31.96%\n",
      "Epoch 1 Train batch 1260/1939 Loss=3.8069 Acc=31.98%\n",
      "Epoch 1 Train batch 1270/1939 Loss=3.8035 Acc=32.00%\n",
      "Epoch 1 Train batch 1280/1939 Loss=3.8006 Acc=32.03%\n",
      "Epoch 1 Train batch 1290/1939 Loss=3.7977 Acc=32.05%\n",
      "Epoch 1 Train batch 1300/1939 Loss=3.7951 Acc=32.07%\n",
      "Epoch 1 Train batch 1310/1939 Loss=3.7917 Acc=32.10%\n",
      "Epoch 1 Train batch 1320/1939 Loss=3.7891 Acc=32.12%\n",
      "Epoch 1 Train batch 1330/1939 Loss=3.7857 Acc=32.15%\n",
      "Epoch 1 Train batch 1340/1939 Loss=3.7826 Acc=32.17%\n",
      "Epoch 1 Train batch 1350/1939 Loss=3.7798 Acc=32.19%\n",
      "Epoch 1 Train batch 1360/1939 Loss=3.7774 Acc=32.21%\n",
      "Epoch 1 Train batch 1370/1939 Loss=3.7745 Acc=32.23%\n",
      "Epoch 1 Train batch 1380/1939 Loss=3.7715 Acc=32.26%\n",
      "Epoch 1 Train batch 1390/1939 Loss=3.7689 Acc=32.28%\n",
      "Epoch 1 Train batch 1400/1939 Loss=3.7659 Acc=32.31%\n",
      "Epoch 1 Train batch 1410/1939 Loss=3.7635 Acc=32.33%\n",
      "Epoch 1 Train batch 1420/1939 Loss=3.7604 Acc=32.35%\n",
      "Epoch 1 Train batch 1430/1939 Loss=3.7573 Acc=32.38%\n",
      "Epoch 1 Train batch 1440/1939 Loss=3.7546 Acc=32.40%\n",
      "Epoch 1 Train batch 1450/1939 Loss=3.7521 Acc=32.42%\n",
      "Epoch 1 Train batch 1460/1939 Loss=3.7501 Acc=32.43%\n",
      "Epoch 1 Train batch 1470/1939 Loss=3.7477 Acc=32.45%\n",
      "Epoch 1 Train batch 1480/1939 Loss=3.7452 Acc=32.47%\n",
      "Epoch 1 Train batch 1490/1939 Loss=3.7429 Acc=32.49%\n",
      "Epoch 1 Train batch 1500/1939 Loss=3.7406 Acc=32.51%\n",
      "Epoch 1 Train batch 1510/1939 Loss=3.7381 Acc=32.53%\n",
      "Epoch 1 Train batch 1520/1939 Loss=3.7356 Acc=32.55%\n",
      "Epoch 1 Train batch 1530/1939 Loss=3.7333 Acc=32.57%\n",
      "Epoch 1 Train batch 1540/1939 Loss=3.7310 Acc=32.58%\n",
      "Epoch 1 Train batch 1550/1939 Loss=3.7288 Acc=32.60%\n",
      "Epoch 1 Train batch 1560/1939 Loss=3.7261 Acc=32.62%\n",
      "Epoch 1 Train batch 1570/1939 Loss=3.7236 Acc=32.64%\n",
      "Epoch 1 Train batch 1580/1939 Loss=3.7215 Acc=32.66%\n",
      "Epoch 1 Train batch 1590/1939 Loss=3.7191 Acc=32.68%\n",
      "Epoch 1 Train batch 1600/1939 Loss=3.7172 Acc=32.69%\n",
      "Epoch 1 Train batch 1610/1939 Loss=3.7148 Acc=32.71%\n",
      "Epoch 1 Train batch 1620/1939 Loss=3.7125 Acc=32.73%\n",
      "Epoch 1 Train batch 1630/1939 Loss=3.7102 Acc=32.75%\n",
      "Epoch 1 Train batch 1640/1939 Loss=3.7079 Acc=32.76%\n",
      "Epoch 1 Train batch 1650/1939 Loss=3.7058 Acc=32.78%\n",
      "Epoch 1 Train batch 1660/1939 Loss=3.7035 Acc=32.80%\n",
      "Epoch 1 Train batch 1670/1939 Loss=3.7013 Acc=32.82%\n",
      "Epoch 1 Train batch 1680/1939 Loss=3.6992 Acc=32.84%\n",
      "Epoch 1 Train batch 1690/1939 Loss=3.6965 Acc=32.86%\n",
      "Epoch 1 Train batch 1700/1939 Loss=3.6946 Acc=32.88%\n",
      "Epoch 1 Train batch 1710/1939 Loss=3.6925 Acc=32.90%\n",
      "Epoch 1 Train batch 1720/1939 Loss=3.6904 Acc=32.91%\n",
      "Epoch 1 Train batch 1730/1939 Loss=3.6885 Acc=32.93%\n",
      "Epoch 1 Train batch 1740/1939 Loss=3.6865 Acc=32.95%\n",
      "Epoch 1 Train batch 1750/1939 Loss=3.6842 Acc=32.96%\n",
      "Epoch 1 Train batch 1760/1939 Loss=3.6827 Acc=32.98%\n",
      "Epoch 1 Train batch 1770/1939 Loss=3.6808 Acc=32.99%\n",
      "Epoch 1 Train batch 1780/1939 Loss=3.6788 Acc=33.01%\n",
      "Epoch 1 Train batch 1790/1939 Loss=3.6768 Acc=33.02%\n",
      "Epoch 1 Train batch 1800/1939 Loss=3.6749 Acc=33.04%\n",
      "Epoch 1 Train batch 1810/1939 Loss=3.6727 Acc=33.05%\n",
      "Epoch 1 Train batch 1820/1939 Loss=3.6712 Acc=33.07%\n",
      "Epoch 1 Train batch 1830/1939 Loss=3.6693 Acc=33.09%\n",
      "Epoch 1 Train batch 1840/1939 Loss=3.6674 Acc=33.10%\n",
      "Epoch 1 Train batch 1850/1939 Loss=3.6652 Acc=33.12%\n",
      "Epoch 1 Train batch 1860/1939 Loss=3.6634 Acc=33.13%\n",
      "Epoch 1 Train batch 1870/1939 Loss=3.6619 Acc=33.15%\n",
      "Epoch 1 Train batch 1880/1939 Loss=3.6600 Acc=33.16%\n",
      "Epoch 1 Train batch 1890/1939 Loss=3.6581 Acc=33.18%\n",
      "Epoch 1 Train batch 1900/1939 Loss=3.6563 Acc=33.19%\n",
      "Epoch 1 Train batch 1910/1939 Loss=3.6548 Acc=33.20%\n",
      "Epoch 1 Train batch 1920/1939 Loss=3.6530 Acc=33.22%\n",
      "Epoch 1 Train batch 1930/1939 Loss=3.6510 Acc=33.23%\n",
      "Epoch 1 Train batch 1939/1939 Loss=3.6498 Acc=33.24%\n",
      "Epoch 1/5 train_loss=3.6498 train_acc=33.24% val_loss=3.1970 val_acc=37.32%\n",
      "Epoch 2 Train batch 10/1939 Loss=3.1188 Acc=36.98%\n",
      "Epoch 2 Train batch 20/1939 Loss=3.1676 Acc=36.57%\n",
      "Epoch 2 Train batch 30/1939 Loss=3.1362 Acc=36.90%\n",
      "Epoch 2 Train batch 40/1939 Loss=3.1588 Acc=36.71%\n",
      "Epoch 2 Train batch 50/1939 Loss=3.1594 Acc=36.74%\n",
      "Epoch 2 Train batch 60/1939 Loss=3.1614 Acc=36.71%\n",
      "Epoch 2 Train batch 70/1939 Loss=3.1568 Acc=36.84%\n",
      "Epoch 2 Train batch 80/1939 Loss=3.1626 Acc=36.72%\n",
      "Epoch 2 Train batch 90/1939 Loss=3.1636 Acc=36.80%\n",
      "Epoch 2 Train batch 100/1939 Loss=3.1655 Acc=36.74%\n",
      "Epoch 2 Train batch 110/1939 Loss=3.1680 Acc=36.77%\n",
      "Epoch 2 Train batch 120/1939 Loss=3.1663 Acc=36.79%\n",
      "Epoch 2 Train batch 130/1939 Loss=3.1673 Acc=36.76%\n",
      "Epoch 2 Train batch 140/1939 Loss=3.1626 Acc=36.83%\n",
      "Epoch 2 Train batch 150/1939 Loss=3.1656 Acc=36.82%\n",
      "Epoch 2 Train batch 160/1939 Loss=3.1674 Acc=36.78%\n",
      "Epoch 2 Train batch 170/1939 Loss=3.1671 Acc=36.77%\n",
      "Epoch 2 Train batch 180/1939 Loss=3.1658 Acc=36.77%\n",
      "Epoch 2 Train batch 190/1939 Loss=3.1642 Acc=36.82%\n",
      "Epoch 2 Train batch 200/1939 Loss=3.1648 Acc=36.83%\n",
      "Epoch 2 Train batch 210/1939 Loss=3.1688 Acc=36.76%\n",
      "Epoch 2 Train batch 220/1939 Loss=3.1731 Acc=36.76%\n",
      "Epoch 2 Train batch 230/1939 Loss=3.1712 Acc=36.78%\n",
      "Epoch 2 Train batch 240/1939 Loss=3.1715 Acc=36.76%\n",
      "Epoch 2 Train batch 250/1939 Loss=3.1710 Acc=36.75%\n",
      "Epoch 2 Train batch 260/1939 Loss=3.1726 Acc=36.71%\n",
      "Epoch 2 Train batch 270/1939 Loss=3.1715 Acc=36.72%\n",
      "Epoch 2 Train batch 280/1939 Loss=3.1727 Acc=36.71%\n",
      "Epoch 2 Train batch 290/1939 Loss=3.1707 Acc=36.70%\n",
      "Epoch 2 Train batch 300/1939 Loss=3.1698 Acc=36.72%\n",
      "Epoch 2 Train batch 310/1939 Loss=3.1695 Acc=36.74%\n",
      "Epoch 2 Train batch 320/1939 Loss=3.1681 Acc=36.76%\n",
      "Epoch 2 Train batch 330/1939 Loss=3.1690 Acc=36.73%\n",
      "Epoch 2 Train batch 340/1939 Loss=3.1677 Acc=36.73%\n",
      "Epoch 2 Train batch 350/1939 Loss=3.1684 Acc=36.70%\n",
      "Epoch 2 Train batch 360/1939 Loss=3.1691 Acc=36.70%\n",
      "Epoch 2 Train batch 370/1939 Loss=3.1698 Acc=36.68%\n",
      "Epoch 2 Train batch 380/1939 Loss=3.1696 Acc=36.67%\n",
      "Epoch 2 Train batch 390/1939 Loss=3.1690 Acc=36.68%\n",
      "Epoch 2 Train batch 400/1939 Loss=3.1700 Acc=36.67%\n",
      "Epoch 2 Train batch 410/1939 Loss=3.1699 Acc=36.68%\n",
      "Epoch 2 Train batch 420/1939 Loss=3.1688 Acc=36.69%\n",
      "Epoch 2 Train batch 430/1939 Loss=3.1676 Acc=36.70%\n",
      "Epoch 2 Train batch 440/1939 Loss=3.1678 Acc=36.70%\n",
      "Epoch 2 Train batch 450/1939 Loss=3.1663 Acc=36.74%\n",
      "Epoch 2 Train batch 460/1939 Loss=3.1662 Acc=36.75%\n",
      "Epoch 2 Train batch 470/1939 Loss=3.1670 Acc=36.73%\n",
      "Epoch 2 Train batch 480/1939 Loss=3.1661 Acc=36.76%\n",
      "Epoch 2 Train batch 490/1939 Loss=3.1659 Acc=36.76%\n",
      "Epoch 2 Train batch 500/1939 Loss=3.1635 Acc=36.77%\n",
      "Epoch 2 Train batch 510/1939 Loss=3.1630 Acc=36.78%\n",
      "Epoch 2 Train batch 520/1939 Loss=3.1624 Acc=36.77%\n",
      "Epoch 2 Train batch 530/1939 Loss=3.1622 Acc=36.78%\n",
      "Epoch 2 Train batch 540/1939 Loss=3.1628 Acc=36.78%\n",
      "Epoch 2 Train batch 550/1939 Loss=3.1623 Acc=36.78%\n",
      "Epoch 2 Train batch 560/1939 Loss=3.1619 Acc=36.79%\n",
      "Epoch 2 Train batch 570/1939 Loss=3.1619 Acc=36.79%\n",
      "Epoch 2 Train batch 580/1939 Loss=3.1615 Acc=36.79%\n",
      "Epoch 2 Train batch 590/1939 Loss=3.1613 Acc=36.79%\n",
      "Epoch 2 Train batch 600/1939 Loss=3.1609 Acc=36.80%\n",
      "Epoch 2 Train batch 610/1939 Loss=3.1607 Acc=36.80%\n",
      "Epoch 2 Train batch 620/1939 Loss=3.1600 Acc=36.80%\n",
      "Epoch 2 Train batch 630/1939 Loss=3.1608 Acc=36.79%\n",
      "Epoch 2 Train batch 640/1939 Loss=3.1598 Acc=36.80%\n",
      "Epoch 2 Train batch 650/1939 Loss=3.1596 Acc=36.79%\n",
      "Epoch 2 Train batch 660/1939 Loss=3.1597 Acc=36.79%\n",
      "Epoch 2 Train batch 670/1939 Loss=3.1599 Acc=36.79%\n",
      "Epoch 2 Train batch 680/1939 Loss=3.1595 Acc=36.80%\n",
      "Epoch 2 Train batch 690/1939 Loss=3.1592 Acc=36.81%\n",
      "Epoch 2 Train batch 700/1939 Loss=3.1582 Acc=36.81%\n",
      "Epoch 2 Train batch 710/1939 Loss=3.1581 Acc=36.82%\n",
      "Epoch 2 Train batch 720/1939 Loss=3.1573 Acc=36.84%\n",
      "Epoch 2 Train batch 730/1939 Loss=3.1576 Acc=36.84%\n",
      "Epoch 2 Train batch 740/1939 Loss=3.1578 Acc=36.84%\n",
      "Epoch 2 Train batch 750/1939 Loss=3.1580 Acc=36.84%\n",
      "Epoch 2 Train batch 760/1939 Loss=3.1580 Acc=36.85%\n",
      "Epoch 2 Train batch 770/1939 Loss=3.1571 Acc=36.86%\n",
      "Epoch 2 Train batch 780/1939 Loss=3.1568 Acc=36.87%\n",
      "Epoch 2 Train batch 790/1939 Loss=3.1570 Acc=36.87%\n",
      "Epoch 2 Train batch 800/1939 Loss=3.1572 Acc=36.86%\n",
      "Epoch 2 Train batch 810/1939 Loss=3.1578 Acc=36.85%\n",
      "Epoch 2 Train batch 820/1939 Loss=3.1583 Acc=36.85%\n",
      "Epoch 2 Train batch 830/1939 Loss=3.1581 Acc=36.85%\n",
      "Epoch 2 Train batch 840/1939 Loss=3.1583 Acc=36.84%\n",
      "Epoch 2 Train batch 850/1939 Loss=3.1582 Acc=36.83%\n",
      "Epoch 2 Train batch 860/1939 Loss=3.1576 Acc=36.83%\n",
      "Epoch 2 Train batch 870/1939 Loss=3.1578 Acc=36.83%\n",
      "Epoch 2 Train batch 880/1939 Loss=3.1576 Acc=36.84%\n",
      "Epoch 2 Train batch 890/1939 Loss=3.1573 Acc=36.85%\n",
      "Epoch 2 Train batch 900/1939 Loss=3.1574 Acc=36.84%\n",
      "Epoch 2 Train batch 910/1939 Loss=3.1569 Acc=36.85%\n",
      "Epoch 2 Train batch 920/1939 Loss=3.1566 Acc=36.86%\n",
      "Epoch 2 Train batch 930/1939 Loss=3.1567 Acc=36.86%\n",
      "Epoch 2 Train batch 940/1939 Loss=3.1572 Acc=36.86%\n",
      "Epoch 2 Train batch 950/1939 Loss=3.1573 Acc=36.86%\n",
      "Epoch 2 Train batch 960/1939 Loss=3.1577 Acc=36.85%\n",
      "Epoch 2 Train batch 970/1939 Loss=3.1580 Acc=36.84%\n",
      "Epoch 2 Train batch 980/1939 Loss=3.1573 Acc=36.85%\n",
      "Epoch 2 Train batch 990/1939 Loss=3.1572 Acc=36.86%\n",
      "Epoch 2 Train batch 1000/1939 Loss=3.1573 Acc=36.86%\n",
      "Epoch 2 Train batch 1010/1939 Loss=3.1569 Acc=36.86%\n",
      "Epoch 2 Train batch 1020/1939 Loss=3.1569 Acc=36.87%\n",
      "Epoch 2 Train batch 1030/1939 Loss=3.1569 Acc=36.87%\n",
      "Epoch 2 Train batch 1040/1939 Loss=3.1568 Acc=36.86%\n",
      "Epoch 2 Train batch 1050/1939 Loss=3.1564 Acc=36.86%\n",
      "Epoch 2 Train batch 1060/1939 Loss=3.1555 Acc=36.88%\n",
      "Epoch 2 Train batch 1070/1939 Loss=3.1551 Acc=36.89%\n",
      "Epoch 2 Train batch 1080/1939 Loss=3.1547 Acc=36.89%\n",
      "Epoch 2 Train batch 1090/1939 Loss=3.1550 Acc=36.89%\n",
      "Epoch 2 Train batch 1100/1939 Loss=3.1546 Acc=36.90%\n",
      "Epoch 2 Train batch 1110/1939 Loss=3.1547 Acc=36.91%\n",
      "Epoch 2 Train batch 1120/1939 Loss=3.1542 Acc=36.91%\n",
      "Epoch 2 Train batch 1130/1939 Loss=3.1537 Acc=36.92%\n",
      "Epoch 2 Train batch 1140/1939 Loss=3.1535 Acc=36.92%\n",
      "Epoch 2 Train batch 1150/1939 Loss=3.1531 Acc=36.92%\n",
      "Epoch 2 Train batch 1160/1939 Loss=3.1531 Acc=36.93%\n",
      "Epoch 2 Train batch 1170/1939 Loss=3.1529 Acc=36.93%\n",
      "Epoch 2 Train batch 1180/1939 Loss=3.1525 Acc=36.93%\n",
      "Epoch 2 Train batch 1190/1939 Loss=3.1526 Acc=36.93%\n",
      "Epoch 2 Train batch 1200/1939 Loss=3.1522 Acc=36.94%\n",
      "Epoch 2 Train batch 1210/1939 Loss=3.1527 Acc=36.93%\n",
      "Epoch 2 Train batch 1220/1939 Loss=3.1523 Acc=36.94%\n",
      "Epoch 2 Train batch 1230/1939 Loss=3.1523 Acc=36.94%\n",
      "Epoch 2 Train batch 1240/1939 Loss=3.1519 Acc=36.94%\n",
      "Epoch 2 Train batch 1250/1939 Loss=3.1518 Acc=36.94%\n",
      "Epoch 2 Train batch 1260/1939 Loss=3.1517 Acc=36.94%\n",
      "Epoch 2 Train batch 1270/1939 Loss=3.1520 Acc=36.93%\n",
      "Epoch 2 Train batch 1280/1939 Loss=3.1515 Acc=36.94%\n",
      "Epoch 2 Train batch 1290/1939 Loss=3.1515 Acc=36.94%\n",
      "Epoch 2 Train batch 1300/1939 Loss=3.1512 Acc=36.94%\n",
      "Epoch 2 Train batch 1310/1939 Loss=3.1512 Acc=36.94%\n",
      "Epoch 2 Train batch 1320/1939 Loss=3.1507 Acc=36.95%\n",
      "Epoch 2 Train batch 1330/1939 Loss=3.1508 Acc=36.94%\n",
      "Epoch 2 Train batch 1340/1939 Loss=3.1504 Acc=36.94%\n",
      "Epoch 2 Train batch 1350/1939 Loss=3.1500 Acc=36.95%\n",
      "Epoch 2 Train batch 1360/1939 Loss=3.1495 Acc=36.95%\n",
      "Epoch 2 Train batch 1370/1939 Loss=3.1492 Acc=36.95%\n",
      "Epoch 2 Train batch 1380/1939 Loss=3.1488 Acc=36.96%\n",
      "Epoch 2 Train batch 1390/1939 Loss=3.1487 Acc=36.97%\n",
      "Epoch 2 Train batch 1400/1939 Loss=3.1482 Acc=36.97%\n",
      "Epoch 2 Train batch 1410/1939 Loss=3.1476 Acc=36.98%\n",
      "Epoch 2 Train batch 1420/1939 Loss=3.1473 Acc=36.98%\n",
      "Epoch 2 Train batch 1430/1939 Loss=3.1472 Acc=36.98%\n",
      "Epoch 2 Train batch 1440/1939 Loss=3.1468 Acc=36.99%\n",
      "Epoch 2 Train batch 1450/1939 Loss=3.1467 Acc=37.00%\n",
      "Epoch 2 Train batch 1460/1939 Loss=3.1465 Acc=37.00%\n",
      "Epoch 2 Train batch 1470/1939 Loss=3.1462 Acc=37.00%\n",
      "Epoch 2 Train batch 1480/1939 Loss=3.1463 Acc=37.00%\n",
      "Epoch 2 Train batch 1490/1939 Loss=3.1461 Acc=37.00%\n",
      "Epoch 2 Train batch 1500/1939 Loss=3.1461 Acc=37.01%\n",
      "Epoch 2 Train batch 1510/1939 Loss=3.1456 Acc=37.01%\n",
      "Epoch 2 Train batch 1520/1939 Loss=3.1451 Acc=37.02%\n",
      "Epoch 2 Train batch 1530/1939 Loss=3.1445 Acc=37.03%\n",
      "Epoch 2 Train batch 1540/1939 Loss=3.1443 Acc=37.03%\n",
      "Epoch 2 Train batch 1550/1939 Loss=3.1446 Acc=37.03%\n",
      "Epoch 2 Train batch 1560/1939 Loss=3.1440 Acc=37.04%\n",
      "Epoch 2 Train batch 1570/1939 Loss=3.1442 Acc=37.04%\n",
      "Epoch 2 Train batch 1580/1939 Loss=3.1438 Acc=37.04%\n",
      "Epoch 2 Train batch 1590/1939 Loss=3.1434 Acc=37.04%\n",
      "Epoch 2 Train batch 1600/1939 Loss=3.1438 Acc=37.04%\n",
      "Epoch 2 Train batch 1610/1939 Loss=3.1438 Acc=37.05%\n",
      "Epoch 2 Train batch 1620/1939 Loss=3.1439 Acc=37.05%\n",
      "Epoch 2 Train batch 1630/1939 Loss=3.1436 Acc=37.06%\n",
      "Epoch 2 Train batch 1640/1939 Loss=3.1432 Acc=37.06%\n",
      "Epoch 2 Train batch 1650/1939 Loss=3.1425 Acc=37.07%\n",
      "Epoch 2 Train batch 1660/1939 Loss=3.1422 Acc=37.07%\n",
      "Epoch 2 Train batch 1670/1939 Loss=3.1421 Acc=37.08%\n",
      "Epoch 2 Train batch 1680/1939 Loss=3.1420 Acc=37.08%\n",
      "Epoch 2 Train batch 1690/1939 Loss=3.1422 Acc=37.08%\n",
      "Epoch 2 Train batch 1700/1939 Loss=3.1422 Acc=37.08%\n",
      "Epoch 2 Train batch 1710/1939 Loss=3.1418 Acc=37.08%\n",
      "Epoch 2 Train batch 1720/1939 Loss=3.1422 Acc=37.08%\n",
      "Epoch 2 Train batch 1730/1939 Loss=3.1423 Acc=37.08%\n",
      "Epoch 2 Train batch 1740/1939 Loss=3.1419 Acc=37.08%\n",
      "Epoch 2 Train batch 1750/1939 Loss=3.1418 Acc=37.09%\n",
      "Epoch 2 Train batch 1760/1939 Loss=3.1415 Acc=37.09%\n",
      "Epoch 2 Train batch 1770/1939 Loss=3.1410 Acc=37.10%\n",
      "Epoch 2 Train batch 1780/1939 Loss=3.1409 Acc=37.10%\n",
      "Epoch 2 Train batch 1790/1939 Loss=3.1410 Acc=37.10%\n",
      "Epoch 2 Train batch 1800/1939 Loss=3.1405 Acc=37.10%\n",
      "Epoch 2 Train batch 1810/1939 Loss=3.1405 Acc=37.10%\n",
      "Epoch 2 Train batch 1820/1939 Loss=3.1407 Acc=37.10%\n",
      "Epoch 2 Train batch 1830/1939 Loss=3.1403 Acc=37.11%\n",
      "Epoch 2 Train batch 1840/1939 Loss=3.1401 Acc=37.11%\n",
      "Epoch 2 Train batch 1850/1939 Loss=3.1397 Acc=37.11%\n",
      "Epoch 2 Train batch 1860/1939 Loss=3.1398 Acc=37.11%\n",
      "Epoch 2 Train batch 1870/1939 Loss=3.1399 Acc=37.11%\n",
      "Epoch 2 Train batch 1880/1939 Loss=3.1399 Acc=37.11%\n",
      "Epoch 2 Train batch 1890/1939 Loss=3.1394 Acc=37.11%\n",
      "Epoch 2 Train batch 1900/1939 Loss=3.1389 Acc=37.12%\n",
      "Epoch 2 Train batch 1910/1939 Loss=3.1387 Acc=37.13%\n",
      "Epoch 2 Train batch 1920/1939 Loss=3.1390 Acc=37.12%\n",
      "Epoch 2 Train batch 1930/1939 Loss=3.1388 Acc=37.13%\n",
      "Epoch 2 Train batch 1939/1939 Loss=3.1388 Acc=37.13%\n",
      "Epoch 2/5 train_loss=3.1388 train_acc=37.13% val_loss=3.0622 val_acc=38.67%\n",
      "Epoch 3 Train batch 10/1939 Loss=2.9609 Acc=38.27%\n",
      "Epoch 3 Train batch 20/1939 Loss=2.9278 Acc=38.74%\n",
      "Epoch 3 Train batch 30/1939 Loss=2.9314 Acc=38.79%\n",
      "Epoch 3 Train batch 40/1939 Loss=2.9294 Acc=38.70%\n",
      "Epoch 3 Train batch 50/1939 Loss=2.9253 Acc=38.79%\n",
      "Epoch 3 Train batch 60/1939 Loss=2.9328 Acc=38.71%\n",
      "Epoch 3 Train batch 70/1939 Loss=2.9337 Acc=38.68%\n",
      "Epoch 3 Train batch 80/1939 Loss=2.9316 Acc=38.62%\n",
      "Epoch 3 Train batch 90/1939 Loss=2.9344 Acc=38.55%\n",
      "Epoch 3 Train batch 100/1939 Loss=2.9400 Acc=38.43%\n",
      "Epoch 3 Train batch 110/1939 Loss=2.9440 Acc=38.37%\n",
      "Epoch 3 Train batch 120/1939 Loss=2.9450 Acc=38.33%\n",
      "Epoch 3 Train batch 130/1939 Loss=2.9442 Acc=38.31%\n",
      "Epoch 3 Train batch 140/1939 Loss=2.9431 Acc=38.32%\n",
      "Epoch 3 Train batch 150/1939 Loss=2.9468 Acc=38.25%\n",
      "Epoch 3 Train batch 160/1939 Loss=2.9431 Acc=38.32%\n",
      "Epoch 3 Train batch 170/1939 Loss=2.9419 Acc=38.33%\n",
      "Epoch 3 Train batch 180/1939 Loss=2.9412 Acc=38.36%\n",
      "Epoch 3 Train batch 190/1939 Loss=2.9401 Acc=38.37%\n",
      "Epoch 3 Train batch 200/1939 Loss=2.9399 Acc=38.37%\n",
      "Epoch 3 Train batch 210/1939 Loss=2.9416 Acc=38.36%\n",
      "Epoch 3 Train batch 220/1939 Loss=2.9418 Acc=38.37%\n",
      "Epoch 3 Train batch 230/1939 Loss=2.9436 Acc=38.35%\n",
      "Epoch 3 Train batch 240/1939 Loss=2.9429 Acc=38.36%\n",
      "Epoch 3 Train batch 250/1939 Loss=2.9433 Acc=38.36%\n",
      "Epoch 3 Train batch 260/1939 Loss=2.9459 Acc=38.34%\n",
      "Epoch 3 Train batch 270/1939 Loss=2.9457 Acc=38.35%\n",
      "Epoch 3 Train batch 280/1939 Loss=2.9453 Acc=38.39%\n",
      "Epoch 3 Train batch 290/1939 Loss=2.9455 Acc=38.38%\n",
      "Epoch 3 Train batch 300/1939 Loss=2.9447 Acc=38.40%\n",
      "Epoch 3 Train batch 310/1939 Loss=2.9440 Acc=38.41%\n",
      "Epoch 3 Train batch 320/1939 Loss=2.9462 Acc=38.40%\n",
      "Epoch 3 Train batch 330/1939 Loss=2.9466 Acc=38.41%\n",
      "Epoch 3 Train batch 340/1939 Loss=2.9466 Acc=38.42%\n",
      "Epoch 3 Train batch 350/1939 Loss=2.9477 Acc=38.40%\n",
      "Epoch 3 Train batch 360/1939 Loss=2.9475 Acc=38.41%\n",
      "Epoch 3 Train batch 370/1939 Loss=2.9483 Acc=38.40%\n",
      "Epoch 3 Train batch 380/1939 Loss=2.9479 Acc=38.43%\n",
      "Epoch 3 Train batch 390/1939 Loss=2.9491 Acc=38.42%\n",
      "Epoch 3 Train batch 400/1939 Loss=2.9476 Acc=38.44%\n",
      "Epoch 3 Train batch 410/1939 Loss=2.9478 Acc=38.43%\n",
      "Epoch 3 Train batch 420/1939 Loss=2.9476 Acc=38.45%\n",
      "Epoch 3 Train batch 430/1939 Loss=2.9483 Acc=38.44%\n",
      "Epoch 3 Train batch 440/1939 Loss=2.9477 Acc=38.44%\n",
      "Epoch 3 Train batch 450/1939 Loss=2.9479 Acc=38.44%\n",
      "Epoch 3 Train batch 460/1939 Loss=2.9482 Acc=38.43%\n",
      "Epoch 3 Train batch 470/1939 Loss=2.9483 Acc=38.43%\n",
      "Epoch 3 Train batch 480/1939 Loss=2.9496 Acc=38.41%\n",
      "Epoch 3 Train batch 490/1939 Loss=2.9490 Acc=38.41%\n",
      "Epoch 3 Train batch 500/1939 Loss=2.9495 Acc=38.42%\n",
      "Epoch 3 Train batch 510/1939 Loss=2.9495 Acc=38.43%\n",
      "Epoch 3 Train batch 520/1939 Loss=2.9496 Acc=38.43%\n",
      "Epoch 3 Train batch 530/1939 Loss=2.9501 Acc=38.41%\n",
      "Epoch 3 Train batch 540/1939 Loss=2.9502 Acc=38.41%\n",
      "Epoch 3 Train batch 550/1939 Loss=2.9508 Acc=38.42%\n",
      "Epoch 3 Train batch 560/1939 Loss=2.9508 Acc=38.41%\n",
      "Epoch 3 Train batch 570/1939 Loss=2.9508 Acc=38.41%\n",
      "Epoch 3 Train batch 580/1939 Loss=2.9509 Acc=38.41%\n",
      "Epoch 3 Train batch 590/1939 Loss=2.9510 Acc=38.41%\n",
      "Epoch 3 Train batch 600/1939 Loss=2.9519 Acc=38.41%\n",
      "Epoch 3 Train batch 610/1939 Loss=2.9522 Acc=38.42%\n",
      "Epoch 3 Train batch 620/1939 Loss=2.9526 Acc=38.42%\n",
      "Epoch 3 Train batch 630/1939 Loss=2.9534 Acc=38.40%\n",
      "Epoch 3 Train batch 640/1939 Loss=2.9542 Acc=38.39%\n",
      "Epoch 3 Train batch 650/1939 Loss=2.9541 Acc=38.39%\n",
      "Epoch 3 Train batch 660/1939 Loss=2.9545 Acc=38.39%\n",
      "Epoch 3 Train batch 670/1939 Loss=2.9553 Acc=38.39%\n",
      "Epoch 3 Train batch 680/1939 Loss=2.9561 Acc=38.38%\n",
      "Epoch 3 Train batch 690/1939 Loss=2.9565 Acc=38.37%\n",
      "Epoch 3 Train batch 700/1939 Loss=2.9575 Acc=38.37%\n",
      "Epoch 3 Train batch 710/1939 Loss=2.9575 Acc=38.37%\n",
      "Epoch 3 Train batch 720/1939 Loss=2.9568 Acc=38.38%\n",
      "Epoch 3 Train batch 730/1939 Loss=2.9565 Acc=38.38%\n",
      "Epoch 3 Train batch 740/1939 Loss=2.9565 Acc=38.38%\n",
      "Epoch 3 Train batch 750/1939 Loss=2.9567 Acc=38.38%\n",
      "Epoch 3 Train batch 760/1939 Loss=2.9574 Acc=38.38%\n",
      "Epoch 3 Train batch 770/1939 Loss=2.9579 Acc=38.38%\n",
      "Epoch 3 Train batch 780/1939 Loss=2.9577 Acc=38.39%\n",
      "Epoch 3 Train batch 790/1939 Loss=2.9583 Acc=38.38%\n",
      "Epoch 3 Train batch 800/1939 Loss=2.9573 Acc=38.40%\n",
      "Epoch 3 Train batch 810/1939 Loss=2.9583 Acc=38.39%\n",
      "Epoch 3 Train batch 820/1939 Loss=2.9587 Acc=38.38%\n",
      "Epoch 3 Train batch 830/1939 Loss=2.9584 Acc=38.39%\n",
      "Epoch 3 Train batch 840/1939 Loss=2.9585 Acc=38.39%\n",
      "Epoch 3 Train batch 850/1939 Loss=2.9590 Acc=38.39%\n",
      "Epoch 3 Train batch 860/1939 Loss=2.9587 Acc=38.39%\n",
      "Epoch 3 Train batch 870/1939 Loss=2.9588 Acc=38.40%\n",
      "Epoch 3 Train batch 880/1939 Loss=2.9586 Acc=38.41%\n",
      "Epoch 3 Train batch 890/1939 Loss=2.9586 Acc=38.41%\n",
      "Epoch 3 Train batch 900/1939 Loss=2.9590 Acc=38.41%\n",
      "Epoch 3 Train batch 910/1939 Loss=2.9591 Acc=38.41%\n",
      "Epoch 3 Train batch 920/1939 Loss=2.9592 Acc=38.42%\n",
      "Epoch 3 Train batch 930/1939 Loss=2.9594 Acc=38.41%\n",
      "Epoch 3 Train batch 940/1939 Loss=2.9595 Acc=38.41%\n",
      "Epoch 3 Train batch 950/1939 Loss=2.9596 Acc=38.41%\n",
      "Epoch 3 Train batch 960/1939 Loss=2.9592 Acc=38.42%\n",
      "Epoch 3 Train batch 970/1939 Loss=2.9588 Acc=38.42%\n",
      "Epoch 3 Train batch 980/1939 Loss=2.9589 Acc=38.42%\n",
      "Epoch 3 Train batch 990/1939 Loss=2.9588 Acc=38.42%\n",
      "Epoch 3 Train batch 1000/1939 Loss=2.9585 Acc=38.43%\n",
      "Epoch 3 Train batch 1010/1939 Loss=2.9587 Acc=38.44%\n",
      "Epoch 3 Train batch 1020/1939 Loss=2.9585 Acc=38.45%\n",
      "Epoch 3 Train batch 1030/1939 Loss=2.9587 Acc=38.45%\n",
      "Epoch 3 Train batch 1040/1939 Loss=2.9590 Acc=38.45%\n",
      "Epoch 3 Train batch 1050/1939 Loss=2.9587 Acc=38.46%\n",
      "Epoch 3 Train batch 1060/1939 Loss=2.9588 Acc=38.45%\n",
      "Epoch 3 Train batch 1070/1939 Loss=2.9590 Acc=38.45%\n",
      "Epoch 3 Train batch 1080/1939 Loss=2.9584 Acc=38.46%\n",
      "Epoch 3 Train batch 1090/1939 Loss=2.9585 Acc=38.47%\n",
      "Epoch 3 Train batch 1100/1939 Loss=2.9589 Acc=38.46%\n",
      "Epoch 3 Train batch 1110/1939 Loss=2.9594 Acc=38.47%\n",
      "Epoch 3 Train batch 1120/1939 Loss=2.9597 Acc=38.46%\n",
      "Epoch 3 Train batch 1130/1939 Loss=2.9598 Acc=38.46%\n",
      "Epoch 3 Train batch 1140/1939 Loss=2.9599 Acc=38.46%\n",
      "Epoch 3 Train batch 1150/1939 Loss=2.9604 Acc=38.45%\n",
      "Epoch 3 Train batch 1160/1939 Loss=2.9603 Acc=38.45%\n",
      "Epoch 3 Train batch 1170/1939 Loss=2.9599 Acc=38.46%\n",
      "Epoch 3 Train batch 1180/1939 Loss=2.9595 Acc=38.46%\n",
      "Epoch 3 Train batch 1190/1939 Loss=2.9599 Acc=38.46%\n",
      "Epoch 3 Train batch 1200/1939 Loss=2.9605 Acc=38.46%\n",
      "Epoch 3 Train batch 1210/1939 Loss=2.9608 Acc=38.46%\n",
      "Epoch 3 Train batch 1220/1939 Loss=2.9611 Acc=38.46%\n",
      "Epoch 3 Train batch 1230/1939 Loss=2.9610 Acc=38.46%\n",
      "Epoch 3 Train batch 1240/1939 Loss=2.9617 Acc=38.45%\n",
      "Epoch 3 Train batch 1250/1939 Loss=2.9616 Acc=38.46%\n",
      "Epoch 3 Train batch 1260/1939 Loss=2.9618 Acc=38.46%\n",
      "Epoch 3 Train batch 1270/1939 Loss=2.9619 Acc=38.46%\n",
      "Epoch 3 Train batch 1280/1939 Loss=2.9615 Acc=38.46%\n",
      "Epoch 3 Train batch 1290/1939 Loss=2.9618 Acc=38.46%\n",
      "Epoch 3 Train batch 1300/1939 Loss=2.9616 Acc=38.47%\n",
      "Epoch 3 Train batch 1310/1939 Loss=2.9617 Acc=38.47%\n",
      "Epoch 3 Train batch 1320/1939 Loss=2.9618 Acc=38.47%\n",
      "Epoch 3 Train batch 1330/1939 Loss=2.9618 Acc=38.47%\n",
      "Epoch 3 Train batch 1340/1939 Loss=2.9622 Acc=38.47%\n",
      "Epoch 3 Train batch 1350/1939 Loss=2.9624 Acc=38.47%\n",
      "Epoch 3 Train batch 1360/1939 Loss=2.9627 Acc=38.47%\n",
      "Epoch 3 Train batch 1370/1939 Loss=2.9629 Acc=38.46%\n",
      "Epoch 3 Train batch 1380/1939 Loss=2.9629 Acc=38.46%\n",
      "Epoch 3 Train batch 1390/1939 Loss=2.9629 Acc=38.47%\n",
      "Epoch 3 Train batch 1400/1939 Loss=2.9632 Acc=38.47%\n",
      "Epoch 3 Train batch 1410/1939 Loss=2.9631 Acc=38.47%\n",
      "Epoch 3 Train batch 1420/1939 Loss=2.9631 Acc=38.48%\n",
      "Epoch 3 Train batch 1430/1939 Loss=2.9633 Acc=38.48%\n",
      "Epoch 3 Train batch 1440/1939 Loss=2.9635 Acc=38.48%\n",
      "Epoch 3 Train batch 1450/1939 Loss=2.9640 Acc=38.47%\n",
      "Epoch 3 Train batch 1460/1939 Loss=2.9646 Acc=38.47%\n",
      "Epoch 3 Train batch 1470/1939 Loss=2.9648 Acc=38.46%\n",
      "Epoch 3 Train batch 1480/1939 Loss=2.9649 Acc=38.47%\n",
      "Epoch 3 Train batch 1490/1939 Loss=2.9649 Acc=38.47%\n",
      "Epoch 3 Train batch 1500/1939 Loss=2.9650 Acc=38.47%\n",
      "Epoch 3 Train batch 1510/1939 Loss=2.9648 Acc=38.47%\n",
      "Epoch 3 Train batch 1520/1939 Loss=2.9647 Acc=38.47%\n",
      "Epoch 3 Train batch 1530/1939 Loss=2.9645 Acc=38.48%\n",
      "Epoch 3 Train batch 1540/1939 Loss=2.9645 Acc=38.47%\n",
      "Epoch 3 Train batch 1550/1939 Loss=2.9646 Acc=38.48%\n",
      "Epoch 3 Train batch 1560/1939 Loss=2.9643 Acc=38.48%\n",
      "Epoch 3 Train batch 1570/1939 Loss=2.9643 Acc=38.48%\n",
      "Epoch 3 Train batch 1580/1939 Loss=2.9643 Acc=38.49%\n",
      "Epoch 3 Train batch 1590/1939 Loss=2.9646 Acc=38.48%\n",
      "Epoch 3 Train batch 1600/1939 Loss=2.9648 Acc=38.48%\n",
      "Epoch 3 Train batch 1610/1939 Loss=2.9647 Acc=38.49%\n",
      "Epoch 3 Train batch 1620/1939 Loss=2.9650 Acc=38.49%\n",
      "Epoch 3 Train batch 1630/1939 Loss=2.9650 Acc=38.48%\n",
      "Epoch 3 Train batch 1640/1939 Loss=2.9650 Acc=38.49%\n",
      "Epoch 3 Train batch 1650/1939 Loss=2.9649 Acc=38.49%\n",
      "Epoch 3 Train batch 1660/1939 Loss=2.9651 Acc=38.49%\n",
      "Epoch 3 Train batch 1670/1939 Loss=2.9651 Acc=38.49%\n",
      "Epoch 3 Train batch 1680/1939 Loss=2.9655 Acc=38.48%\n",
      "Epoch 3 Train batch 1690/1939 Loss=2.9654 Acc=38.49%\n",
      "Epoch 3 Train batch 1700/1939 Loss=2.9654 Acc=38.49%\n",
      "Epoch 3 Train batch 1710/1939 Loss=2.9654 Acc=38.50%\n",
      "Epoch 3 Train batch 1720/1939 Loss=2.9656 Acc=38.49%\n",
      "Epoch 3 Train batch 1730/1939 Loss=2.9660 Acc=38.49%\n",
      "Epoch 3 Train batch 1740/1939 Loss=2.9661 Acc=38.49%\n",
      "Epoch 3 Train batch 1750/1939 Loss=2.9663 Acc=38.49%\n",
      "Epoch 3 Train batch 1760/1939 Loss=2.9661 Acc=38.49%\n",
      "Epoch 3 Train batch 1770/1939 Loss=2.9665 Acc=38.49%\n",
      "Epoch 3 Train batch 1780/1939 Loss=2.9664 Acc=38.49%\n",
      "Epoch 3 Train batch 1790/1939 Loss=2.9667 Acc=38.49%\n",
      "Epoch 3 Train batch 1800/1939 Loss=2.9666 Acc=38.49%\n",
      "Epoch 3 Train batch 1810/1939 Loss=2.9668 Acc=38.48%\n",
      "Epoch 3 Train batch 1820/1939 Loss=2.9669 Acc=38.48%\n",
      "Epoch 3 Train batch 1830/1939 Loss=2.9670 Acc=38.48%\n",
      "Epoch 3 Train batch 1840/1939 Loss=2.9670 Acc=38.49%\n",
      "Epoch 3 Train batch 1850/1939 Loss=2.9671 Acc=38.48%\n",
      "Epoch 3 Train batch 1860/1939 Loss=2.9670 Acc=38.49%\n",
      "Epoch 3 Train batch 1870/1939 Loss=2.9671 Acc=38.49%\n",
      "Epoch 3 Train batch 1880/1939 Loss=2.9673 Acc=38.48%\n",
      "Epoch 3 Train batch 1890/1939 Loss=2.9674 Acc=38.48%\n",
      "Epoch 3 Train batch 1900/1939 Loss=2.9676 Acc=38.49%\n",
      "Epoch 3 Train batch 1910/1939 Loss=2.9678 Acc=38.48%\n",
      "Epoch 3 Train batch 1920/1939 Loss=2.9681 Acc=38.48%\n",
      "Epoch 3 Train batch 1930/1939 Loss=2.9679 Acc=38.48%\n",
      "Epoch 3 Train batch 1939/1939 Loss=2.9679 Acc=38.49%\n",
      "Epoch 3/5 train_loss=2.9679 train_acc=38.49% val_loss=3.0245 val_acc=39.14%\n",
      "Epoch 4 Train batch 10/1939 Loss=2.7778 Acc=39.79%\n",
      "Epoch 4 Train batch 20/1939 Loss=2.7921 Acc=39.62%\n",
      "Epoch 4 Train batch 30/1939 Loss=2.8010 Acc=39.63%\n",
      "Epoch 4 Train batch 40/1939 Loss=2.7979 Acc=39.89%\n",
      "Epoch 4 Train batch 50/1939 Loss=2.7904 Acc=39.80%\n",
      "Epoch 4 Train batch 60/1939 Loss=2.7889 Acc=39.80%\n",
      "Epoch 4 Train batch 70/1939 Loss=2.7932 Acc=39.85%\n",
      "Epoch 4 Train batch 80/1939 Loss=2.7963 Acc=39.80%\n",
      "Epoch 4 Train batch 90/1939 Loss=2.7938 Acc=39.87%\n",
      "Epoch 4 Train batch 100/1939 Loss=2.7919 Acc=39.86%\n",
      "Epoch 4 Train batch 110/1939 Loss=2.7944 Acc=39.84%\n",
      "Epoch 4 Train batch 120/1939 Loss=2.7950 Acc=39.78%\n",
      "Epoch 4 Train batch 130/1939 Loss=2.7966 Acc=39.77%\n",
      "Epoch 4 Train batch 140/1939 Loss=2.8011 Acc=39.70%\n",
      "Epoch 4 Train batch 150/1939 Loss=2.7999 Acc=39.73%\n",
      "Epoch 4 Train batch 160/1939 Loss=2.8018 Acc=39.70%\n",
      "Epoch 4 Train batch 170/1939 Loss=2.8026 Acc=39.71%\n",
      "Epoch 4 Train batch 180/1939 Loss=2.8053 Acc=39.64%\n",
      "Epoch 4 Train batch 190/1939 Loss=2.8075 Acc=39.62%\n",
      "Epoch 4 Train batch 200/1939 Loss=2.8093 Acc=39.59%\n",
      "Epoch 4 Train batch 210/1939 Loss=2.8105 Acc=39.60%\n",
      "Epoch 4 Train batch 220/1939 Loss=2.8125 Acc=39.59%\n",
      "Epoch 4 Train batch 230/1939 Loss=2.8145 Acc=39.58%\n",
      "Epoch 4 Train batch 240/1939 Loss=2.8140 Acc=39.57%\n",
      "Epoch 4 Train batch 250/1939 Loss=2.8146 Acc=39.55%\n",
      "Epoch 4 Train batch 260/1939 Loss=2.8136 Acc=39.56%\n",
      "Epoch 4 Train batch 270/1939 Loss=2.8135 Acc=39.57%\n",
      "Epoch 4 Train batch 280/1939 Loss=2.8151 Acc=39.54%\n",
      "Epoch 4 Train batch 290/1939 Loss=2.8145 Acc=39.56%\n",
      "Epoch 4 Train batch 300/1939 Loss=2.8154 Acc=39.55%\n",
      "Epoch 4 Train batch 310/1939 Loss=2.8159 Acc=39.58%\n",
      "Epoch 4 Train batch 320/1939 Loss=2.8168 Acc=39.56%\n",
      "Epoch 4 Train batch 330/1939 Loss=2.8177 Acc=39.55%\n",
      "Epoch 4 Train batch 340/1939 Loss=2.8162 Acc=39.57%\n",
      "Epoch 4 Train batch 350/1939 Loss=2.8181 Acc=39.54%\n",
      "Epoch 4 Train batch 360/1939 Loss=2.8184 Acc=39.54%\n",
      "Epoch 4 Train batch 370/1939 Loss=2.8186 Acc=39.52%\n",
      "Epoch 4 Train batch 380/1939 Loss=2.8178 Acc=39.53%\n",
      "Epoch 4 Train batch 390/1939 Loss=2.8178 Acc=39.54%\n",
      "Epoch 4 Train batch 400/1939 Loss=2.8173 Acc=39.55%\n",
      "Epoch 4 Train batch 410/1939 Loss=2.8175 Acc=39.55%\n",
      "Epoch 4 Train batch 420/1939 Loss=2.8187 Acc=39.55%\n",
      "Epoch 4 Train batch 430/1939 Loss=2.8191 Acc=39.56%\n",
      "Epoch 4 Train batch 440/1939 Loss=2.8196 Acc=39.57%\n",
      "Epoch 4 Train batch 450/1939 Loss=2.8199 Acc=39.57%\n",
      "Epoch 4 Train batch 460/1939 Loss=2.8195 Acc=39.56%\n",
      "Epoch 4 Train batch 470/1939 Loss=2.8200 Acc=39.55%\n",
      "Epoch 4 Train batch 480/1939 Loss=2.8199 Acc=39.55%\n",
      "Epoch 4 Train batch 490/1939 Loss=2.8199 Acc=39.56%\n",
      "Epoch 4 Train batch 500/1939 Loss=2.8206 Acc=39.57%\n",
      "Epoch 4 Train batch 510/1939 Loss=2.8203 Acc=39.58%\n",
      "Epoch 4 Train batch 520/1939 Loss=2.8215 Acc=39.57%\n",
      "Epoch 4 Train batch 530/1939 Loss=2.8220 Acc=39.56%\n",
      "Epoch 4 Train batch 540/1939 Loss=2.8222 Acc=39.56%\n",
      "Epoch 4 Train batch 550/1939 Loss=2.8225 Acc=39.55%\n",
      "Epoch 4 Train batch 560/1939 Loss=2.8221 Acc=39.57%\n",
      "Epoch 4 Train batch 570/1939 Loss=2.8223 Acc=39.57%\n",
      "Epoch 4 Train batch 580/1939 Loss=2.8217 Acc=39.57%\n",
      "Epoch 4 Train batch 590/1939 Loss=2.8222 Acc=39.57%\n",
      "Epoch 4 Train batch 600/1939 Loss=2.8222 Acc=39.57%\n",
      "Epoch 4 Train batch 610/1939 Loss=2.8220 Acc=39.59%\n",
      "Epoch 4 Train batch 620/1939 Loss=2.8216 Acc=39.59%\n",
      "Epoch 4 Train batch 630/1939 Loss=2.8217 Acc=39.59%\n",
      "Epoch 4 Train batch 640/1939 Loss=2.8219 Acc=39.58%\n",
      "Epoch 4 Train batch 650/1939 Loss=2.8235 Acc=39.57%\n",
      "Epoch 4 Train batch 660/1939 Loss=2.8241 Acc=39.57%\n",
      "Epoch 4 Train batch 670/1939 Loss=2.8254 Acc=39.55%\n",
      "Epoch 4 Train batch 680/1939 Loss=2.8262 Acc=39.55%\n",
      "Epoch 4 Train batch 690/1939 Loss=2.8271 Acc=39.54%\n",
      "Epoch 4 Train batch 700/1939 Loss=2.8280 Acc=39.54%\n",
      "Epoch 4 Train batch 710/1939 Loss=2.8291 Acc=39.52%\n",
      "Epoch 4 Train batch 720/1939 Loss=2.8297 Acc=39.52%\n",
      "Epoch 4 Train batch 730/1939 Loss=2.8300 Acc=39.52%\n",
      "Epoch 4 Train batch 740/1939 Loss=2.8306 Acc=39.51%\n",
      "Epoch 4 Train batch 750/1939 Loss=2.8306 Acc=39.51%\n",
      "Epoch 4 Train batch 760/1939 Loss=2.8314 Acc=39.50%\n",
      "Epoch 4 Train batch 770/1939 Loss=2.8322 Acc=39.49%\n",
      "Epoch 4 Train batch 780/1939 Loss=2.8330 Acc=39.48%\n",
      "Epoch 4 Train batch 790/1939 Loss=2.8330 Acc=39.48%\n",
      "Epoch 4 Train batch 800/1939 Loss=2.8327 Acc=39.49%\n",
      "Epoch 4 Train batch 810/1939 Loss=2.8329 Acc=39.50%\n",
      "Epoch 4 Train batch 820/1939 Loss=2.8331 Acc=39.50%\n",
      "Epoch 4 Train batch 830/1939 Loss=2.8330 Acc=39.49%\n",
      "Epoch 4 Train batch 840/1939 Loss=2.8341 Acc=39.48%\n",
      "Epoch 4 Train batch 850/1939 Loss=2.8340 Acc=39.48%\n",
      "Epoch 4 Train batch 860/1939 Loss=2.8344 Acc=39.48%\n",
      "Epoch 4 Train batch 870/1939 Loss=2.8350 Acc=39.48%\n",
      "Epoch 4 Train batch 880/1939 Loss=2.8358 Acc=39.47%\n",
      "Epoch 4 Train batch 890/1939 Loss=2.8358 Acc=39.47%\n",
      "Epoch 4 Train batch 900/1939 Loss=2.8366 Acc=39.46%\n",
      "Epoch 4 Train batch 910/1939 Loss=2.8366 Acc=39.45%\n",
      "Epoch 4 Train batch 920/1939 Loss=2.8374 Acc=39.44%\n",
      "Epoch 4 Train batch 930/1939 Loss=2.8375 Acc=39.44%\n",
      "Epoch 4 Train batch 940/1939 Loss=2.8378 Acc=39.44%\n",
      "Epoch 4 Train batch 950/1939 Loss=2.8385 Acc=39.43%\n",
      "Epoch 4 Train batch 960/1939 Loss=2.8390 Acc=39.43%\n",
      "Epoch 4 Train batch 970/1939 Loss=2.8395 Acc=39.43%\n",
      "Epoch 4 Train batch 980/1939 Loss=2.8403 Acc=39.42%\n",
      "Epoch 4 Train batch 990/1939 Loss=2.8402 Acc=39.43%\n",
      "Epoch 4 Train batch 1000/1939 Loss=2.8400 Acc=39.42%\n",
      "Epoch 4 Train batch 1010/1939 Loss=2.8399 Acc=39.42%\n",
      "Epoch 4 Train batch 1020/1939 Loss=2.8400 Acc=39.43%\n",
      "Epoch 4 Train batch 1030/1939 Loss=2.8402 Acc=39.42%\n",
      "Epoch 4 Train batch 1040/1939 Loss=2.8411 Acc=39.41%\n",
      "Epoch 4 Train batch 1050/1939 Loss=2.8418 Acc=39.41%\n",
      "Epoch 4 Train batch 1060/1939 Loss=2.8419 Acc=39.41%\n",
      "Epoch 4 Train batch 1070/1939 Loss=2.8426 Acc=39.40%\n",
      "Epoch 4 Train batch 1080/1939 Loss=2.8426 Acc=39.39%\n",
      "Epoch 4 Train batch 1090/1939 Loss=2.8429 Acc=39.39%\n",
      "Epoch 4 Train batch 1100/1939 Loss=2.8432 Acc=39.39%\n",
      "Epoch 4 Train batch 1110/1939 Loss=2.8436 Acc=39.39%\n",
      "Epoch 4 Train batch 1120/1939 Loss=2.8440 Acc=39.38%\n",
      "Epoch 4 Train batch 1130/1939 Loss=2.8437 Acc=39.38%\n",
      "Epoch 4 Train batch 1140/1939 Loss=2.8435 Acc=39.39%\n",
      "Epoch 4 Train batch 1150/1939 Loss=2.8439 Acc=39.38%\n",
      "Epoch 4 Train batch 1160/1939 Loss=2.8440 Acc=39.38%\n",
      "Epoch 4 Train batch 1170/1939 Loss=2.8446 Acc=39.38%\n",
      "Epoch 4 Train batch 1180/1939 Loss=2.8444 Acc=39.38%\n",
      "Epoch 4 Train batch 1190/1939 Loss=2.8448 Acc=39.38%\n",
      "Epoch 4 Train batch 1200/1939 Loss=2.8450 Acc=39.38%\n",
      "Epoch 4 Train batch 1210/1939 Loss=2.8452 Acc=39.38%\n",
      "Epoch 4 Train batch 1220/1939 Loss=2.8450 Acc=39.38%\n",
      "Epoch 4 Train batch 1230/1939 Loss=2.8453 Acc=39.38%\n",
      "Epoch 4 Train batch 1240/1939 Loss=2.8454 Acc=39.38%\n",
      "Epoch 4 Train batch 1250/1939 Loss=2.8454 Acc=39.39%\n",
      "Epoch 4 Train batch 1260/1939 Loss=2.8455 Acc=39.38%\n",
      "Epoch 4 Train batch 1270/1939 Loss=2.8459 Acc=39.38%\n",
      "Epoch 4 Train batch 1280/1939 Loss=2.8458 Acc=39.38%\n",
      "Epoch 4 Train batch 1290/1939 Loss=2.8462 Acc=39.38%\n",
      "Epoch 4 Train batch 1300/1939 Loss=2.8464 Acc=39.38%\n",
      "Epoch 4 Train batch 1310/1939 Loss=2.8469 Acc=39.37%\n",
      "Epoch 4 Train batch 1320/1939 Loss=2.8469 Acc=39.38%\n",
      "Epoch 4 Train batch 1330/1939 Loss=2.8471 Acc=39.38%\n",
      "Epoch 4 Train batch 1340/1939 Loss=2.8477 Acc=39.37%\n",
      "Epoch 4 Train batch 1350/1939 Loss=2.8474 Acc=39.38%\n",
      "Epoch 4 Train batch 1360/1939 Loss=2.8481 Acc=39.37%\n",
      "Epoch 4 Train batch 1370/1939 Loss=2.8483 Acc=39.37%\n",
      "Epoch 4 Train batch 1380/1939 Loss=2.8488 Acc=39.36%\n",
      "Epoch 4 Train batch 1390/1939 Loss=2.8488 Acc=39.36%\n",
      "Epoch 4 Train batch 1400/1939 Loss=2.8491 Acc=39.36%\n",
      "Epoch 4 Train batch 1410/1939 Loss=2.8492 Acc=39.36%\n",
      "Epoch 4 Train batch 1420/1939 Loss=2.8493 Acc=39.36%\n",
      "Epoch 4 Train batch 1430/1939 Loss=2.8497 Acc=39.35%\n",
      "Epoch 4 Train batch 1440/1939 Loss=2.8503 Acc=39.34%\n",
      "Epoch 4 Train batch 1450/1939 Loss=2.8504 Acc=39.35%\n",
      "Epoch 4 Train batch 1460/1939 Loss=2.8502 Acc=39.35%\n",
      "Epoch 4 Train batch 1470/1939 Loss=2.8500 Acc=39.36%\n",
      "Epoch 4 Train batch 1480/1939 Loss=2.8500 Acc=39.36%\n",
      "Epoch 4 Train batch 1490/1939 Loss=2.8505 Acc=39.35%\n",
      "Epoch 4 Train batch 1500/1939 Loss=2.8509 Acc=39.35%\n",
      "Epoch 4 Train batch 1510/1939 Loss=2.8510 Acc=39.35%\n",
      "Epoch 4 Train batch 1520/1939 Loss=2.8515 Acc=39.35%\n",
      "Epoch 4 Train batch 1530/1939 Loss=2.8516 Acc=39.35%\n",
      "Epoch 4 Train batch 1540/1939 Loss=2.8519 Acc=39.35%\n",
      "Epoch 4 Train batch 1550/1939 Loss=2.8523 Acc=39.34%\n",
      "Epoch 4 Train batch 1560/1939 Loss=2.8523 Acc=39.35%\n",
      "Epoch 4 Train batch 1570/1939 Loss=2.8521 Acc=39.35%\n",
      "Epoch 4 Train batch 1580/1939 Loss=2.8526 Acc=39.34%\n",
      "Epoch 4 Train batch 1590/1939 Loss=2.8527 Acc=39.34%\n",
      "Epoch 4 Train batch 1600/1939 Loss=2.8528 Acc=39.34%\n",
      "Epoch 4 Train batch 1610/1939 Loss=2.8531 Acc=39.34%\n",
      "Epoch 4 Train batch 1620/1939 Loss=2.8530 Acc=39.34%\n",
      "Epoch 4 Train batch 1630/1939 Loss=2.8534 Acc=39.33%\n",
      "Epoch 4 Train batch 1640/1939 Loss=2.8536 Acc=39.33%\n",
      "Epoch 4 Train batch 1650/1939 Loss=2.8535 Acc=39.33%\n",
      "Epoch 4 Train batch 1660/1939 Loss=2.8537 Acc=39.33%\n",
      "Epoch 4 Train batch 1670/1939 Loss=2.8538 Acc=39.33%\n",
      "Epoch 4 Train batch 1680/1939 Loss=2.8539 Acc=39.33%\n",
      "Epoch 4 Train batch 1690/1939 Loss=2.8543 Acc=39.32%\n",
      "Epoch 4 Train batch 1700/1939 Loss=2.8543 Acc=39.33%\n",
      "Epoch 4 Train batch 1710/1939 Loss=2.8545 Acc=39.33%\n",
      "Epoch 4 Train batch 1720/1939 Loss=2.8548 Acc=39.32%\n",
      "Epoch 4 Train batch 1730/1939 Loss=2.8548 Acc=39.33%\n",
      "Epoch 4 Train batch 1740/1939 Loss=2.8552 Acc=39.33%\n",
      "Epoch 4 Train batch 1750/1939 Loss=2.8551 Acc=39.33%\n",
      "Epoch 4 Train batch 1760/1939 Loss=2.8552 Acc=39.33%\n",
      "Epoch 4 Train batch 1770/1939 Loss=2.8554 Acc=39.33%\n",
      "Epoch 4 Train batch 1780/1939 Loss=2.8555 Acc=39.33%\n",
      "Epoch 4 Train batch 1790/1939 Loss=2.8556 Acc=39.33%\n",
      "Epoch 4 Train batch 1800/1939 Loss=2.8560 Acc=39.32%\n",
      "Epoch 4 Train batch 1810/1939 Loss=2.8562 Acc=39.32%\n",
      "Epoch 4 Train batch 1820/1939 Loss=2.8561 Acc=39.32%\n",
      "Epoch 4 Train batch 1830/1939 Loss=2.8563 Acc=39.32%\n",
      "Epoch 4 Train batch 1840/1939 Loss=2.8563 Acc=39.33%\n",
      "Epoch 4 Train batch 1850/1939 Loss=2.8566 Acc=39.32%\n",
      "Epoch 4 Train batch 1860/1939 Loss=2.8568 Acc=39.32%\n",
      "Epoch 4 Train batch 1870/1939 Loss=2.8568 Acc=39.33%\n",
      "Epoch 4 Train batch 1880/1939 Loss=2.8572 Acc=39.32%\n",
      "Epoch 4 Train batch 1890/1939 Loss=2.8575 Acc=39.32%\n",
      "Epoch 4 Train batch 1900/1939 Loss=2.8576 Acc=39.32%\n",
      "Epoch 4 Train batch 1910/1939 Loss=2.8579 Acc=39.32%\n",
      "Epoch 4 Train batch 1920/1939 Loss=2.8579 Acc=39.33%\n",
      "Epoch 4 Train batch 1930/1939 Loss=2.8581 Acc=39.32%\n",
      "Epoch 4 Train batch 1939/1939 Loss=2.8584 Acc=39.32%\n",
      "Epoch 4/5 train_loss=2.8584 train_acc=39.32% val_loss=3.0031 val_acc=39.50%\n",
      "Epoch 5 Train batch 10/1939 Loss=2.7072 Acc=40.34%\n",
      "Epoch 5 Train batch 20/1939 Loss=2.7015 Acc=40.74%\n",
      "Epoch 5 Train batch 30/1939 Loss=2.6969 Acc=40.74%\n",
      "Epoch 5 Train batch 40/1939 Loss=2.6940 Acc=40.77%\n",
      "Epoch 5 Train batch 50/1939 Loss=2.7038 Acc=40.53%\n",
      "Epoch 5 Train batch 60/1939 Loss=2.7067 Acc=40.45%\n",
      "Epoch 5 Train batch 70/1939 Loss=2.7055 Acc=40.37%\n",
      "Epoch 5 Train batch 80/1939 Loss=2.7011 Acc=40.39%\n",
      "Epoch 5 Train batch 90/1939 Loss=2.7029 Acc=40.39%\n",
      "Epoch 5 Train batch 100/1939 Loss=2.7064 Acc=40.30%\n",
      "Epoch 5 Train batch 110/1939 Loss=2.7107 Acc=40.32%\n",
      "Epoch 5 Train batch 120/1939 Loss=2.7138 Acc=40.35%\n",
      "Epoch 5 Train batch 130/1939 Loss=2.7152 Acc=40.29%\n",
      "Epoch 5 Train batch 140/1939 Loss=2.7192 Acc=40.26%\n",
      "Epoch 5 Train batch 150/1939 Loss=2.7174 Acc=40.27%\n",
      "Epoch 5 Train batch 160/1939 Loss=2.7169 Acc=40.24%\n",
      "Epoch 5 Train batch 170/1939 Loss=2.7208 Acc=40.23%\n",
      "Epoch 5 Train batch 180/1939 Loss=2.7197 Acc=40.25%\n",
      "Epoch 5 Train batch 190/1939 Loss=2.7211 Acc=40.24%\n",
      "Epoch 5 Train batch 200/1939 Loss=2.7193 Acc=40.30%\n",
      "Epoch 5 Train batch 210/1939 Loss=2.7192 Acc=40.31%\n",
      "Epoch 5 Train batch 220/1939 Loss=2.7171 Acc=40.36%\n",
      "Epoch 5 Train batch 230/1939 Loss=2.7176 Acc=40.38%\n",
      "Epoch 5 Train batch 240/1939 Loss=2.7182 Acc=40.38%\n",
      "Epoch 5 Train batch 250/1939 Loss=2.7173 Acc=40.39%\n",
      "Epoch 5 Train batch 260/1939 Loss=2.7179 Acc=40.38%\n",
      "Epoch 5 Train batch 270/1939 Loss=2.7187 Acc=40.35%\n",
      "Epoch 5 Train batch 280/1939 Loss=2.7203 Acc=40.35%\n",
      "Epoch 5 Train batch 290/1939 Loss=2.7210 Acc=40.35%\n",
      "Epoch 5 Train batch 300/1939 Loss=2.7217 Acc=40.35%\n",
      "Epoch 5 Train batch 310/1939 Loss=2.7207 Acc=40.34%\n",
      "Epoch 5 Train batch 320/1939 Loss=2.7218 Acc=40.34%\n",
      "Epoch 5 Train batch 330/1939 Loss=2.7218 Acc=40.34%\n",
      "Epoch 5 Train batch 340/1939 Loss=2.7231 Acc=40.32%\n",
      "Epoch 5 Train batch 350/1939 Loss=2.7239 Acc=40.30%\n",
      "Epoch 5 Train batch 360/1939 Loss=2.7237 Acc=40.32%\n",
      "Epoch 5 Train batch 370/1939 Loss=2.7252 Acc=40.30%\n",
      "Epoch 5 Train batch 380/1939 Loss=2.7258 Acc=40.29%\n",
      "Epoch 5 Train batch 390/1939 Loss=2.7269 Acc=40.27%\n",
      "Epoch 5 Train batch 400/1939 Loss=2.7273 Acc=40.26%\n",
      "Epoch 5 Train batch 410/1939 Loss=2.7275 Acc=40.28%\n",
      "Epoch 5 Train batch 420/1939 Loss=2.7282 Acc=40.27%\n",
      "Epoch 5 Train batch 430/1939 Loss=2.7298 Acc=40.24%\n",
      "Epoch 5 Train batch 440/1939 Loss=2.7299 Acc=40.25%\n",
      "Epoch 5 Train batch 450/1939 Loss=2.7304 Acc=40.26%\n",
      "Epoch 5 Train batch 460/1939 Loss=2.7304 Acc=40.27%\n",
      "Epoch 5 Train batch 470/1939 Loss=2.7314 Acc=40.26%\n",
      "Epoch 5 Train batch 480/1939 Loss=2.7314 Acc=40.28%\n",
      "Epoch 5 Train batch 490/1939 Loss=2.7315 Acc=40.30%\n",
      "Epoch 5 Train batch 500/1939 Loss=2.7326 Acc=40.29%\n",
      "Epoch 5 Train batch 510/1939 Loss=2.7334 Acc=40.28%\n",
      "Epoch 5 Train batch 520/1939 Loss=2.7337 Acc=40.27%\n",
      "Epoch 5 Train batch 530/1939 Loss=2.7336 Acc=40.27%\n",
      "Epoch 5 Train batch 540/1939 Loss=2.7338 Acc=40.27%\n",
      "Epoch 5 Train batch 550/1939 Loss=2.7334 Acc=40.26%\n",
      "Epoch 5 Train batch 560/1939 Loss=2.7337 Acc=40.27%\n",
      "Epoch 5 Train batch 570/1939 Loss=2.7353 Acc=40.24%\n",
      "Epoch 5 Train batch 580/1939 Loss=2.7358 Acc=40.23%\n",
      "Epoch 5 Train batch 590/1939 Loss=2.7364 Acc=40.23%\n",
      "Epoch 5 Train batch 600/1939 Loss=2.7373 Acc=40.22%\n",
      "Epoch 5 Train batch 610/1939 Loss=2.7382 Acc=40.21%\n",
      "Epoch 5 Train batch 620/1939 Loss=2.7381 Acc=40.23%\n",
      "Epoch 5 Train batch 630/1939 Loss=2.7387 Acc=40.23%\n",
      "Epoch 5 Train batch 640/1939 Loss=2.7394 Acc=40.22%\n",
      "Epoch 5 Train batch 650/1939 Loss=2.7394 Acc=40.23%\n",
      "Epoch 5 Train batch 660/1939 Loss=2.7396 Acc=40.23%\n",
      "Epoch 5 Train batch 670/1939 Loss=2.7406 Acc=40.21%\n",
      "Epoch 5 Train batch 680/1939 Loss=2.7407 Acc=40.21%\n",
      "Epoch 5 Train batch 690/1939 Loss=2.7419 Acc=40.20%\n",
      "Epoch 5 Train batch 700/1939 Loss=2.7429 Acc=40.18%\n",
      "Epoch 5 Train batch 710/1939 Loss=2.7435 Acc=40.18%\n",
      "Epoch 5 Train batch 720/1939 Loss=2.7446 Acc=40.16%\n",
      "Epoch 5 Train batch 730/1939 Loss=2.7453 Acc=40.15%\n",
      "Epoch 5 Train batch 740/1939 Loss=2.7466 Acc=40.15%\n",
      "Epoch 5 Train batch 750/1939 Loss=2.7477 Acc=40.14%\n",
      "Epoch 5 Train batch 760/1939 Loss=2.7473 Acc=40.15%\n",
      "Epoch 5 Train batch 770/1939 Loss=2.7474 Acc=40.16%\n",
      "Epoch 5 Train batch 780/1939 Loss=2.7482 Acc=40.16%\n",
      "Epoch 5 Train batch 790/1939 Loss=2.7487 Acc=40.16%\n",
      "Epoch 5 Train batch 800/1939 Loss=2.7486 Acc=40.16%\n",
      "Epoch 5 Train batch 810/1939 Loss=2.7493 Acc=40.15%\n",
      "Epoch 5 Train batch 820/1939 Loss=2.7501 Acc=40.15%\n",
      "Epoch 5 Train batch 830/1939 Loss=2.7501 Acc=40.15%\n",
      "Epoch 5 Train batch 840/1939 Loss=2.7503 Acc=40.15%\n",
      "Epoch 5 Train batch 850/1939 Loss=2.7502 Acc=40.15%\n",
      "Epoch 5 Train batch 860/1939 Loss=2.7503 Acc=40.16%\n",
      "Epoch 5 Train batch 870/1939 Loss=2.7507 Acc=40.15%\n",
      "Epoch 5 Train batch 880/1939 Loss=2.7515 Acc=40.14%\n",
      "Epoch 5 Train batch 890/1939 Loss=2.7521 Acc=40.12%\n",
      "Epoch 5 Train batch 900/1939 Loss=2.7516 Acc=40.13%\n",
      "Epoch 5 Train batch 910/1939 Loss=2.7520 Acc=40.13%\n",
      "Epoch 5 Train batch 920/1939 Loss=2.7522 Acc=40.12%\n",
      "Epoch 5 Train batch 930/1939 Loss=2.7527 Acc=40.12%\n",
      "Epoch 5 Train batch 940/1939 Loss=2.7530 Acc=40.12%\n",
      "Epoch 5 Train batch 950/1939 Loss=2.7532 Acc=40.12%\n",
      "Epoch 5 Train batch 960/1939 Loss=2.7534 Acc=40.12%\n",
      "Epoch 5 Train batch 970/1939 Loss=2.7537 Acc=40.12%\n",
      "Epoch 5 Train batch 980/1939 Loss=2.7543 Acc=40.11%\n",
      "Epoch 5 Train batch 990/1939 Loss=2.7548 Acc=40.11%\n",
      "Epoch 5 Train batch 1000/1939 Loss=2.7542 Acc=40.12%\n",
      "Epoch 5 Train batch 1010/1939 Loss=2.7548 Acc=40.11%\n",
      "Epoch 5 Train batch 1020/1939 Loss=2.7552 Acc=40.11%\n",
      "Epoch 5 Train batch 1030/1939 Loss=2.7553 Acc=40.11%\n",
      "Epoch 5 Train batch 1040/1939 Loss=2.7558 Acc=40.10%\n",
      "Epoch 5 Train batch 1050/1939 Loss=2.7562 Acc=40.10%\n",
      "Epoch 5 Train batch 1060/1939 Loss=2.7566 Acc=40.09%\n",
      "Epoch 5 Train batch 1070/1939 Loss=2.7570 Acc=40.09%\n",
      "Epoch 5 Train batch 1080/1939 Loss=2.7572 Acc=40.09%\n",
      "Epoch 5 Train batch 1090/1939 Loss=2.7576 Acc=40.10%\n",
      "Epoch 5 Train batch 1100/1939 Loss=2.7582 Acc=40.09%\n",
      "Epoch 5 Train batch 1110/1939 Loss=2.7585 Acc=40.10%\n",
      "Epoch 5 Train batch 1120/1939 Loss=2.7583 Acc=40.10%\n",
      "Epoch 5 Train batch 1130/1939 Loss=2.7581 Acc=40.11%\n",
      "Epoch 5 Train batch 1140/1939 Loss=2.7583 Acc=40.10%\n",
      "Epoch 5 Train batch 1150/1939 Loss=2.7582 Acc=40.10%\n",
      "Epoch 5 Train batch 1160/1939 Loss=2.7587 Acc=40.10%\n",
      "Epoch 5 Train batch 1170/1939 Loss=2.7591 Acc=40.10%\n",
      "Epoch 5 Train batch 1180/1939 Loss=2.7602 Acc=40.09%\n",
      "Epoch 5 Train batch 1190/1939 Loss=2.7609 Acc=40.08%\n",
      "Epoch 5 Train batch 1200/1939 Loss=2.7609 Acc=40.08%\n",
      "Epoch 5 Train batch 1210/1939 Loss=2.7611 Acc=40.08%\n",
      "Epoch 5 Train batch 1220/1939 Loss=2.7619 Acc=40.07%\n",
      "Epoch 5 Train batch 1230/1939 Loss=2.7621 Acc=40.07%\n",
      "Epoch 5 Train batch 1240/1939 Loss=2.7623 Acc=40.07%\n",
      "Epoch 5 Train batch 1250/1939 Loss=2.7628 Acc=40.07%\n",
      "Epoch 5 Train batch 1260/1939 Loss=2.7630 Acc=40.06%\n",
      "Epoch 5 Train batch 1270/1939 Loss=2.7636 Acc=40.06%\n",
      "Epoch 5 Train batch 1280/1939 Loss=2.7637 Acc=40.06%\n",
      "Epoch 5 Train batch 1290/1939 Loss=2.7638 Acc=40.06%\n",
      "Epoch 5 Train batch 1300/1939 Loss=2.7640 Acc=40.06%\n",
      "Epoch 5 Train batch 1310/1939 Loss=2.7646 Acc=40.05%\n",
      "Epoch 5 Train batch 1320/1939 Loss=2.7644 Acc=40.06%\n",
      "Epoch 5 Train batch 1330/1939 Loss=2.7642 Acc=40.06%\n",
      "Epoch 5 Train batch 1340/1939 Loss=2.7645 Acc=40.05%\n",
      "Epoch 5 Train batch 1350/1939 Loss=2.7645 Acc=40.06%\n",
      "Epoch 5 Train batch 1360/1939 Loss=2.7649 Acc=40.05%\n",
      "Epoch 5 Train batch 1370/1939 Loss=2.7650 Acc=40.05%\n",
      "Epoch 5 Train batch 1380/1939 Loss=2.7654 Acc=40.05%\n",
      "Epoch 5 Train batch 1390/1939 Loss=2.7657 Acc=40.04%\n",
      "Epoch 5 Train batch 1400/1939 Loss=2.7658 Acc=40.04%\n",
      "Epoch 5 Train batch 1410/1939 Loss=2.7660 Acc=40.04%\n",
      "Epoch 5 Train batch 1420/1939 Loss=2.7663 Acc=40.04%\n",
      "Epoch 5 Train batch 1430/1939 Loss=2.7661 Acc=40.04%\n",
      "Epoch 5 Train batch 1440/1939 Loss=2.7666 Acc=40.04%\n",
      "Epoch 5 Train batch 1450/1939 Loss=2.7668 Acc=40.04%\n",
      "Epoch 5 Train batch 1460/1939 Loss=2.7672 Acc=40.03%\n",
      "Epoch 5 Train batch 1470/1939 Loss=2.7672 Acc=40.04%\n",
      "Epoch 5 Train batch 1480/1939 Loss=2.7677 Acc=40.03%\n",
      "Epoch 5 Train batch 1490/1939 Loss=2.7681 Acc=40.03%\n",
      "Epoch 5 Train batch 1500/1939 Loss=2.7683 Acc=40.03%\n",
      "Epoch 5 Train batch 1510/1939 Loss=2.7684 Acc=40.03%\n",
      "Epoch 5 Train batch 1520/1939 Loss=2.7687 Acc=40.03%\n",
      "Epoch 5 Train batch 1530/1939 Loss=2.7691 Acc=40.02%\n",
      "Epoch 5 Train batch 1540/1939 Loss=2.7694 Acc=40.02%\n",
      "Epoch 5 Train batch 1550/1939 Loss=2.7696 Acc=40.02%\n",
      "Epoch 5 Train batch 1560/1939 Loss=2.7697 Acc=40.02%\n",
      "Epoch 5 Train batch 1570/1939 Loss=2.7698 Acc=40.02%\n",
      "Epoch 5 Train batch 1580/1939 Loss=2.7703 Acc=40.02%\n",
      "Epoch 5 Train batch 1590/1939 Loss=2.7708 Acc=40.01%\n",
      "Epoch 5 Train batch 1600/1939 Loss=2.7709 Acc=40.01%\n",
      "Epoch 5 Train batch 1610/1939 Loss=2.7713 Acc=40.01%\n",
      "Epoch 5 Train batch 1620/1939 Loss=2.7716 Acc=40.01%\n",
      "Epoch 5 Train batch 1630/1939 Loss=2.7718 Acc=40.01%\n",
      "Epoch 5 Train batch 1640/1939 Loss=2.7722 Acc=40.00%\n",
      "Epoch 5 Train batch 1650/1939 Loss=2.7725 Acc=40.00%\n",
      "Epoch 5 Train batch 1660/1939 Loss=2.7727 Acc=40.00%\n",
      "Epoch 5 Train batch 1670/1939 Loss=2.7730 Acc=40.00%\n",
      "Epoch 5 Train batch 1680/1939 Loss=2.7731 Acc=40.00%\n",
      "Epoch 5 Train batch 1690/1939 Loss=2.7734 Acc=40.00%\n",
      "Epoch 5 Train batch 1700/1939 Loss=2.7737 Acc=40.00%\n",
      "Epoch 5 Train batch 1710/1939 Loss=2.7738 Acc=39.99%\n",
      "Epoch 5 Train batch 1720/1939 Loss=2.7739 Acc=39.99%\n",
      "Epoch 5 Train batch 1730/1939 Loss=2.7741 Acc=39.99%\n",
      "Epoch 5 Train batch 1740/1939 Loss=2.7741 Acc=39.99%\n",
      "Epoch 5 Train batch 1750/1939 Loss=2.7745 Acc=39.99%\n",
      "Epoch 5 Train batch 1760/1939 Loss=2.7750 Acc=39.98%\n",
      "Epoch 5 Train batch 1770/1939 Loss=2.7751 Acc=39.98%\n",
      "Epoch 5 Train batch 1780/1939 Loss=2.7754 Acc=39.98%\n",
      "Epoch 5 Train batch 1790/1939 Loss=2.7758 Acc=39.98%\n",
      "Epoch 5 Train batch 1800/1939 Loss=2.7762 Acc=39.98%\n",
      "Epoch 5 Train batch 1810/1939 Loss=2.7763 Acc=39.98%\n",
      "Epoch 5 Train batch 1820/1939 Loss=2.7765 Acc=39.98%\n",
      "Epoch 5 Train batch 1830/1939 Loss=2.7769 Acc=39.98%\n",
      "Epoch 5 Train batch 1840/1939 Loss=2.7774 Acc=39.97%\n",
      "Epoch 5 Train batch 1850/1939 Loss=2.7777 Acc=39.97%\n",
      "Epoch 5 Train batch 1860/1939 Loss=2.7776 Acc=39.97%\n",
      "Epoch 5 Train batch 1870/1939 Loss=2.7778 Acc=39.97%\n",
      "Epoch 5 Train batch 1880/1939 Loss=2.7778 Acc=39.98%\n",
      "Epoch 5 Train batch 1890/1939 Loss=2.7780 Acc=39.98%\n",
      "Epoch 5 Train batch 1900/1939 Loss=2.7781 Acc=39.98%\n",
      "Epoch 5 Train batch 1910/1939 Loss=2.7783 Acc=39.98%\n",
      "Epoch 5 Train batch 1920/1939 Loss=2.7790 Acc=39.97%\n",
      "Epoch 5 Train batch 1930/1939 Loss=2.7792 Acc=39.97%\n",
      "Epoch 5 Train batch 1939/1939 Loss=2.7794 Acc=39.97%\n",
      "Epoch 5/5 train_loss=2.7794 train_acc=39.97% val_loss=3.0031 val_acc=39.72%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2: ViT trained from scratch",
   "id": "82eb4f9c519c051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers trained from scratch.\n",
    "\n",
    "Encoder: Visual transformer (ViT), no transfer learning.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "48aaf7e1f14ee57c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:43:46.914659Z",
     "start_time": "2025-10-20T22:41:35.964696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Simple patch embedding: conv with stride=patch_size.\"\"\"\n",
    "    def __init__(self, in_ch=3, embed_dim=256, patch_size=16):\n",
    "        super().__init__()\n",
    "        assert embed_dim % 1 == 0\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)                     # (B, E, Hp, Wp)\n",
    "        x = x.flatten(2).transpose(1, 2)     # (B, S, E) where S = Hp*Wp\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, patch_size=16, num_layers=4, num_heads=4, mlp_dim=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(in_ch=3, embed_dim=embed_dim, patch_size=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = None  # created lazily based on sequence length\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,H,W)\n",
    "        x = self.patch_embed(x)   # (B, S, E)\n",
    "        B, S, E = x.shape\n",
    "        if self.pos_embed is None or self.pos_embed.size(1) != (S+1):\n",
    "            # initialize positional embedding (1, S+1, E)\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, S+1, E).to(x.device))\n",
    "            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B,1,E)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)          # (B, S+1, E)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer_encoder(x)                # (B, S+1, E)\n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:, 0]                              # (B, E) - pooled\n",
    "        return cls_out\n",
    "\n",
    "# --------------- Decoder: ---------------\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (T, B, D)\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pe[:seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerDecoderAdapter(DecoderBase):\n",
    "    def __init__(self, embed_size, vocab_size, nhead=8, num_layers=3, dim_feedforward=2048, dropout=0.2, max_len=50):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len=max_len)\n",
    "        self.feature_proj = nn.Linear(embed_size, embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.linear_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # features: (B, E)\n",
    "        device = features.device\n",
    "        B, L = captions.size()\n",
    "        memory = self.feature_proj(features).unsqueeze(0)  # (1, B, E)\n",
    "        tgt = self.embed(captions).permute(1, 0, 2)        # (L, B, E)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(L, device)\n",
    "        out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # (L, B, E)\n",
    "        out = out.permute(1, 0, 2)  # (B, L, E)\n",
    "        logits = self.linear_out(out)  # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, features, start_id=None, max_len=None):\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "        assert start_id is not None, \"start_id must be provided for transformer sampling\"\n",
    "        device = features.device\n",
    "        B = features.size(0)\n",
    "        memory = self.feature_proj(features).unsqueeze(0)  # (1, B, E)\n",
    "        generated = torch.full((B, 1), fill_value=start_id, dtype=torch.long, device=device)\n",
    "        ids = []\n",
    "        for t in range(max_len):\n",
    "            tgt = self.embed(generated).permute(1, 0, 2)   # (T, B, E)\n",
    "            tgt = self.pos_enc(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(tgt.size(0), device)\n",
    "            out = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # (T, B, E)\n",
    "            last = out[-1]  # (B, E)\n",
    "            logits = self.linear_out(last)  # (B, V)\n",
    "            pred = logits.argmax(dim=1)     # (B,)\n",
    "            ids.append(pred)\n",
    "            generated = torch.cat([generated, pred.unsqueeze(1)], dim=1)\n",
    "        ids = torch.stack(ids, dim=1)  # (B, max_len)\n",
    "        return ids\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "ENC_LR = 5e-5\n",
    "DEC_LR = 1e-4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "EMBED_SIZE = 768\n",
    "ENC_NUM_LAYERS = 6\n",
    "DEC_NUM_LAYERS = 3\n",
    "NUM_HEADS = 8\n",
    "DIM_FEEDFORWARD = 2048\n",
    "\n",
    "OUTPUT_DIR = \"./models_vit_no_tl\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Instantiate models\n",
    "enc = ViTEncoder(\n",
    "    embed_dim=EMBED_SIZE,\n",
    "    patch_size=16,\n",
    "    num_layers=ENC_NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dim=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "dec = TransformerDecoderAdapter(\n",
    "    embed_size=EMBED_SIZE,\n",
    "    vocab_size=len(vocab),\n",
    "    nhead=NUM_HEADS,\n",
    "    num_layers=DEC_NUM_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "# Optimizers and loss\n",
    "enc_opt = optim.Adam(enc.parameters(), lr=ENC_LR)\n",
    "dec_opt = optim.Adam(dec.parameters(), lr=DEC_LR)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Train\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS\n",
    ")"
   ],
   "id": "3e4340c01d6f5a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 6989\n",
      "Epoch 1 Train batch 10/3877 Loss=7.2976 Acc=12.21%\n",
      "Epoch 1 Train batch 20/3877 Loss=6.8139 Acc=13.30%\n",
      "Epoch 1 Train batch 30/3877 Loss=6.5160 Acc=14.79%\n",
      "Epoch 1 Train batch 40/3877 Loss=6.2681 Acc=16.09%\n",
      "Epoch 1 Train batch 50/3877 Loss=6.0802 Acc=17.21%\n",
      "Epoch 1 Train batch 60/3877 Loss=5.9588 Acc=17.77%\n",
      "Epoch 1 Train batch 70/3877 Loss=5.8436 Acc=18.39%\n",
      "Epoch 1 Train batch 80/3877 Loss=5.7417 Acc=19.00%\n",
      "Epoch 1 Train batch 90/3877 Loss=5.6524 Acc=19.65%\n",
      "Epoch 1 Train batch 100/3877 Loss=5.5766 Acc=20.14%\n",
      "Epoch 1 Train batch 110/3877 Loss=5.5095 Acc=20.58%\n",
      "Epoch 1 Train batch 120/3877 Loss=5.4592 Acc=20.91%\n",
      "Epoch 1 Train batch 130/3877 Loss=5.4103 Acc=21.22%\n",
      "Epoch 1 Train batch 140/3877 Loss=5.3577 Acc=21.58%\n",
      "Epoch 1 Train batch 150/3877 Loss=5.3108 Acc=21.87%\n",
      "Epoch 1 Train batch 160/3877 Loss=5.2700 Acc=22.12%\n",
      "Epoch 1 Train batch 170/3877 Loss=5.2314 Acc=22.35%\n",
      "Epoch 1 Train batch 180/3877 Loss=5.1939 Acc=22.59%\n",
      "Epoch 1 Train batch 190/3877 Loss=5.1620 Acc=22.81%\n",
      "Epoch 1 Train batch 200/3877 Loss=5.1302 Acc=23.03%\n",
      "Epoch 1 Train batch 210/3877 Loss=5.1022 Acc=23.21%\n",
      "Epoch 1 Train batch 220/3877 Loss=5.0724 Acc=23.47%\n",
      "Epoch 1 Train batch 230/3877 Loss=5.0476 Acc=23.61%\n",
      "Epoch 1 Train batch 240/3877 Loss=5.0241 Acc=23.79%\n",
      "Epoch 1 Train batch 250/3877 Loss=4.9972 Acc=23.95%\n",
      "Epoch 1 Train batch 260/3877 Loss=4.9764 Acc=24.12%\n",
      "Epoch 1 Train batch 270/3877 Loss=4.9564 Acc=24.24%\n",
      "Epoch 1 Train batch 280/3877 Loss=4.9366 Acc=24.38%\n",
      "Epoch 1 Train batch 290/3877 Loss=4.9182 Acc=24.50%\n",
      "Epoch 1 Train batch 300/3877 Loss=4.9020 Acc=24.58%\n",
      "Epoch 1 Train batch 310/3877 Loss=4.8811 Acc=24.71%\n",
      "Epoch 1 Train batch 320/3877 Loss=4.8669 Acc=24.81%\n",
      "Epoch 1 Train batch 330/3877 Loss=4.8489 Acc=24.95%\n",
      "Epoch 1 Train batch 340/3877 Loss=4.8326 Acc=25.08%\n",
      "Epoch 1 Train batch 350/3877 Loss=4.8178 Acc=25.18%\n",
      "Epoch 1 Train batch 360/3877 Loss=4.8050 Acc=25.28%\n",
      "Epoch 1 Train batch 370/3877 Loss=4.7926 Acc=25.34%\n",
      "Epoch 1 Train batch 380/3877 Loss=4.7816 Acc=25.41%\n",
      "Epoch 1 Train batch 390/3877 Loss=4.7695 Acc=25.53%\n",
      "Epoch 1 Train batch 400/3877 Loss=4.7576 Acc=25.59%\n",
      "Epoch 1 Train batch 410/3877 Loss=4.7440 Acc=25.71%\n",
      "Epoch 1 Train batch 420/3877 Loss=4.7308 Acc=25.80%\n",
      "Epoch 1 Train batch 430/3877 Loss=4.7168 Acc=25.88%\n",
      "Epoch 1 Train batch 440/3877 Loss=4.7052 Acc=25.97%\n",
      "Epoch 1 Train batch 450/3877 Loss=4.6942 Acc=26.05%\n",
      "Epoch 1 Train batch 460/3877 Loss=4.6820 Acc=26.14%\n",
      "Epoch 1 Train batch 470/3877 Loss=4.6717 Acc=26.19%\n",
      "Epoch 1 Train batch 480/3877 Loss=4.6618 Acc=26.23%\n",
      "Epoch 1 Train batch 490/3877 Loss=4.6516 Acc=26.29%\n",
      "Epoch 1 Train batch 500/3877 Loss=4.6415 Acc=26.35%\n",
      "Epoch 1 Train batch 510/3877 Loss=4.6314 Acc=26.43%\n",
      "Epoch 1 Train batch 520/3877 Loss=4.6234 Acc=26.49%\n",
      "Epoch 1 Train batch 530/3877 Loss=4.6159 Acc=26.54%\n",
      "Epoch 1 Train batch 540/3877 Loss=4.6067 Acc=26.60%\n",
      "Epoch 1 Train batch 550/3877 Loss=4.5977 Acc=26.66%\n",
      "Epoch 1 Train batch 560/3877 Loss=4.5886 Acc=26.73%\n",
      "Epoch 1 Train batch 570/3877 Loss=4.5789 Acc=26.81%\n",
      "Epoch 1 Train batch 580/3877 Loss=4.5699 Acc=26.88%\n",
      "Epoch 1 Train batch 590/3877 Loss=4.5624 Acc=26.92%\n",
      "Epoch 1 Train batch 600/3877 Loss=4.5535 Acc=26.99%\n",
      "Epoch 1 Train batch 610/3877 Loss=4.5437 Acc=27.06%\n",
      "Epoch 1 Train batch 620/3877 Loss=4.5366 Acc=27.12%\n",
      "Epoch 1 Train batch 630/3877 Loss=4.5299 Acc=27.17%\n",
      "Epoch 1 Train batch 640/3877 Loss=4.5217 Acc=27.24%\n",
      "Epoch 1 Train batch 650/3877 Loss=4.5155 Acc=27.28%\n",
      "Epoch 1 Train batch 660/3877 Loss=4.5069 Acc=27.33%\n",
      "Epoch 1 Train batch 670/3877 Loss=4.5004 Acc=27.37%\n",
      "Epoch 1 Train batch 680/3877 Loss=4.4946 Acc=27.40%\n",
      "Epoch 1 Train batch 690/3877 Loss=4.4870 Acc=27.45%\n",
      "Epoch 1 Train batch 700/3877 Loss=4.4806 Acc=27.49%\n",
      "Epoch 1 Train batch 710/3877 Loss=4.4735 Acc=27.54%\n",
      "Epoch 1 Train batch 720/3877 Loss=4.4674 Acc=27.58%\n",
      "Epoch 1 Train batch 730/3877 Loss=4.4603 Acc=27.62%\n",
      "Epoch 1 Train batch 740/3877 Loss=4.4546 Acc=27.66%\n",
      "Epoch 1 Train batch 750/3877 Loss=4.4494 Acc=27.69%\n",
      "Epoch 1 Train batch 760/3877 Loss=4.4430 Acc=27.74%\n",
      "Epoch 1 Train batch 770/3877 Loss=4.4365 Acc=27.78%\n",
      "Epoch 1 Train batch 780/3877 Loss=4.4310 Acc=27.81%\n",
      "Epoch 1 Train batch 790/3877 Loss=4.4245 Acc=27.86%\n",
      "Epoch 1 Train batch 800/3877 Loss=4.4176 Acc=27.91%\n",
      "Epoch 1 Train batch 810/3877 Loss=4.4120 Acc=27.94%\n",
      "Epoch 1 Train batch 820/3877 Loss=4.4065 Acc=27.97%\n",
      "Epoch 1 Train batch 830/3877 Loss=4.4015 Acc=28.01%\n",
      "Epoch 1 Train batch 840/3877 Loss=4.3968 Acc=28.04%\n",
      "Epoch 1 Train batch 850/3877 Loss=4.3921 Acc=28.06%\n",
      "Epoch 1 Train batch 860/3877 Loss=4.3872 Acc=28.09%\n",
      "Epoch 1 Train batch 870/3877 Loss=4.3821 Acc=28.13%\n",
      "Epoch 1 Train batch 880/3877 Loss=4.3764 Acc=28.17%\n",
      "Epoch 1 Train batch 890/3877 Loss=4.3716 Acc=28.21%\n",
      "Epoch 1 Train batch 900/3877 Loss=4.3674 Acc=28.23%\n",
      "Epoch 1 Train batch 910/3877 Loss=4.3614 Acc=28.28%\n",
      "Epoch 1 Train batch 920/3877 Loss=4.3568 Acc=28.31%\n",
      "Epoch 1 Train batch 930/3877 Loss=4.3522 Acc=28.33%\n",
      "Epoch 1 Train batch 940/3877 Loss=4.3471 Acc=28.37%\n",
      "Epoch 1 Train batch 950/3877 Loss=4.3424 Acc=28.41%\n",
      "Epoch 1 Train batch 960/3877 Loss=4.3382 Acc=28.45%\n",
      "Epoch 1 Train batch 970/3877 Loss=4.3335 Acc=28.49%\n",
      "Epoch 1 Train batch 980/3877 Loss=4.3285 Acc=28.53%\n",
      "Epoch 1 Train batch 990/3877 Loss=4.3240 Acc=28.56%\n",
      "Epoch 1 Train batch 1000/3877 Loss=4.3190 Acc=28.59%\n",
      "Epoch 1 Train batch 1010/3877 Loss=4.3152 Acc=28.62%\n",
      "Epoch 1 Train batch 1020/3877 Loss=4.3105 Acc=28.65%\n",
      "Epoch 1 Train batch 1030/3877 Loss=4.3060 Acc=28.69%\n",
      "Epoch 1 Train batch 1040/3877 Loss=4.3022 Acc=28.72%\n",
      "Epoch 1 Train batch 1050/3877 Loss=4.2985 Acc=28.73%\n",
      "Epoch 1 Train batch 1060/3877 Loss=4.2941 Acc=28.77%\n",
      "Epoch 1 Train batch 1070/3877 Loss=4.2891 Acc=28.81%\n",
      "Epoch 1 Train batch 1080/3877 Loss=4.2847 Acc=28.84%\n",
      "Epoch 1 Train batch 1090/3877 Loss=4.2801 Acc=28.87%\n",
      "Epoch 1 Train batch 1100/3877 Loss=4.2760 Acc=28.90%\n",
      "Epoch 1 Train batch 1110/3877 Loss=4.2721 Acc=28.93%\n",
      "Epoch 1 Train batch 1120/3877 Loss=4.2683 Acc=28.96%\n",
      "Epoch 1 Train batch 1130/3877 Loss=4.2645 Acc=28.99%\n",
      "Epoch 1 Train batch 1140/3877 Loss=4.2609 Acc=29.02%\n",
      "Epoch 1 Train batch 1150/3877 Loss=4.2576 Acc=29.05%\n",
      "Epoch 1 Train batch 1160/3877 Loss=4.2550 Acc=29.06%\n",
      "Epoch 1 Train batch 1170/3877 Loss=4.2521 Acc=29.08%\n",
      "Epoch 1 Train batch 1180/3877 Loss=4.2482 Acc=29.11%\n",
      "Epoch 1 Train batch 1190/3877 Loss=4.2443 Acc=29.13%\n",
      "Epoch 1 Train batch 1200/3877 Loss=4.2405 Acc=29.15%\n",
      "Epoch 1 Train batch 1210/3877 Loss=4.2373 Acc=29.17%\n",
      "Epoch 1 Train batch 1220/3877 Loss=4.2335 Acc=29.19%\n",
      "Epoch 1 Train batch 1230/3877 Loss=4.2302 Acc=29.21%\n",
      "Epoch 1 Train batch 1240/3877 Loss=4.2278 Acc=29.21%\n",
      "Epoch 1 Train batch 1250/3877 Loss=4.2232 Acc=29.25%\n",
      "Epoch 1 Train batch 1260/3877 Loss=4.2190 Acc=29.28%\n",
      "Epoch 1 Train batch 1270/3877 Loss=4.2151 Acc=29.31%\n",
      "Epoch 1 Train batch 1280/3877 Loss=4.2117 Acc=29.34%\n",
      "Epoch 1 Train batch 1290/3877 Loss=4.2079 Acc=29.37%\n",
      "Epoch 1 Train batch 1300/3877 Loss=4.2048 Acc=29.39%\n",
      "Epoch 1 Train batch 1310/3877 Loss=4.2015 Acc=29.42%\n",
      "Epoch 1 Train batch 1320/3877 Loss=4.1982 Acc=29.44%\n",
      "Epoch 1 Train batch 1330/3877 Loss=4.1950 Acc=29.46%\n",
      "Epoch 1 Train batch 1340/3877 Loss=4.1920 Acc=29.48%\n",
      "Epoch 1 Train batch 1350/3877 Loss=4.1888 Acc=29.50%\n",
      "Epoch 1 Train batch 1360/3877 Loss=4.1862 Acc=29.52%\n",
      "Epoch 1 Train batch 1370/3877 Loss=4.1829 Acc=29.55%\n",
      "Epoch 1 Train batch 1380/3877 Loss=4.1805 Acc=29.57%\n",
      "Epoch 1 Train batch 1390/3877 Loss=4.1770 Acc=29.60%\n",
      "Epoch 1 Train batch 1400/3877 Loss=4.1738 Acc=29.63%\n",
      "Epoch 1 Train batch 1410/3877 Loss=4.1709 Acc=29.64%\n",
      "Epoch 1 Train batch 1420/3877 Loss=4.1678 Acc=29.66%\n",
      "Epoch 1 Train batch 1430/3877 Loss=4.1654 Acc=29.67%\n",
      "Epoch 1 Train batch 1440/3877 Loss=4.1628 Acc=29.69%\n",
      "Epoch 1 Train batch 1450/3877 Loss=4.1595 Acc=29.70%\n",
      "Epoch 1 Train batch 1460/3877 Loss=4.1569 Acc=29.72%\n",
      "Epoch 1 Train batch 1470/3877 Loss=4.1547 Acc=29.73%\n",
      "Epoch 1 Train batch 1480/3877 Loss=4.1517 Acc=29.76%\n",
      "Epoch 1 Train batch 1490/3877 Loss=4.1485 Acc=29.78%\n",
      "Epoch 1 Train batch 1500/3877 Loss=4.1462 Acc=29.79%\n",
      "Epoch 1 Train batch 1510/3877 Loss=4.1432 Acc=29.81%\n",
      "Epoch 1 Train batch 1520/3877 Loss=4.1401 Acc=29.83%\n",
      "Epoch 1 Train batch 1530/3877 Loss=4.1377 Acc=29.85%\n",
      "Epoch 1 Train batch 1540/3877 Loss=4.1356 Acc=29.86%\n",
      "Epoch 1 Train batch 1550/3877 Loss=4.1325 Acc=29.89%\n",
      "Epoch 1 Train batch 1560/3877 Loss=4.1299 Acc=29.91%\n",
      "Epoch 1 Train batch 1570/3877 Loss=4.1271 Acc=29.93%\n",
      "Epoch 1 Train batch 1580/3877 Loss=4.1247 Acc=29.94%\n",
      "Epoch 1 Train batch 1590/3877 Loss=4.1225 Acc=29.95%\n",
      "Epoch 1 Train batch 1600/3877 Loss=4.1203 Acc=29.97%\n",
      "Epoch 1 Train batch 1610/3877 Loss=4.1185 Acc=29.97%\n",
      "Epoch 1 Train batch 1620/3877 Loss=4.1160 Acc=29.99%\n",
      "Epoch 1 Train batch 1630/3877 Loss=4.1132 Acc=30.01%\n",
      "Epoch 1 Train batch 1640/3877 Loss=4.1111 Acc=30.02%\n",
      "Epoch 1 Train batch 1650/3877 Loss=4.1085 Acc=30.04%\n",
      "Epoch 1 Train batch 1660/3877 Loss=4.1062 Acc=30.06%\n",
      "Epoch 1 Train batch 1670/3877 Loss=4.1039 Acc=30.08%\n",
      "Epoch 1 Train batch 1680/3877 Loss=4.1013 Acc=30.09%\n",
      "Epoch 1 Train batch 1690/3877 Loss=4.0988 Acc=30.10%\n",
      "Epoch 1 Train batch 1700/3877 Loss=4.0967 Acc=30.12%\n",
      "Epoch 1 Train batch 1710/3877 Loss=4.0944 Acc=30.13%\n",
      "Epoch 1 Train batch 1720/3877 Loss=4.0919 Acc=30.14%\n",
      "Epoch 1 Train batch 1730/3877 Loss=4.0900 Acc=30.16%\n",
      "Epoch 1 Train batch 1740/3877 Loss=4.0878 Acc=30.17%\n",
      "Epoch 1 Train batch 1750/3877 Loss=4.0854 Acc=30.19%\n",
      "Epoch 1 Train batch 1760/3877 Loss=4.0827 Acc=30.21%\n",
      "Epoch 1 Train batch 1770/3877 Loss=4.0806 Acc=30.23%\n",
      "Epoch 1 Train batch 1780/3877 Loss=4.0782 Acc=30.24%\n",
      "Epoch 1 Train batch 1790/3877 Loss=4.0757 Acc=30.26%\n",
      "Epoch 1 Train batch 1800/3877 Loss=4.0742 Acc=30.27%\n",
      "Epoch 1 Train batch 1810/3877 Loss=4.0723 Acc=30.29%\n",
      "Epoch 1 Train batch 1820/3877 Loss=4.0702 Acc=30.30%\n",
      "Epoch 1 Train batch 1830/3877 Loss=4.0680 Acc=30.31%\n",
      "Epoch 1 Train batch 1840/3877 Loss=4.0659 Acc=30.33%\n",
      "Epoch 1 Train batch 1850/3877 Loss=4.0638 Acc=30.34%\n",
      "Epoch 1 Train batch 1860/3877 Loss=4.0619 Acc=30.35%\n",
      "Epoch 1 Train batch 1870/3877 Loss=4.0602 Acc=30.36%\n",
      "Epoch 1 Train batch 1880/3877 Loss=4.0582 Acc=30.38%\n",
      "Epoch 1 Train batch 1890/3877 Loss=4.0561 Acc=30.40%\n",
      "Epoch 1 Train batch 1900/3877 Loss=4.0546 Acc=30.41%\n",
      "Epoch 1 Train batch 1910/3877 Loss=4.0528 Acc=30.42%\n",
      "Epoch 1 Train batch 1920/3877 Loss=4.0502 Acc=30.43%\n",
      "Epoch 1 Train batch 1930/3877 Loss=4.0479 Acc=30.45%\n",
      "Epoch 1 Train batch 1940/3877 Loss=4.0461 Acc=30.46%\n",
      "Epoch 1 Train batch 1950/3877 Loss=4.0440 Acc=30.48%\n",
      "Epoch 1 Train batch 1960/3877 Loss=4.0420 Acc=30.49%\n",
      "Epoch 1 Train batch 1970/3877 Loss=4.0399 Acc=30.51%\n",
      "Epoch 1 Train batch 1980/3877 Loss=4.0376 Acc=30.52%\n",
      "Epoch 1 Train batch 1990/3877 Loss=4.0360 Acc=30.54%\n",
      "Epoch 1 Train batch 2000/3877 Loss=4.0339 Acc=30.56%\n",
      "Epoch 1 Train batch 2010/3877 Loss=4.0324 Acc=30.57%\n",
      "Epoch 1 Train batch 2020/3877 Loss=4.0302 Acc=30.58%\n",
      "Epoch 1 Train batch 2030/3877 Loss=4.0282 Acc=30.59%\n",
      "Epoch 1 Train batch 2040/3877 Loss=4.0260 Acc=30.61%\n",
      "Epoch 1 Train batch 2050/3877 Loss=4.0239 Acc=30.63%\n",
      "Epoch 1 Train batch 2060/3877 Loss=4.0219 Acc=30.64%\n",
      "Epoch 1 Train batch 2070/3877 Loss=4.0199 Acc=30.66%\n",
      "Epoch 1 Train batch 2080/3877 Loss=4.0180 Acc=30.67%\n",
      "Epoch 1 Train batch 2090/3877 Loss=4.0157 Acc=30.69%\n",
      "Epoch 1 Train batch 2100/3877 Loss=4.0140 Acc=30.70%\n",
      "Epoch 1 Train batch 2110/3877 Loss=4.0123 Acc=30.71%\n",
      "Epoch 1 Train batch 2120/3877 Loss=4.0109 Acc=30.72%\n",
      "Epoch 1 Train batch 2130/3877 Loss=4.0090 Acc=30.73%\n",
      "Epoch 1 Train batch 2140/3877 Loss=4.0070 Acc=30.75%\n",
      "Epoch 1 Train batch 2150/3877 Loss=4.0057 Acc=30.76%\n",
      "Epoch 1 Train batch 2160/3877 Loss=4.0038 Acc=30.77%\n",
      "Epoch 1 Train batch 2170/3877 Loss=4.0022 Acc=30.78%\n",
      "Epoch 1 Train batch 2180/3877 Loss=4.0005 Acc=30.80%\n",
      "Epoch 1 Train batch 2190/3877 Loss=3.9987 Acc=30.81%\n",
      "Epoch 1 Train batch 2200/3877 Loss=3.9973 Acc=30.81%\n",
      "Epoch 1 Train batch 2210/3877 Loss=3.9953 Acc=30.83%\n",
      "Epoch 1 Train batch 2220/3877 Loss=3.9932 Acc=30.85%\n",
      "Epoch 1 Train batch 2230/3877 Loss=3.9914 Acc=30.86%\n",
      "Epoch 1 Train batch 2240/3877 Loss=3.9895 Acc=30.87%\n",
      "Epoch 1 Train batch 2250/3877 Loss=3.9876 Acc=30.89%\n",
      "Epoch 1 Train batch 2260/3877 Loss=3.9861 Acc=30.90%\n",
      "Epoch 1 Train batch 2270/3877 Loss=3.9846 Acc=30.92%\n",
      "Epoch 1 Train batch 2280/3877 Loss=3.9825 Acc=30.94%\n",
      "Epoch 1 Train batch 2290/3877 Loss=3.9809 Acc=30.95%\n",
      "Epoch 1 Train batch 2300/3877 Loss=3.9794 Acc=30.95%\n",
      "Epoch 1 Train batch 2310/3877 Loss=3.9777 Acc=30.97%\n",
      "Epoch 1 Train batch 2320/3877 Loss=3.9759 Acc=30.98%\n",
      "Epoch 1 Train batch 2330/3877 Loss=3.9742 Acc=31.00%\n",
      "Epoch 1 Train batch 2340/3877 Loss=3.9723 Acc=31.01%\n",
      "Epoch 1 Train batch 2350/3877 Loss=3.9706 Acc=31.02%\n",
      "Epoch 1 Train batch 2360/3877 Loss=3.9690 Acc=31.03%\n",
      "Epoch 1 Train batch 2370/3877 Loss=3.9670 Acc=31.03%\n",
      "Epoch 1 Train batch 2380/3877 Loss=3.9655 Acc=31.05%\n",
      "Epoch 1 Train batch 2390/3877 Loss=3.9634 Acc=31.06%\n",
      "Epoch 1 Train batch 2400/3877 Loss=3.9623 Acc=31.07%\n",
      "Epoch 1 Train batch 2410/3877 Loss=3.9610 Acc=31.08%\n",
      "Epoch 1 Train batch 2420/3877 Loss=3.9596 Acc=31.09%\n",
      "Epoch 1 Train batch 2430/3877 Loss=3.9578 Acc=31.10%\n",
      "Epoch 1 Train batch 2440/3877 Loss=3.9566 Acc=31.10%\n",
      "Epoch 1 Train batch 2450/3877 Loss=3.9550 Acc=31.12%\n",
      "Epoch 1 Train batch 2460/3877 Loss=3.9532 Acc=31.13%\n",
      "Epoch 1 Train batch 2470/3877 Loss=3.9514 Acc=31.15%\n",
      "Epoch 1 Train batch 2480/3877 Loss=3.9496 Acc=31.16%\n",
      "Epoch 1 Train batch 2490/3877 Loss=3.9478 Acc=31.18%\n",
      "Epoch 1 Train batch 2500/3877 Loss=3.9464 Acc=31.18%\n",
      "Epoch 1 Train batch 2510/3877 Loss=3.9447 Acc=31.20%\n",
      "Epoch 1 Train batch 2520/3877 Loss=3.9434 Acc=31.21%\n",
      "Epoch 1 Train batch 2530/3877 Loss=3.9422 Acc=31.21%\n",
      "Epoch 1 Train batch 2540/3877 Loss=3.9407 Acc=31.22%\n",
      "Epoch 1 Train batch 2550/3877 Loss=3.9393 Acc=31.24%\n",
      "Epoch 1 Train batch 2560/3877 Loss=3.9379 Acc=31.24%\n",
      "Epoch 1 Train batch 2570/3877 Loss=3.9366 Acc=31.25%\n",
      "Epoch 1 Train batch 2580/3877 Loss=3.9348 Acc=31.26%\n",
      "Epoch 1 Train batch 2590/3877 Loss=3.9334 Acc=31.27%\n",
      "Epoch 1 Train batch 2600/3877 Loss=3.9318 Acc=31.29%\n",
      "Epoch 1 Train batch 2610/3877 Loss=3.9303 Acc=31.30%\n",
      "Epoch 1 Train batch 2620/3877 Loss=3.9292 Acc=31.30%\n",
      "Epoch 1 Train batch 2630/3877 Loss=3.9277 Acc=31.32%\n",
      "Epoch 1 Train batch 2640/3877 Loss=3.9265 Acc=31.32%\n",
      "Epoch 1 Train batch 2650/3877 Loss=3.9248 Acc=31.33%\n",
      "Epoch 1 Train batch 2660/3877 Loss=3.9239 Acc=31.34%\n",
      "Epoch 1 Train batch 2670/3877 Loss=3.9226 Acc=31.35%\n",
      "Epoch 1 Train batch 2680/3877 Loss=3.9215 Acc=31.35%\n",
      "Epoch 1 Train batch 2690/3877 Loss=3.9201 Acc=31.36%\n",
      "Epoch 1 Train batch 2700/3877 Loss=3.9190 Acc=31.37%\n",
      "Epoch 1 Train batch 2710/3877 Loss=3.9177 Acc=31.38%\n",
      "Epoch 1 Train batch 2720/3877 Loss=3.9165 Acc=31.39%\n",
      "Epoch 1 Train batch 2730/3877 Loss=3.9150 Acc=31.40%\n",
      "Epoch 1 Train batch 2740/3877 Loss=3.9133 Acc=31.41%\n",
      "Epoch 1 Train batch 2750/3877 Loss=3.9117 Acc=31.42%\n",
      "Epoch 1 Train batch 2760/3877 Loss=3.9103 Acc=31.43%\n",
      "Epoch 1 Train batch 2770/3877 Loss=3.9092 Acc=31.44%\n",
      "Epoch 1 Train batch 2780/3877 Loss=3.9079 Acc=31.45%\n",
      "Epoch 1 Train batch 2790/3877 Loss=3.9065 Acc=31.46%\n",
      "Epoch 1 Train batch 2800/3877 Loss=3.9056 Acc=31.46%\n",
      "Epoch 1 Train batch 2810/3877 Loss=3.9042 Acc=31.48%\n",
      "Epoch 1 Train batch 2820/3877 Loss=3.9029 Acc=31.49%\n",
      "Epoch 1 Train batch 2830/3877 Loss=3.9016 Acc=31.50%\n",
      "Epoch 1 Train batch 2840/3877 Loss=3.9003 Acc=31.50%\n",
      "Epoch 1 Train batch 2850/3877 Loss=3.8988 Acc=31.51%\n",
      "Epoch 1 Train batch 2860/3877 Loss=3.8973 Acc=31.53%\n",
      "Epoch 1 Train batch 2870/3877 Loss=3.8961 Acc=31.54%\n",
      "Epoch 1 Train batch 2880/3877 Loss=3.8949 Acc=31.54%\n",
      "Epoch 1 Train batch 2890/3877 Loss=3.8938 Acc=31.55%\n",
      "Epoch 1 Train batch 2900/3877 Loss=3.8924 Acc=31.57%\n",
      "Epoch 1 Train batch 2910/3877 Loss=3.8912 Acc=31.57%\n",
      "Epoch 1 Train batch 2920/3877 Loss=3.8902 Acc=31.58%\n",
      "Epoch 1 Train batch 2930/3877 Loss=3.8894 Acc=31.58%\n",
      "Epoch 1 Train batch 2940/3877 Loss=3.8882 Acc=31.59%\n",
      "Epoch 1 Train batch 2950/3877 Loss=3.8867 Acc=31.60%\n",
      "Epoch 1 Train batch 2960/3877 Loss=3.8854 Acc=31.61%\n",
      "Epoch 1 Train batch 2970/3877 Loss=3.8840 Acc=31.62%\n",
      "Epoch 1 Train batch 2980/3877 Loss=3.8828 Acc=31.63%\n",
      "Epoch 1 Train batch 2990/3877 Loss=3.8818 Acc=31.64%\n",
      "Epoch 1 Train batch 3000/3877 Loss=3.8808 Acc=31.64%\n",
      "Epoch 1 Train batch 3010/3877 Loss=3.8798 Acc=31.65%\n",
      "Epoch 1 Train batch 3020/3877 Loss=3.8782 Acc=31.66%\n",
      "Epoch 1 Train batch 3030/3877 Loss=3.8769 Acc=31.67%\n",
      "Epoch 1 Train batch 3040/3877 Loss=3.8758 Acc=31.68%\n",
      "Epoch 1 Train batch 3050/3877 Loss=3.8744 Acc=31.69%\n",
      "Epoch 1 Train batch 3060/3877 Loss=3.8732 Acc=31.70%\n",
      "Epoch 1 Train batch 3070/3877 Loss=3.8721 Acc=31.71%\n",
      "Epoch 1 Train batch 3080/3877 Loss=3.8709 Acc=31.72%\n",
      "Epoch 1 Train batch 3090/3877 Loss=3.8697 Acc=31.73%\n",
      "Epoch 1 Train batch 3100/3877 Loss=3.8684 Acc=31.74%\n",
      "Epoch 1 Train batch 3110/3877 Loss=3.8673 Acc=31.75%\n",
      "Epoch 1 Train batch 3120/3877 Loss=3.8662 Acc=31.76%\n",
      "Epoch 1 Train batch 3130/3877 Loss=3.8651 Acc=31.77%\n",
      "Epoch 1 Train batch 3140/3877 Loss=3.8638 Acc=31.78%\n",
      "Epoch 1 Train batch 3150/3877 Loss=3.8629 Acc=31.78%\n",
      "Epoch 1 Train batch 3160/3877 Loss=3.8618 Acc=31.79%\n",
      "Epoch 1 Train batch 3170/3877 Loss=3.8610 Acc=31.80%\n",
      "Epoch 1 Train batch 3180/3877 Loss=3.8597 Acc=31.80%\n",
      "Epoch 1 Train batch 3190/3877 Loss=3.8585 Acc=31.81%\n",
      "Epoch 1 Train batch 3200/3877 Loss=3.8575 Acc=31.81%\n",
      "Epoch 1 Train batch 3210/3877 Loss=3.8561 Acc=31.83%\n",
      "Epoch 1 Train batch 3220/3877 Loss=3.8547 Acc=31.84%\n",
      "Epoch 1 Train batch 3230/3877 Loss=3.8533 Acc=31.85%\n",
      "Epoch 1 Train batch 3240/3877 Loss=3.8522 Acc=31.85%\n",
      "Epoch 1 Train batch 3250/3877 Loss=3.8508 Acc=31.86%\n",
      "Epoch 1 Train batch 3260/3877 Loss=3.8494 Acc=31.88%\n",
      "Epoch 1 Train batch 3270/3877 Loss=3.8486 Acc=31.88%\n",
      "Epoch 1 Train batch 3280/3877 Loss=3.8476 Acc=31.89%\n",
      "Epoch 1 Train batch 3290/3877 Loss=3.8464 Acc=31.90%\n",
      "Epoch 1 Train batch 3300/3877 Loss=3.8456 Acc=31.90%\n",
      "Epoch 1 Train batch 3310/3877 Loss=3.8447 Acc=31.91%\n",
      "Epoch 1 Train batch 3320/3877 Loss=3.8437 Acc=31.92%\n",
      "Epoch 1 Train batch 3330/3877 Loss=3.8426 Acc=31.93%\n",
      "Epoch 1 Train batch 3340/3877 Loss=3.8416 Acc=31.93%\n",
      "Epoch 1 Train batch 3350/3877 Loss=3.8406 Acc=31.94%\n",
      "Epoch 1 Train batch 3360/3877 Loss=3.8400 Acc=31.94%\n",
      "Epoch 1 Train batch 3370/3877 Loss=3.8390 Acc=31.95%\n",
      "Epoch 1 Train batch 3380/3877 Loss=3.8380 Acc=31.95%\n",
      "Epoch 1 Train batch 3390/3877 Loss=3.8369 Acc=31.96%\n",
      "Epoch 1 Train batch 3400/3877 Loss=3.8358 Acc=31.97%\n",
      "Epoch 1 Train batch 3410/3877 Loss=3.8347 Acc=31.98%\n",
      "Epoch 1 Train batch 3420/3877 Loss=3.8336 Acc=31.99%\n",
      "Epoch 1 Train batch 3430/3877 Loss=3.8327 Acc=31.99%\n",
      "Epoch 1 Train batch 3440/3877 Loss=3.8319 Acc=32.00%\n",
      "Epoch 1 Train batch 3450/3877 Loss=3.8309 Acc=32.01%\n",
      "Epoch 1 Train batch 3460/3877 Loss=3.8300 Acc=32.01%\n",
      "Epoch 1 Train batch 3470/3877 Loss=3.8289 Acc=32.02%\n",
      "Epoch 1 Train batch 3480/3877 Loss=3.8281 Acc=32.03%\n",
      "Epoch 1 Train batch 3490/3877 Loss=3.8268 Acc=32.04%\n",
      "Epoch 1 Train batch 3500/3877 Loss=3.8260 Acc=32.04%\n",
      "Epoch 1 Train batch 3510/3877 Loss=3.8248 Acc=32.05%\n",
      "Epoch 1 Train batch 3520/3877 Loss=3.8239 Acc=32.06%\n",
      "Epoch 1 Train batch 3530/3877 Loss=3.8231 Acc=32.06%\n",
      "Epoch 1 Train batch 3540/3877 Loss=3.8220 Acc=32.07%\n",
      "Epoch 1 Train batch 3550/3877 Loss=3.8209 Acc=32.08%\n",
      "Epoch 1 Train batch 3560/3877 Loss=3.8198 Acc=32.09%\n",
      "Epoch 1 Train batch 3570/3877 Loss=3.8187 Acc=32.10%\n",
      "Epoch 1 Train batch 3580/3877 Loss=3.8178 Acc=32.10%\n",
      "Epoch 1 Train batch 3590/3877 Loss=3.8168 Acc=32.11%\n",
      "Epoch 1 Train batch 3600/3877 Loss=3.8161 Acc=32.12%\n",
      "Epoch 1 Train batch 3610/3877 Loss=3.8151 Acc=32.13%\n",
      "Epoch 1 Train batch 3620/3877 Loss=3.8140 Acc=32.13%\n",
      "Epoch 1 Train batch 3630/3877 Loss=3.8129 Acc=32.14%\n",
      "Epoch 1 Train batch 3640/3877 Loss=3.8119 Acc=32.15%\n",
      "Epoch 1 Train batch 3650/3877 Loss=3.8110 Acc=32.16%\n",
      "Epoch 1 Train batch 3660/3877 Loss=3.8098 Acc=32.17%\n",
      "Epoch 1 Train batch 3670/3877 Loss=3.8088 Acc=32.18%\n",
      "Epoch 1 Train batch 3680/3877 Loss=3.8080 Acc=32.18%\n",
      "Epoch 1 Train batch 3690/3877 Loss=3.8072 Acc=32.19%\n",
      "Epoch 1 Train batch 3700/3877 Loss=3.8063 Acc=32.20%\n",
      "Epoch 1 Train batch 3710/3877 Loss=3.8051 Acc=32.21%\n",
      "Epoch 1 Train batch 3720/3877 Loss=3.8042 Acc=32.21%\n",
      "Epoch 1 Train batch 3730/3877 Loss=3.8034 Acc=32.22%\n",
      "Epoch 1 Train batch 3740/3877 Loss=3.8027 Acc=32.22%\n",
      "Epoch 1 Train batch 3750/3877 Loss=3.8017 Acc=32.23%\n",
      "Epoch 1 Train batch 3760/3877 Loss=3.8008 Acc=32.24%\n",
      "Epoch 1 Train batch 3770/3877 Loss=3.7997 Acc=32.25%\n",
      "Epoch 1 Train batch 3780/3877 Loss=3.7986 Acc=32.26%\n",
      "Epoch 1 Train batch 3790/3877 Loss=3.7975 Acc=32.27%\n",
      "Epoch 1 Train batch 3800/3877 Loss=3.7964 Acc=32.27%\n",
      "Epoch 1 Train batch 3810/3877 Loss=3.7954 Acc=32.28%\n",
      "Epoch 1 Train batch 3820/3877 Loss=3.7945 Acc=32.29%\n",
      "Epoch 1 Train batch 3830/3877 Loss=3.7936 Acc=32.29%\n",
      "Epoch 1 Train batch 3840/3877 Loss=3.7928 Acc=32.30%\n",
      "Epoch 1 Train batch 3850/3877 Loss=3.7918 Acc=32.31%\n",
      "Epoch 1 Train batch 3860/3877 Loss=3.7909 Acc=32.31%\n",
      "Epoch 1 Train batch 3870/3877 Loss=3.7901 Acc=32.32%\n",
      "Epoch 1 Train batch 3877/3877 Loss=3.7894 Acc=32.32%\n",
      "Epoch 1/5 train_loss=3.7894 train_acc=32.32% val_loss=3.3905 val_acc=35.54%\n",
      "Epoch 2 Train batch 10/3877 Loss=3.3811 Acc=34.49%\n",
      "Epoch 2 Train batch 20/3877 Loss=3.3486 Acc=35.10%\n",
      "Epoch 2 Train batch 30/3877 Loss=3.3183 Acc=35.45%\n",
      "Epoch 2 Train batch 40/3877 Loss=3.3344 Acc=35.35%\n",
      "Epoch 2 Train batch 50/3877 Loss=3.3350 Acc=35.38%\n",
      "Epoch 2 Train batch 60/3877 Loss=3.3321 Acc=35.48%\n",
      "Epoch 2 Train batch 70/3877 Loss=3.3283 Acc=35.47%\n",
      "Epoch 2 Train batch 80/3877 Loss=3.3290 Acc=35.41%\n",
      "Epoch 2 Train batch 90/3877 Loss=3.3317 Acc=35.43%\n",
      "Epoch 2 Train batch 100/3877 Loss=3.3249 Acc=35.53%\n",
      "Epoch 2 Train batch 110/3877 Loss=3.3382 Acc=35.44%\n",
      "Epoch 2 Train batch 120/3877 Loss=3.3383 Acc=35.40%\n",
      "Epoch 2 Train batch 130/3877 Loss=3.3397 Acc=35.39%\n",
      "Epoch 2 Train batch 140/3877 Loss=3.3435 Acc=35.44%\n",
      "Epoch 2 Train batch 150/3877 Loss=3.3465 Acc=35.43%\n",
      "Epoch 2 Train batch 160/3877 Loss=3.3449 Acc=35.44%\n",
      "Epoch 2 Train batch 170/3877 Loss=3.3425 Acc=35.51%\n",
      "Epoch 2 Train batch 180/3877 Loss=3.3410 Acc=35.58%\n",
      "Epoch 2 Train batch 190/3877 Loss=3.3411 Acc=35.61%\n",
      "Epoch 2 Train batch 200/3877 Loss=3.3435 Acc=35.59%\n",
      "Epoch 2 Train batch 210/3877 Loss=3.3408 Acc=35.60%\n",
      "Epoch 2 Train batch 220/3877 Loss=3.3415 Acc=35.57%\n",
      "Epoch 2 Train batch 230/3877 Loss=3.3458 Acc=35.52%\n",
      "Epoch 2 Train batch 240/3877 Loss=3.3481 Acc=35.51%\n",
      "Epoch 2 Train batch 250/3877 Loss=3.3519 Acc=35.49%\n",
      "Epoch 2 Train batch 260/3877 Loss=3.3470 Acc=35.55%\n",
      "Epoch 2 Train batch 270/3877 Loss=3.3461 Acc=35.56%\n",
      "Epoch 2 Train batch 280/3877 Loss=3.3481 Acc=35.54%\n",
      "Epoch 2 Train batch 290/3877 Loss=3.3492 Acc=35.52%\n",
      "Epoch 2 Train batch 300/3877 Loss=3.3474 Acc=35.56%\n",
      "Epoch 2 Train batch 310/3877 Loss=3.3477 Acc=35.55%\n",
      "Epoch 2 Train batch 320/3877 Loss=3.3518 Acc=35.48%\n",
      "Epoch 2 Train batch 330/3877 Loss=3.3489 Acc=35.52%\n",
      "Epoch 2 Train batch 340/3877 Loss=3.3499 Acc=35.52%\n",
      "Epoch 2 Train batch 350/3877 Loss=3.3513 Acc=35.50%\n",
      "Epoch 2 Train batch 360/3877 Loss=3.3504 Acc=35.50%\n",
      "Epoch 2 Train batch 370/3877 Loss=3.3517 Acc=35.48%\n",
      "Epoch 2 Train batch 380/3877 Loss=3.3520 Acc=35.47%\n",
      "Epoch 2 Train batch 390/3877 Loss=3.3494 Acc=35.48%\n",
      "Epoch 2 Train batch 400/3877 Loss=3.3488 Acc=35.46%\n",
      "Epoch 2 Train batch 410/3877 Loss=3.3495 Acc=35.46%\n",
      "Epoch 2 Train batch 420/3877 Loss=3.3484 Acc=35.47%\n",
      "Epoch 2 Train batch 430/3877 Loss=3.3480 Acc=35.48%\n",
      "Epoch 2 Train batch 440/3877 Loss=3.3485 Acc=35.48%\n",
      "Epoch 2 Train batch 450/3877 Loss=3.3499 Acc=35.44%\n",
      "Epoch 2 Train batch 460/3877 Loss=3.3492 Acc=35.46%\n",
      "Epoch 2 Train batch 470/3877 Loss=3.3501 Acc=35.43%\n",
      "Epoch 2 Train batch 480/3877 Loss=3.3513 Acc=35.41%\n",
      "Epoch 2 Train batch 490/3877 Loss=3.3533 Acc=35.38%\n",
      "Epoch 2 Train batch 500/3877 Loss=3.3510 Acc=35.39%\n",
      "Epoch 2 Train batch 510/3877 Loss=3.3508 Acc=35.40%\n",
      "Epoch 2 Train batch 520/3877 Loss=3.3516 Acc=35.37%\n",
      "Epoch 2 Train batch 530/3877 Loss=3.3510 Acc=35.39%\n",
      "Epoch 2 Train batch 540/3877 Loss=3.3494 Acc=35.39%\n",
      "Epoch 2 Train batch 550/3877 Loss=3.3499 Acc=35.40%\n",
      "Epoch 2 Train batch 560/3877 Loss=3.3506 Acc=35.38%\n",
      "Epoch 2 Train batch 570/3877 Loss=3.3500 Acc=35.39%\n",
      "Epoch 2 Train batch 580/3877 Loss=3.3490 Acc=35.41%\n",
      "Epoch 2 Train batch 590/3877 Loss=3.3475 Acc=35.43%\n",
      "Epoch 2 Train batch 600/3877 Loss=3.3478 Acc=35.45%\n",
      "Epoch 2 Train batch 610/3877 Loss=3.3473 Acc=35.45%\n",
      "Epoch 2 Train batch 620/3877 Loss=3.3470 Acc=35.44%\n",
      "Epoch 2 Train batch 630/3877 Loss=3.3476 Acc=35.44%\n",
      "Epoch 2 Train batch 640/3877 Loss=3.3477 Acc=35.43%\n",
      "Epoch 2 Train batch 650/3877 Loss=3.3480 Acc=35.44%\n",
      "Epoch 2 Train batch 660/3877 Loss=3.3475 Acc=35.44%\n",
      "Epoch 2 Train batch 670/3877 Loss=3.3469 Acc=35.44%\n",
      "Epoch 2 Train batch 680/3877 Loss=3.3461 Acc=35.46%\n",
      "Epoch 2 Train batch 690/3877 Loss=3.3457 Acc=35.46%\n",
      "Epoch 2 Train batch 700/3877 Loss=3.3445 Acc=35.48%\n",
      "Epoch 2 Train batch 710/3877 Loss=3.3447 Acc=35.47%\n",
      "Epoch 2 Train batch 720/3877 Loss=3.3430 Acc=35.49%\n",
      "Epoch 2 Train batch 730/3877 Loss=3.3427 Acc=35.48%\n",
      "Epoch 2 Train batch 740/3877 Loss=3.3429 Acc=35.46%\n",
      "Epoch 2 Train batch 750/3877 Loss=3.3424 Acc=35.47%\n",
      "Epoch 2 Train batch 760/3877 Loss=3.3420 Acc=35.48%\n",
      "Epoch 2 Train batch 770/3877 Loss=3.3412 Acc=35.50%\n",
      "Epoch 2 Train batch 780/3877 Loss=3.3421 Acc=35.49%\n",
      "Epoch 2 Train batch 790/3877 Loss=3.3417 Acc=35.49%\n",
      "Epoch 2 Train batch 800/3877 Loss=3.3427 Acc=35.48%\n",
      "Epoch 2 Train batch 810/3877 Loss=3.3416 Acc=35.49%\n",
      "Epoch 2 Train batch 820/3877 Loss=3.3414 Acc=35.49%\n",
      "Epoch 2 Train batch 830/3877 Loss=3.3407 Acc=35.50%\n",
      "Epoch 2 Train batch 840/3877 Loss=3.3406 Acc=35.51%\n",
      "Epoch 2 Train batch 850/3877 Loss=3.3408 Acc=35.49%\n",
      "Epoch 2 Train batch 860/3877 Loss=3.3404 Acc=35.50%\n",
      "Epoch 2 Train batch 870/3877 Loss=3.3396 Acc=35.51%\n",
      "Epoch 2 Train batch 880/3877 Loss=3.3395 Acc=35.52%\n",
      "Epoch 2 Train batch 890/3877 Loss=3.3384 Acc=35.53%\n",
      "Epoch 2 Train batch 900/3877 Loss=3.3388 Acc=35.52%\n",
      "Epoch 2 Train batch 910/3877 Loss=3.3385 Acc=35.52%\n",
      "Epoch 2 Train batch 920/3877 Loss=3.3379 Acc=35.53%\n",
      "Epoch 2 Train batch 930/3877 Loss=3.3380 Acc=35.53%\n",
      "Epoch 2 Train batch 940/3877 Loss=3.3386 Acc=35.52%\n",
      "Epoch 2 Train batch 950/3877 Loss=3.3383 Acc=35.52%\n",
      "Epoch 2 Train batch 960/3877 Loss=3.3378 Acc=35.53%\n",
      "Epoch 2 Train batch 970/3877 Loss=3.3375 Acc=35.54%\n",
      "Epoch 2 Train batch 980/3877 Loss=3.3380 Acc=35.53%\n",
      "Epoch 2 Train batch 990/3877 Loss=3.3388 Acc=35.53%\n",
      "Epoch 2 Train batch 1000/3877 Loss=3.3383 Acc=35.54%\n",
      "Epoch 2 Train batch 1010/3877 Loss=3.3385 Acc=35.53%\n",
      "Epoch 2 Train batch 1020/3877 Loss=3.3384 Acc=35.53%\n",
      "Epoch 2 Train batch 1030/3877 Loss=3.3383 Acc=35.53%\n",
      "Epoch 2 Train batch 1040/3877 Loss=3.3373 Acc=35.53%\n",
      "Epoch 2 Train batch 1050/3877 Loss=3.3372 Acc=35.54%\n",
      "Epoch 2 Train batch 1060/3877 Loss=3.3360 Acc=35.56%\n",
      "Epoch 2 Train batch 1070/3877 Loss=3.3361 Acc=35.56%\n",
      "Epoch 2 Train batch 1080/3877 Loss=3.3353 Acc=35.56%\n",
      "Epoch 2 Train batch 1090/3877 Loss=3.3347 Acc=35.57%\n",
      "Epoch 2 Train batch 1100/3877 Loss=3.3351 Acc=35.57%\n",
      "Epoch 2 Train batch 1110/3877 Loss=3.3346 Acc=35.58%\n",
      "Epoch 2 Train batch 1120/3877 Loss=3.3336 Acc=35.58%\n",
      "Epoch 2 Train batch 1130/3877 Loss=3.3327 Acc=35.59%\n",
      "Epoch 2 Train batch 1140/3877 Loss=3.3322 Acc=35.59%\n",
      "Epoch 2 Train batch 1150/3877 Loss=3.3319 Acc=35.59%\n",
      "Epoch 2 Train batch 1160/3877 Loss=3.3307 Acc=35.61%\n",
      "Epoch 2 Train batch 1170/3877 Loss=3.3307 Acc=35.61%\n",
      "Epoch 2 Train batch 1180/3877 Loss=3.3302 Acc=35.62%\n",
      "Epoch 2 Train batch 1190/3877 Loss=3.3305 Acc=35.61%\n",
      "Epoch 2 Train batch 1200/3877 Loss=3.3303 Acc=35.61%\n",
      "Epoch 2 Train batch 1210/3877 Loss=3.3301 Acc=35.60%\n",
      "Epoch 2 Train batch 1220/3877 Loss=3.3295 Acc=35.61%\n",
      "Epoch 2 Train batch 1230/3877 Loss=3.3296 Acc=35.61%\n",
      "Epoch 2 Train batch 1240/3877 Loss=3.3297 Acc=35.61%\n",
      "Epoch 2 Train batch 1250/3877 Loss=3.3292 Acc=35.61%\n",
      "Epoch 2 Train batch 1260/3877 Loss=3.3284 Acc=35.61%\n",
      "Epoch 2 Train batch 1270/3877 Loss=3.3278 Acc=35.62%\n",
      "Epoch 2 Train batch 1280/3877 Loss=3.3277 Acc=35.61%\n",
      "Epoch 2 Train batch 1290/3877 Loss=3.3270 Acc=35.62%\n",
      "Epoch 2 Train batch 1300/3877 Loss=3.3264 Acc=35.63%\n",
      "Epoch 2 Train batch 1310/3877 Loss=3.3262 Acc=35.64%\n",
      "Epoch 2 Train batch 1320/3877 Loss=3.3258 Acc=35.64%\n",
      "Epoch 2 Train batch 1330/3877 Loss=3.3262 Acc=35.64%\n",
      "Epoch 2 Train batch 1340/3877 Loss=3.3263 Acc=35.65%\n",
      "Epoch 2 Train batch 1350/3877 Loss=3.3254 Acc=35.66%\n",
      "Epoch 2 Train batch 1360/3877 Loss=3.3259 Acc=35.66%\n",
      "Epoch 2 Train batch 1370/3877 Loss=3.3259 Acc=35.66%\n",
      "Epoch 2 Train batch 1380/3877 Loss=3.3265 Acc=35.64%\n",
      "Epoch 2 Train batch 1390/3877 Loss=3.3258 Acc=35.66%\n",
      "Epoch 2 Train batch 1400/3877 Loss=3.3253 Acc=35.66%\n",
      "Epoch 2 Train batch 1410/3877 Loss=3.3249 Acc=35.67%\n",
      "Epoch 2 Train batch 1420/3877 Loss=3.3254 Acc=35.66%\n",
      "Epoch 2 Train batch 1430/3877 Loss=3.3246 Acc=35.66%\n",
      "Epoch 2 Train batch 1440/3877 Loss=3.3243 Acc=35.66%\n",
      "Epoch 2 Train batch 1450/3877 Loss=3.3242 Acc=35.67%\n",
      "Epoch 2 Train batch 1460/3877 Loss=3.3240 Acc=35.67%\n",
      "Epoch 2 Train batch 1470/3877 Loss=3.3235 Acc=35.67%\n",
      "Epoch 2 Train batch 1480/3877 Loss=3.3231 Acc=35.68%\n",
      "Epoch 2 Train batch 1490/3877 Loss=3.3232 Acc=35.67%\n",
      "Epoch 2 Train batch 1500/3877 Loss=3.3237 Acc=35.67%\n",
      "Epoch 2 Train batch 1510/3877 Loss=3.3235 Acc=35.67%\n",
      "Epoch 2 Train batch 1520/3877 Loss=3.3238 Acc=35.68%\n",
      "Epoch 2 Train batch 1530/3877 Loss=3.3229 Acc=35.69%\n",
      "Epoch 2 Train batch 1540/3877 Loss=3.3227 Acc=35.69%\n",
      "Epoch 2 Train batch 1550/3877 Loss=3.3223 Acc=35.69%\n",
      "Epoch 2 Train batch 1560/3877 Loss=3.3221 Acc=35.70%\n",
      "Epoch 2 Train batch 1570/3877 Loss=3.3220 Acc=35.70%\n",
      "Epoch 2 Train batch 1580/3877 Loss=3.3216 Acc=35.70%\n",
      "Epoch 2 Train batch 1590/3877 Loss=3.3217 Acc=35.70%\n",
      "Epoch 2 Train batch 1600/3877 Loss=3.3213 Acc=35.71%\n",
      "Epoch 2 Train batch 1610/3877 Loss=3.3211 Acc=35.71%\n",
      "Epoch 2 Train batch 1620/3877 Loss=3.3212 Acc=35.70%\n",
      "Epoch 2 Train batch 1630/3877 Loss=3.3217 Acc=35.70%\n",
      "Epoch 2 Train batch 1640/3877 Loss=3.3216 Acc=35.71%\n",
      "Epoch 2 Train batch 1650/3877 Loss=3.3213 Acc=35.71%\n",
      "Epoch 2 Train batch 1660/3877 Loss=3.3214 Acc=35.71%\n",
      "Epoch 2 Train batch 1670/3877 Loss=3.3214 Acc=35.71%\n",
      "Epoch 2 Train batch 1680/3877 Loss=3.3216 Acc=35.71%\n",
      "Epoch 2 Train batch 1690/3877 Loss=3.3217 Acc=35.71%\n",
      "Epoch 2 Train batch 1700/3877 Loss=3.3214 Acc=35.72%\n",
      "Epoch 2 Train batch 1710/3877 Loss=3.3206 Acc=35.72%\n",
      "Epoch 2 Train batch 1720/3877 Loss=3.3201 Acc=35.73%\n",
      "Epoch 2 Train batch 1730/3877 Loss=3.3201 Acc=35.73%\n",
      "Epoch 2 Train batch 1740/3877 Loss=3.3199 Acc=35.73%\n",
      "Epoch 2 Train batch 1750/3877 Loss=3.3200 Acc=35.73%\n",
      "Epoch 2 Train batch 1760/3877 Loss=3.3200 Acc=35.73%\n",
      "Epoch 2 Train batch 1770/3877 Loss=3.3202 Acc=35.73%\n",
      "Epoch 2 Train batch 1780/3877 Loss=3.3198 Acc=35.74%\n",
      "Epoch 2 Train batch 1790/3877 Loss=3.3193 Acc=35.74%\n",
      "Epoch 2 Train batch 1800/3877 Loss=3.3188 Acc=35.75%\n",
      "Epoch 2 Train batch 1810/3877 Loss=3.3183 Acc=35.75%\n",
      "Epoch 2 Train batch 1820/3877 Loss=3.3180 Acc=35.76%\n",
      "Epoch 2 Train batch 1830/3877 Loss=3.3177 Acc=35.75%\n",
      "Epoch 2 Train batch 1840/3877 Loss=3.3177 Acc=35.75%\n",
      "Epoch 2 Train batch 1850/3877 Loss=3.3173 Acc=35.75%\n",
      "Epoch 2 Train batch 1860/3877 Loss=3.3170 Acc=35.76%\n",
      "Epoch 2 Train batch 1870/3877 Loss=3.3167 Acc=35.76%\n",
      "Epoch 2 Train batch 1880/3877 Loss=3.3167 Acc=35.76%\n",
      "Epoch 2 Train batch 1890/3877 Loss=3.3163 Acc=35.76%\n",
      "Epoch 2 Train batch 1900/3877 Loss=3.3161 Acc=35.76%\n",
      "Epoch 2 Train batch 1910/3877 Loss=3.3159 Acc=35.77%\n",
      "Epoch 2 Train batch 1920/3877 Loss=3.3161 Acc=35.76%\n",
      "Epoch 2 Train batch 1930/3877 Loss=3.3165 Acc=35.77%\n",
      "Epoch 2 Train batch 1940/3877 Loss=3.3163 Acc=35.77%\n",
      "Epoch 2 Train batch 1950/3877 Loss=3.3162 Acc=35.77%\n",
      "Epoch 2 Train batch 1960/3877 Loss=3.3161 Acc=35.77%\n",
      "Epoch 2 Train batch 1970/3877 Loss=3.3157 Acc=35.77%\n",
      "Epoch 2 Train batch 1980/3877 Loss=3.3156 Acc=35.77%\n",
      "Epoch 2 Train batch 1990/3877 Loss=3.3154 Acc=35.77%\n",
      "Epoch 2 Train batch 2000/3877 Loss=3.3153 Acc=35.77%\n",
      "Epoch 2 Train batch 2010/3877 Loss=3.3151 Acc=35.78%\n",
      "Epoch 2 Train batch 2020/3877 Loss=3.3147 Acc=35.79%\n",
      "Epoch 2 Train batch 2030/3877 Loss=3.3148 Acc=35.79%\n",
      "Epoch 2 Train batch 2040/3877 Loss=3.3148 Acc=35.79%\n",
      "Epoch 2 Train batch 2050/3877 Loss=3.3143 Acc=35.79%\n",
      "Epoch 2 Train batch 2060/3877 Loss=3.3142 Acc=35.80%\n",
      "Epoch 2 Train batch 2070/3877 Loss=3.3139 Acc=35.80%\n",
      "Epoch 2 Train batch 2080/3877 Loss=3.3134 Acc=35.81%\n",
      "Epoch 2 Train batch 2090/3877 Loss=3.3131 Acc=35.81%\n",
      "Epoch 2 Train batch 2100/3877 Loss=3.3127 Acc=35.81%\n",
      "Epoch 2 Train batch 2110/3877 Loss=3.3121 Acc=35.82%\n",
      "Epoch 2 Train batch 2120/3877 Loss=3.3121 Acc=35.82%\n",
      "Epoch 2 Train batch 2130/3877 Loss=3.3118 Acc=35.82%\n",
      "Epoch 2 Train batch 2140/3877 Loss=3.3117 Acc=35.83%\n",
      "Epoch 2 Train batch 2150/3877 Loss=3.3113 Acc=35.83%\n",
      "Epoch 2 Train batch 2160/3877 Loss=3.3111 Acc=35.83%\n",
      "Epoch 2 Train batch 2170/3877 Loss=3.3110 Acc=35.83%\n",
      "Epoch 2 Train batch 2180/3877 Loss=3.3112 Acc=35.83%\n",
      "Epoch 2 Train batch 2190/3877 Loss=3.3110 Acc=35.83%\n",
      "Epoch 2 Train batch 2200/3877 Loss=3.3109 Acc=35.83%\n",
      "Epoch 2 Train batch 2210/3877 Loss=3.3109 Acc=35.83%\n",
      "Epoch 2 Train batch 2220/3877 Loss=3.3103 Acc=35.83%\n",
      "Epoch 2 Train batch 2230/3877 Loss=3.3101 Acc=35.84%\n",
      "Epoch 2 Train batch 2240/3877 Loss=3.3103 Acc=35.84%\n",
      "Epoch 2 Train batch 2250/3877 Loss=3.3100 Acc=35.84%\n",
      "Epoch 2 Train batch 2260/3877 Loss=3.3103 Acc=35.84%\n",
      "Epoch 2 Train batch 2270/3877 Loss=3.3103 Acc=35.84%\n",
      "Epoch 2 Train batch 2280/3877 Loss=3.3105 Acc=35.84%\n",
      "Epoch 2 Train batch 2290/3877 Loss=3.3105 Acc=35.84%\n",
      "Epoch 2 Train batch 2300/3877 Loss=3.3104 Acc=35.84%\n",
      "Epoch 2 Train batch 2310/3877 Loss=3.3102 Acc=35.85%\n",
      "Epoch 2 Train batch 2320/3877 Loss=3.3099 Acc=35.85%\n",
      "Epoch 2 Train batch 2330/3877 Loss=3.3101 Acc=35.85%\n",
      "Epoch 2 Train batch 2340/3877 Loss=3.3097 Acc=35.85%\n",
      "Epoch 2 Train batch 2350/3877 Loss=3.3097 Acc=35.86%\n",
      "Epoch 2 Train batch 2360/3877 Loss=3.3093 Acc=35.86%\n",
      "Epoch 2 Train batch 2370/3877 Loss=3.3091 Acc=35.86%\n",
      "Epoch 2 Train batch 2380/3877 Loss=3.3095 Acc=35.86%\n",
      "Epoch 2 Train batch 2390/3877 Loss=3.3092 Acc=35.86%\n",
      "Epoch 2 Train batch 2400/3877 Loss=3.3089 Acc=35.87%\n",
      "Epoch 2 Train batch 2410/3877 Loss=3.3087 Acc=35.87%\n",
      "Epoch 2 Train batch 2420/3877 Loss=3.3083 Acc=35.87%\n",
      "Epoch 2 Train batch 2430/3877 Loss=3.3078 Acc=35.88%\n",
      "Epoch 2 Train batch 2440/3877 Loss=3.3079 Acc=35.88%\n",
      "Epoch 2 Train batch 2450/3877 Loss=3.3078 Acc=35.88%\n",
      "Epoch 2 Train batch 2460/3877 Loss=3.3077 Acc=35.88%\n",
      "Epoch 2 Train batch 2470/3877 Loss=3.3080 Acc=35.88%\n",
      "Epoch 2 Train batch 2480/3877 Loss=3.3079 Acc=35.87%\n",
      "Epoch 2 Train batch 2490/3877 Loss=3.3079 Acc=35.88%\n",
      "Epoch 2 Train batch 2500/3877 Loss=3.3078 Acc=35.87%\n",
      "Epoch 2 Train batch 2510/3877 Loss=3.3075 Acc=35.88%\n",
      "Epoch 2 Train batch 2520/3877 Loss=3.3073 Acc=35.88%\n",
      "Epoch 2 Train batch 2530/3877 Loss=3.3070 Acc=35.88%\n",
      "Epoch 2 Train batch 2540/3877 Loss=3.3066 Acc=35.88%\n",
      "Epoch 2 Train batch 2550/3877 Loss=3.3063 Acc=35.88%\n",
      "Epoch 2 Train batch 2560/3877 Loss=3.3057 Acc=35.89%\n",
      "Epoch 2 Train batch 2570/3877 Loss=3.3052 Acc=35.89%\n",
      "Epoch 2 Train batch 2580/3877 Loss=3.3048 Acc=35.89%\n",
      "Epoch 2 Train batch 2590/3877 Loss=3.3048 Acc=35.89%\n",
      "Epoch 2 Train batch 2600/3877 Loss=3.3049 Acc=35.90%\n",
      "Epoch 2 Train batch 2610/3877 Loss=3.3047 Acc=35.90%\n",
      "Epoch 2 Train batch 2620/3877 Loss=3.3044 Acc=35.90%\n",
      "Epoch 2 Train batch 2630/3877 Loss=3.3045 Acc=35.90%\n",
      "Epoch 2 Train batch 2640/3877 Loss=3.3044 Acc=35.90%\n",
      "Epoch 2 Train batch 2650/3877 Loss=3.3046 Acc=35.89%\n",
      "Epoch 2 Train batch 2660/3877 Loss=3.3045 Acc=35.90%\n",
      "Epoch 2 Train batch 2670/3877 Loss=3.3044 Acc=35.90%\n",
      "Epoch 2 Train batch 2680/3877 Loss=3.3041 Acc=35.90%\n",
      "Epoch 2 Train batch 2690/3877 Loss=3.3038 Acc=35.90%\n",
      "Epoch 2 Train batch 2700/3877 Loss=3.3036 Acc=35.90%\n",
      "Epoch 2 Train batch 2710/3877 Loss=3.3035 Acc=35.90%\n",
      "Epoch 2 Train batch 2720/3877 Loss=3.3034 Acc=35.90%\n",
      "Epoch 2 Train batch 2730/3877 Loss=3.3032 Acc=35.90%\n",
      "Epoch 2 Train batch 2740/3877 Loss=3.3031 Acc=35.91%\n",
      "Epoch 2 Train batch 2750/3877 Loss=3.3028 Acc=35.91%\n",
      "Epoch 2 Train batch 2760/3877 Loss=3.3025 Acc=35.91%\n",
      "Epoch 2 Train batch 2770/3877 Loss=3.3022 Acc=35.91%\n",
      "Epoch 2 Train batch 2780/3877 Loss=3.3016 Acc=35.92%\n",
      "Epoch 2 Train batch 2790/3877 Loss=3.3011 Acc=35.92%\n",
      "Epoch 2 Train batch 2800/3877 Loss=3.3011 Acc=35.93%\n",
      "Epoch 2 Train batch 2810/3877 Loss=3.3010 Acc=35.93%\n",
      "Epoch 2 Train batch 2820/3877 Loss=3.3009 Acc=35.93%\n",
      "Epoch 2 Train batch 2830/3877 Loss=3.3008 Acc=35.93%\n",
      "Epoch 2 Train batch 2840/3877 Loss=3.3006 Acc=35.93%\n",
      "Epoch 2 Train batch 2850/3877 Loss=3.3006 Acc=35.93%\n",
      "Epoch 2 Train batch 2860/3877 Loss=3.3003 Acc=35.94%\n",
      "Epoch 2 Train batch 2870/3877 Loss=3.3001 Acc=35.94%\n",
      "Epoch 2 Train batch 2880/3877 Loss=3.2998 Acc=35.94%\n",
      "Epoch 2 Train batch 2890/3877 Loss=3.2996 Acc=35.94%\n",
      "Epoch 2 Train batch 2900/3877 Loss=3.2996 Acc=35.94%\n",
      "Epoch 2 Train batch 2910/3877 Loss=3.2992 Acc=35.95%\n",
      "Epoch 2 Train batch 2920/3877 Loss=3.2993 Acc=35.95%\n",
      "Epoch 2 Train batch 2930/3877 Loss=3.2991 Acc=35.95%\n",
      "Epoch 2 Train batch 2940/3877 Loss=3.2992 Acc=35.95%\n",
      "Epoch 2 Train batch 2950/3877 Loss=3.2991 Acc=35.96%\n",
      "Epoch 2 Train batch 2960/3877 Loss=3.2988 Acc=35.96%\n",
      "Epoch 2 Train batch 2970/3877 Loss=3.2984 Acc=35.96%\n",
      "Epoch 2 Train batch 2980/3877 Loss=3.2984 Acc=35.96%\n",
      "Epoch 2 Train batch 2990/3877 Loss=3.2981 Acc=35.96%\n",
      "Epoch 2 Train batch 3000/3877 Loss=3.2979 Acc=35.97%\n",
      "Epoch 2 Train batch 3010/3877 Loss=3.2978 Acc=35.97%\n",
      "Epoch 2 Train batch 3020/3877 Loss=3.2977 Acc=35.97%\n",
      "Epoch 2 Train batch 3030/3877 Loss=3.2976 Acc=35.97%\n",
      "Epoch 2 Train batch 3040/3877 Loss=3.2973 Acc=35.98%\n",
      "Epoch 2 Train batch 3050/3877 Loss=3.2968 Acc=35.98%\n",
      "Epoch 2 Train batch 3060/3877 Loss=3.2966 Acc=35.98%\n",
      "Epoch 2 Train batch 3070/3877 Loss=3.2964 Acc=35.98%\n",
      "Epoch 2 Train batch 3080/3877 Loss=3.2963 Acc=35.98%\n",
      "Epoch 2 Train batch 3090/3877 Loss=3.2958 Acc=35.99%\n",
      "Epoch 2 Train batch 3100/3877 Loss=3.2954 Acc=35.99%\n",
      "Epoch 2 Train batch 3110/3877 Loss=3.2950 Acc=36.00%\n",
      "Epoch 2 Train batch 3120/3877 Loss=3.2951 Acc=35.99%\n",
      "Epoch 2 Train batch 3130/3877 Loss=3.2950 Acc=35.99%\n",
      "Epoch 2 Train batch 3140/3877 Loss=3.2947 Acc=36.00%\n",
      "Epoch 2 Train batch 3150/3877 Loss=3.2947 Acc=36.00%\n",
      "Epoch 2 Train batch 3160/3877 Loss=3.2945 Acc=36.00%\n",
      "Epoch 2 Train batch 3170/3877 Loss=3.2942 Acc=36.01%\n",
      "Epoch 2 Train batch 3180/3877 Loss=3.2941 Acc=36.01%\n",
      "Epoch 2 Train batch 3190/3877 Loss=3.2938 Acc=36.01%\n",
      "Epoch 2 Train batch 3200/3877 Loss=3.2940 Acc=36.01%\n",
      "Epoch 2 Train batch 3210/3877 Loss=3.2937 Acc=36.01%\n",
      "Epoch 2 Train batch 3220/3877 Loss=3.2936 Acc=36.02%\n",
      "Epoch 2 Train batch 3230/3877 Loss=3.2934 Acc=36.02%\n",
      "Epoch 2 Train batch 3240/3877 Loss=3.2933 Acc=36.02%\n",
      "Epoch 2 Train batch 3250/3877 Loss=3.2931 Acc=36.02%\n",
      "Epoch 2 Train batch 3260/3877 Loss=3.2931 Acc=36.02%\n",
      "Epoch 2 Train batch 3270/3877 Loss=3.2929 Acc=36.02%\n",
      "Epoch 2 Train batch 3280/3877 Loss=3.2925 Acc=36.02%\n",
      "Epoch 2 Train batch 3290/3877 Loss=3.2923 Acc=36.02%\n",
      "Epoch 2 Train batch 3300/3877 Loss=3.2921 Acc=36.03%\n",
      "Epoch 2 Train batch 3310/3877 Loss=3.2921 Acc=36.03%\n",
      "Epoch 2 Train batch 3320/3877 Loss=3.2918 Acc=36.03%\n",
      "Epoch 2 Train batch 3330/3877 Loss=3.2916 Acc=36.03%\n",
      "Epoch 2 Train batch 3340/3877 Loss=3.2913 Acc=36.04%\n",
      "Epoch 2 Train batch 3350/3877 Loss=3.2910 Acc=36.04%\n",
      "Epoch 2 Train batch 3360/3877 Loss=3.2908 Acc=36.04%\n",
      "Epoch 2 Train batch 3370/3877 Loss=3.2905 Acc=36.04%\n",
      "Epoch 2 Train batch 3380/3877 Loss=3.2904 Acc=36.04%\n",
      "Epoch 2 Train batch 3390/3877 Loss=3.2902 Acc=36.04%\n",
      "Epoch 2 Train batch 3400/3877 Loss=3.2903 Acc=36.04%\n",
      "Epoch 2 Train batch 3410/3877 Loss=3.2903 Acc=36.04%\n",
      "Epoch 2 Train batch 3420/3877 Loss=3.2900 Acc=36.04%\n",
      "Epoch 2 Train batch 3430/3877 Loss=3.2901 Acc=36.04%\n",
      "Epoch 2 Train batch 3440/3877 Loss=3.2899 Acc=36.04%\n",
      "Epoch 2 Train batch 3450/3877 Loss=3.2896 Acc=36.05%\n",
      "Epoch 2 Train batch 3460/3877 Loss=3.2891 Acc=36.05%\n",
      "Epoch 2 Train batch 3470/3877 Loss=3.2888 Acc=36.05%\n",
      "Epoch 2 Train batch 3480/3877 Loss=3.2884 Acc=36.06%\n",
      "Epoch 2 Train batch 3490/3877 Loss=3.2882 Acc=36.06%\n",
      "Epoch 2 Train batch 3500/3877 Loss=3.2880 Acc=36.06%\n",
      "Epoch 2 Train batch 3510/3877 Loss=3.2881 Acc=36.06%\n",
      "Epoch 2 Train batch 3520/3877 Loss=3.2879 Acc=36.06%\n",
      "Epoch 2 Train batch 3530/3877 Loss=3.2874 Acc=36.06%\n",
      "Epoch 2 Train batch 3540/3877 Loss=3.2871 Acc=36.06%\n",
      "Epoch 2 Train batch 3550/3877 Loss=3.2869 Acc=36.07%\n",
      "Epoch 2 Train batch 3560/3877 Loss=3.2868 Acc=36.07%\n",
      "Epoch 2 Train batch 3570/3877 Loss=3.2869 Acc=36.06%\n",
      "Epoch 2 Train batch 3580/3877 Loss=3.2869 Acc=36.06%\n",
      "Epoch 2 Train batch 3590/3877 Loss=3.2869 Acc=36.06%\n",
      "Epoch 2 Train batch 3600/3877 Loss=3.2866 Acc=36.06%\n",
      "Epoch 2 Train batch 3610/3877 Loss=3.2865 Acc=36.06%\n",
      "Epoch 2 Train batch 3620/3877 Loss=3.2863 Acc=36.06%\n",
      "Epoch 2 Train batch 3630/3877 Loss=3.2860 Acc=36.07%\n",
      "Epoch 2 Train batch 3640/3877 Loss=3.2857 Acc=36.07%\n",
      "Epoch 2 Train batch 3650/3877 Loss=3.2858 Acc=36.07%\n",
      "Epoch 2 Train batch 3660/3877 Loss=3.2856 Acc=36.07%\n",
      "Epoch 2 Train batch 3670/3877 Loss=3.2855 Acc=36.07%\n",
      "Epoch 2 Train batch 3680/3877 Loss=3.2853 Acc=36.08%\n",
      "Epoch 2 Train batch 3690/3877 Loss=3.2848 Acc=36.08%\n",
      "Epoch 2 Train batch 3700/3877 Loss=3.2846 Acc=36.09%\n",
      "Epoch 2 Train batch 3710/3877 Loss=3.2845 Acc=36.09%\n",
      "Epoch 2 Train batch 3720/3877 Loss=3.2842 Acc=36.09%\n",
      "Epoch 2 Train batch 3730/3877 Loss=3.2838 Acc=36.10%\n",
      "Epoch 2 Train batch 3740/3877 Loss=3.2836 Acc=36.10%\n",
      "Epoch 2 Train batch 3750/3877 Loss=3.2836 Acc=36.10%\n",
      "Epoch 2 Train batch 3760/3877 Loss=3.2834 Acc=36.10%\n",
      "Epoch 2 Train batch 3770/3877 Loss=3.2832 Acc=36.10%\n",
      "Epoch 2 Train batch 3780/3877 Loss=3.2832 Acc=36.10%\n",
      "Epoch 2 Train batch 3790/3877 Loss=3.2831 Acc=36.10%\n",
      "Epoch 2 Train batch 3800/3877 Loss=3.2829 Acc=36.10%\n",
      "Epoch 2 Train batch 3810/3877 Loss=3.2830 Acc=36.10%\n",
      "Epoch 2 Train batch 3820/3877 Loss=3.2829 Acc=36.10%\n",
      "Epoch 2 Train batch 3830/3877 Loss=3.2828 Acc=36.10%\n",
      "Epoch 2 Train batch 3840/3877 Loss=3.2825 Acc=36.10%\n",
      "Epoch 2 Train batch 3850/3877 Loss=3.2824 Acc=36.10%\n",
      "Epoch 2 Train batch 3860/3877 Loss=3.2821 Acc=36.10%\n",
      "Epoch 2 Train batch 3870/3877 Loss=3.2818 Acc=36.11%\n",
      "Epoch 2 Train batch 3877/3877 Loss=3.2817 Acc=36.11%\n",
      "Epoch 2/5 train_loss=3.2817 train_acc=36.11% val_loss=3.2097 val_acc=37.27%\n",
      "Epoch 3 Train batch 10/3877 Loss=3.0920 Acc=37.74%\n",
      "Epoch 3 Train batch 20/3877 Loss=3.0821 Acc=38.21%\n",
      "Epoch 3 Train batch 30/3877 Loss=3.0816 Acc=38.01%\n",
      "Epoch 3 Train batch 40/3877 Loss=3.0910 Acc=37.67%\n",
      "Epoch 3 Train batch 50/3877 Loss=3.1027 Acc=37.30%\n",
      "Epoch 3 Train batch 60/3877 Loss=3.1110 Acc=37.29%\n",
      "Epoch 3 Train batch 70/3877 Loss=3.1091 Acc=37.28%\n",
      "Epoch 3 Train batch 80/3877 Loss=3.0896 Acc=37.60%\n",
      "Epoch 3 Train batch 90/3877 Loss=3.0899 Acc=37.52%\n",
      "Epoch 3 Train batch 100/3877 Loss=3.0918 Acc=37.59%\n",
      "Epoch 3 Train batch 110/3877 Loss=3.0924 Acc=37.70%\n",
      "Epoch 3 Train batch 120/3877 Loss=3.0912 Acc=37.71%\n",
      "Epoch 3 Train batch 130/3877 Loss=3.0858 Acc=37.74%\n",
      "Epoch 3 Train batch 140/3877 Loss=3.0863 Acc=37.72%\n",
      "Epoch 3 Train batch 150/3877 Loss=3.0845 Acc=37.73%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 162\u001B[39m\n\u001B[32m    159\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mVocab size:\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(vocab))\n\u001B[32m    161\u001B[39m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m history = \u001B[43mfit_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m    \u001B[49m\u001B[43menc\u001B[49m\u001B[43m=\u001B[49m\u001B[43menc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdec\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43menc_opt\u001B[49m\u001B[43m=\u001B[49m\u001B[43menc_opt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_opt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdec_opt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    166\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    167\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    168\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mOUTPUT_DIR\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mNUM_EPOCHS\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 84\u001B[39m, in \u001B[36mfit_model\u001B[39m\u001B[34m(enc, dec, train_loader, val_loader, enc_opt, dec_opt, device, vocab, output_dir, num_epochs, criterion, print_every, train_encoder_only)\u001B[39m\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m enc_opt: enc_opt.step()\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m train_encoder_only \u001B[38;5;129;01mand\u001B[39;00m dec_opt: dec_opt.step()\n\u001B[32m---> \u001B[39m\u001B[32m84\u001B[39m train_loss_accum += \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     85\u001B[39m train_acc_accum += compute_accuracy(logits, caps[:, \u001B[32m1\u001B[39m:])\n\u001B[32m     86\u001B[39m steps += \u001B[32m1\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 3: Pre-trained ViT as encoder, same text transformer decoder:",
   "id": "f825371530f54d33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Image caption model with transformers, pre-trained encoder.\n",
    "\n",
    "Encoder: ViT with transfer learning from google/vit-base-patch16-224.\n",
    "\n",
    "Decoder: Small text transformer, no transfer learning."
   ],
   "id": "33697f2d30915e76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T23:44:31.753286Z",
     "start_time": "2025-10-20T23:44:02.679745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------- Encoder: ---------------\n",
    "\n",
    "# Load pretrained DINOv3 encoder\n",
    "encoder = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Freeze encoder parameters\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define a projection layer if necessary\n",
    "class EncoderWithProjection(nn.Module):\n",
    "    def __init__(self, encoder, embed_size):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.proj = nn.Linear(encoder.config.hidden_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.encoder(x)\n",
    "        pooled = outputs.pooler_output  # (batch_size, hidden_size)\n",
    "        return self.proj(pooled)\n",
    "\n",
    "# --------------- Training: ---------------\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "EMBED_SIZE = 768\n",
    "\n",
    "OUTPUT_DIR = \"./models_vit_DINOv3_enc\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Instantiate models\n",
    "enc = EncoderWithProjection(\n",
    "    encoder,\n",
    "    embed_size=EMBED_SIZE\n",
    ")\n",
    "\n",
    "# Optimizers and loss\n",
    "enc_opt = torch.optim.Adam(enc.parameters(), lr=LEARNING_RATE)\n",
    "dec_opt = None # decoder frozen\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Train\n",
    "history = fit_model(\n",
    "    enc=enc, dec=dec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    enc_opt=enc_opt, dec_opt=dec_opt,\n",
    "    device=DEVICE,\n",
    "    vocab=vocab,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_encoder_only=True\n",
    ")"
   ],
   "id": "ac8f5b46562fe1f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 6989\n",
      "Epoch 1 Train batch 10/3877 Loss=3.2261 Acc=36.74%\n",
      "Epoch 1 Train batch 20/3877 Loss=3.1950 Acc=36.39%\n",
      "Epoch 1 Train batch 30/3877 Loss=3.1916 Acc=36.14%\n",
      "Epoch 1 Train batch 40/3877 Loss=3.1947 Acc=36.23%\n",
      "Epoch 1 Train batch 50/3877 Loss=3.1856 Acc=36.38%\n",
      "Epoch 1 Train batch 60/3877 Loss=3.1844 Acc=36.63%\n",
      "Epoch 1 Train batch 70/3877 Loss=3.1682 Acc=36.90%\n",
      "Epoch 1 Train batch 80/3877 Loss=3.1688 Acc=36.92%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 51\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mVocab size:\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(vocab))\n\u001B[32m     50\u001B[39m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m history = \u001B[43mfit_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[43menc\u001B[49m\u001B[43m=\u001B[49m\u001B[43menc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdec\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[43menc_opt\u001B[49m\u001B[43m=\u001B[49m\u001B[43menc_opt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_opt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdec_opt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mOUTPUT_DIR\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mNUM_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_encoder_only\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     60\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 69\u001B[39m, in \u001B[36mfit_model\u001B[39m\u001B[34m(enc, dec, train_loader, val_loader, enc_opt, dec_opt, device, vocab, output_dir, num_epochs, criterion, print_every, train_encoder_only)\u001B[39m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (images, caps, _) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader, \u001B[32m1\u001B[39m):\n\u001B[32m     68\u001B[39m     images, caps = images.to(device), caps.to(device)\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m     features = \u001B[43menc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m     logits = dec(features, caps[:, :-\u001B[32m1\u001B[39m])\n\u001B[32m     71\u001B[39m     loss = criterion(logits.reshape(-\u001B[32m1\u001B[39m, logits.size(-\u001B[32m1\u001B[39m)), caps[:, \u001B[32m1\u001B[39m:].reshape(-\u001B[32m1\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 18\u001B[39m, in \u001B[36mEncoderWithProjection.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m     pooled = outputs.pooler_output  \u001B[38;5;66;03m# (batch_size, hidden_size)\u001B[39;00m\n\u001B[32m     20\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.proj(pooled)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001B[39m, in \u001B[36mcheck_model_inputs.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1061\u001B[39m                 monkey_patched_layers.append((module, original_forward))\n\u001B[32m   1063\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m     outputs = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m original_exception:\n\u001B[32m   1066\u001B[39m     \u001B[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001B[39;00m\n\u001B[32m   1067\u001B[39m     \u001B[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001B[39;00m\n\u001B[32m   1068\u001B[39m     \u001B[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001B[39;00m\n\u001B[32m   1069\u001B[39m     kwargs_without_recordable = {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs.items() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m recordable_keys}\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:485\u001B[39m, in \u001B[36mViTModel.forward\u001B[39m\u001B[34m(self, pixel_values, bool_masked_pos, head_mask, interpolate_pos_encoding, **kwargs)\u001B[39m\n\u001B[32m    479\u001B[39m     pixel_values = pixel_values.to(expected_dtype)\n\u001B[32m    481\u001B[39m embedding_output = \u001B[38;5;28mself\u001B[39m.embeddings(\n\u001B[32m    482\u001B[39m     pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n\u001B[32m    483\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m485\u001B[39m encoder_outputs: BaseModelOutput = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    487\u001B[39m sequence_output = encoder_outputs.last_hidden_state\n\u001B[32m    488\u001B[39m sequence_output = \u001B[38;5;28mself\u001B[39m.layernorm(sequence_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:368\u001B[39m, in \u001B[36mViTEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask)\u001B[39m\n\u001B[32m    366\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, layer_module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.layer):\n\u001B[32m    367\u001B[39m     layer_head_mask = head_mask[i] \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m368\u001B[39m     hidden_states = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m BaseModelOutput(last_hidden_state=hidden_states)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning_once(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:353\u001B[39m, in \u001B[36mViTLayer.forward\u001B[39m\u001B[34m(self, hidden_states, head_mask)\u001B[39m\n\u001B[32m    350\u001B[39m layer_output = \u001B[38;5;28mself\u001B[39m.intermediate(layer_output)\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# second residual connection is done here\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m layer_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:322\u001B[39m, in \u001B[36mViTOutput.forward\u001B[39m\u001B[34m(self, hidden_states, input_tensor)\u001B[39m\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    323\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.dropout(hidden_states)\n\u001B[32m    324\u001B[39m     hidden_states = hidden_states + input_tensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
