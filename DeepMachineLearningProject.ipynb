{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Machine Learning Project (SSY340)\n",
    "\n",
    "Project Group 92"
   ],
   "id": "755ce3c68a828da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment setup",
   "id": "7ab7239d695adf25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re, random, os, ast, gc, torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "IMAGES_DIR = \"flickr30k-images\"\n",
    "CAPTIONS_FILE = \"Flickr30k.token.txt\"\n",
    "SEED = 0\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ],
   "id": "81b8df0429de0732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup dataset and image-caption:",
   "id": "50c62e26d9f6e174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "csv_path = \"flickr_annotations_30k.csv\"\n",
    "out_path = \"Flickr30k.token.txt\"\n",
    "\n",
    "df = pd.read_csv(csv_path, low_memory=False)\n",
    "print(\"Columns in CSV:\", list(df.columns))\n",
    "display(df.head(3))\n",
    "\n",
    "def _get_image_name(row):\n",
    "    # common img filename columns; adjust or add if your file uses another name\n",
    "    for col in ('file_name','filename','image','img','image_filename','img_name','image_name','img_id','image_id','path'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            return os.path.basename(str(val))\n",
    "    # fallback: use index as filename\n",
    "    return f\"{row.name}.jpg\"\n",
    "\n",
    "def _get_captions(row):\n",
    "    # common caption columns\n",
    "    for col in ('raw','captions','sentences','sentence','caption','raw_captions','sentids'):\n",
    "        if col in df.columns:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            # if already a list (some readers may parse JSON)\n",
    "            if isinstance(val, (list, tuple)):\n",
    "                return [str(x).strip() for x in val if str(x).strip()]\n",
    "            # try to parse python/JSON list stored as string\n",
    "            if isinstance(val, str):\n",
    "                # attempt ast literal_eval for list-like strings\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, (list, tuple)):\n",
    "                        return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "                    if isinstance(parsed, dict) and 'raw' in parsed:\n",
    "                        r = parsed['raw']\n",
    "                        if isinstance(r, (list, tuple)):\n",
    "                            return [str(x).strip() for x in r if str(x).strip()]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # common separators\n",
    "                for sep in ('|||', '||', '\\n'):\n",
    "                    if sep in val:\n",
    "                        return [s.strip() for s in val.split(sep) if s.strip()]\n",
    "                # otherwise treat as single caption string\n",
    "                return [val.strip()]\n",
    "    return []\n",
    "\n",
    "# Write token file\n",
    "with open(out_path, 'w', encoding='utf-8') as fout:\n",
    "    for _, row in df.iterrows():\n",
    "        img_name = _get_image_name(row)\n",
    "        caps = _get_captions(row)\n",
    "        if not caps:\n",
    "            continue\n",
    "        for i, c in enumerate(caps):\n",
    "            fout.write(f\"{img_name}#{i}\\t{c}\\n\")\n",
    "\n",
    "print(\"Wrote\", out_path)"
   ],
   "id": "8eb6d435fed5dbeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset image-caption test",
   "id": "ce31a759b05954fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "IMAGES_DIR = \"flickr30k-images\"\n",
    "TOKENS_FILE = \"Flickr30k.token.txt\"\n",
    "NUM_SAMPLES = 5\n",
    "\n",
    "image_to_caps = defaultdict(list)\n",
    "with open(TOKENS_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        img_token, caption = line.split('\\t', 1)\n",
    "        img_name = img_token.split('#')[0]\n",
    "        image_to_caps[img_name].append(caption)\n",
    "\n",
    "print(f\"Loaded {len(image_to_caps)} images from token file.\")\n",
    "\n",
    "sampled_imgs = random.sample(list(image_to_caps.keys()), min(NUM_SAMPLES, len(image_to_caps)))\n",
    "\n",
    "for img_name in sampled_imgs:\n",
    "    img_path = os.path.join(IMAGES_DIR, img_name)\n",
    "    caps = image_to_caps[img_name]\n",
    "\n",
    "    # show image\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(img_name)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{img_name} — {len(caps)} captions:\")\n",
    "    for i, c in enumerate(caps):\n",
    "        print(f\"  {i+1}. {c}\")\n",
    "    print(\"-\" * 60)"
   ],
   "id": "f8836560b451d7aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN-RNN model:",
   "id": "3c2c59171b681b76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simple CNN->RNN baseline",
   "id": "6e0057fff467073f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clear cache and GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "MIN_FREQ = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "FINE_TUNE = False\n",
    "OUTPUT_DIR = \"./models_baseline\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------- tokenizer / vocab -----------------\n",
    "\n",
    "def tokenize_caption(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, min_freq=5, reserved=None):\n",
    "        if reserved is None:\n",
    "            reserved = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        self.min_freq = min_freq\n",
    "        self.reserved = reserved\n",
    "        self.freq = Counter()\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "\n",
    "    def build(self, token_lists):\n",
    "        for t in token_lists:\n",
    "            self.freq.update(t)\n",
    "        self.itos = list(self.reserved)\n",
    "        for tok, cnt in self.freq.most_common():\n",
    "            if cnt < self.min_freq:\n",
    "                continue\n",
    "            if tok in self.reserved:\n",
    "                continue\n",
    "            self.itos.append(tok)\n",
    "        self.stoi = {tok:i for i,tok in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.stoi.get(t, self.stoi['<unk>']) for t in tokens]\n",
    "\n",
    "# ----------------- Dataset -----------------\n",
    "\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, images_dir, captions_file, vocab=None, transform=None, split='train', train_ratio=0.8, seed=42):\n",
    "        self.images_dir = str(images_dir)\n",
    "        self.transform = transform\n",
    "        image_to_captions = defaultdict(list)\n",
    "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                img_token, cap = parts\n",
    "                img_name = img_token.split('#')[0]\n",
    "                image_to_captions[img_name].append(cap)\n",
    "        available = set(os.listdir(self.images_dir))\n",
    "        self.entries = []\n",
    "        for img, caps in image_to_captions.items():\n",
    "            if img not in available:\n",
    "                continue\n",
    "            for c in caps:\n",
    "                self.entries.append((img, c))\n",
    "        # split images (by unique image) to avoid leakage\n",
    "        images = sorted(list({e[0] for e in self.entries}))\n",
    "        random.Random(seed).shuffle(images)\n",
    "        n_train = int(len(images)*train_ratio)\n",
    "        train_images = set(images[:n_train])\n",
    "        val_images = set(images[n_train:])\n",
    "        if split == 'train':\n",
    "            self.entries = [e for e in self.entries if e[0] in train_images]\n",
    "        elif split == 'val':\n",
    "            self.entries = [e for e in self.entries if e[0] in val_images]\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'val'\")\n",
    "        if vocab is None and split=='train':\n",
    "            token_lists = [tokenize_caption(c) for _,c in self.entries]\n",
    "            self.vocab = Vocab(min_freq=MIN_FREQ)\n",
    "            self.vocab.build(token_lists)\n",
    "        elif vocab is not None:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            raise ValueError(\"Provide vocab for val split\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.entries[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tokens = tokenize_caption(caption)\n",
    "        tokens = ['<start>'] + tokens + ['<end>']\n",
    "        num = torch.tensor(self.vocab.numericalize(tokens), dtype=torch.long)\n",
    "        return image, num\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, caps = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    lengths = [c.size(0) for c in caps]\n",
    "    caps_padded = nn.utils.rnn.pad_sequence(caps, batch_first=True, padding_value=0)\n",
    "    return images, caps_padded, lengths\n",
    "\n",
    "# ----------------- Models -----------------\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, fine_tune=False):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1]  # remove fc\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(512, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fine_tune(fine_tune)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)               # (B, 512, 1, 1)\n",
    "        feat = feat.view(feat.size(0), -1)    # (B, 512)\n",
    "        feat = self.fc(feat)                  # (B, embed)\n",
    "        feat = self.relu(feat)\n",
    "        return feat\n",
    "\n",
    "    def fine_tune(self, fine):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = fine\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # features: (B, embed), captions: (B, max_len)\n",
    "        embeddings = self.embed(captions)            # (B, L, embed)\n",
    "        feats = features.unsqueeze(1)                # (B,1,embed)\n",
    "        inputs = torch.cat([feats, embeddings[:,:-1,:]], dim=1)  # shift right for teacher forcing\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        outputs = self.linear(outputs)               # (B, L, vocab)\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, max_len=30):\n",
    "        ids = []\n",
    "        inputs = features.unsqueeze(1)               # (B,1,embed)\n",
    "        states = None\n",
    "        for _ in range(max_len):\n",
    "            out, states = self.lstm(inputs, states)  # out: (B,1,hidden)\n",
    "            logits = self.linear(out.squeeze(1))     # (B,vocab)\n",
    "            pred = logits.argmax(dim=1)              # (B,)\n",
    "            ids.append(pred)\n",
    "            inputs = self.embed(pred).unsqueeze(1)   # (B,1,embed)\n",
    "        ids = torch.stack(ids, dim=1)                # (B, max_len)\n",
    "        return ids\n",
    "\n",
    "# ----------------- Training utilities -----------------\n",
    "\n",
    "def train_epoch(enc, dec, loader, criterion, enc_opt, dec_opt, device):\n",
    "    enc.train(); dec.train()\n",
    "    total=0; loss_acc=0.0\n",
    "    for images, caps, _ in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        images = images.to(device); caps=caps.to(device)\n",
    "        feats = enc(images)\n",
    "        outputs = dec(feats, caps)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), caps.view(-1))\n",
    "        if enc_opt: enc_opt.zero_grad()\n",
    "        dec_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(dec.parameters(), 5.0)\n",
    "        if enc_opt: nn.utils.clip_grad_norm_(enc.parameters(), 5.0)\n",
    "        if enc_opt: enc_opt.step()\n",
    "        dec_opt.step()\n",
    "        loss_acc += loss.item()\n",
    "        total += 1\n",
    "    return loss_acc/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(enc, dec, loader, criterion, device):\n",
    "    enc.eval(); dec.eval()\n",
    "    total=0; loss_acc=0.0\n",
    "    for images, caps, _ in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        images = images.to(device); caps=caps.to(device)\n",
    "        feats = enc(images)\n",
    "        outputs = dec(feats, caps)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), caps.view(-1))\n",
    "        loss_acc += loss.item()\n",
    "        total += 1\n",
    "    return loss_acc/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(enc, dec, img_path, transform, vocab, device, max_len=30):\n",
    "    enc.eval(); dec.eval()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    feats = enc(x)\n",
    "    gen = dec.sample(feats, max_len=max_len)[0].cpu().tolist()\n",
    "    words=[]\n",
    "    for idx in gen:\n",
    "        if idx < len(vocab.itos):\n",
    "            tok = vocab.itos[idx]\n",
    "        else:\n",
    "            tok = '<unk>'\n",
    "        if tok == '<end>': break\n",
    "        if tok not in ('<pad>','<start>'):\n",
    "            words.append(tok)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# ----------------- Prepare data & models -----------------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "print(\"Preparing datasets...\")\n",
    "train_ds = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=None, transform=transform, split='train', train_ratio=0.8, seed=SEED)\n",
    "vocab = train_ds.vocab\n",
    "val_ds   = Flickr30kDataset(IMAGES_DIR, CAPTIONS_FILE, vocab=vocab, transform=transform, split='val', train_ratio=0.8, seed=SEED)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "enc = Encoder(EMBED_SIZE, fine_tune=FINE_TUNE).to(DEVICE)\n",
    "dec = Decoder(EMBED_SIZE, HIDDEN_SIZE, vocab_size=len(vocab)).to(DEVICE)\n",
    "dec_opt = optim.Adam(dec.parameters(), lr=LEARNING_RATE)\n",
    "enc_opt = optim.Adam(enc.parameters(), lr=LEARNING_RATE*0.1) if FINE_TUNE else None\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
    "\n",
    "# ----------------- Train loop -----------------\n",
    "\n",
    "best_val = 1e9\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    tr_loss = train_epoch(enc, dec, train_loader, criterion, enc_opt, dec_opt, DEVICE)\n",
    "    val_loss = validate(enc, dec, val_loader, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}  train_loss={tr_loss:.4f}  val_loss={val_loss:.4f}\")\n",
    "    ckpt = {\n",
    "        'epoch': epoch,\n",
    "        'encoder': enc.state_dict(),\n",
    "        'decoder': dec.state_dict(),\n",
    "        'vocab': vocab.itos\n",
    "    }\n",
    "    torch.save(ckpt, os.path.join(OUTPUT_DIR, f\"ckpt_epoch_{epoch}.pth\"))\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(ckpt, os.path.join(OUTPUT_DIR, \"best.pth\"))\n",
    "\n",
    "print(\"Done. Models saved to\", OUTPUT_DIR)"
   ],
   "id": "4a815cba02749e22",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
